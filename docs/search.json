[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Additional Resources: Coding",
    "section": "",
    "text": "Here are some additional resources that serve as a good starting point to learning more about the different types of tools involved in environmental data science. All listed resources are beginner-friendly so please feel free to check them out.",
    "crumbs": [
      "Resources",
      "Coding"
    ]
  },
  {
    "objectID": "resources.html#gitgithub",
    "href": "resources.html#gitgithub",
    "title": "Additional Resources: Coding",
    "section": "Git/GitHub",
    "text": "Git/GitHub\n\nCollaborative Coding with GitHub: An introductory workshop on how to use Git with GitHub and RStudio. This workshop was created by the LTER Network for their synthesis working groups but remains a great resource for all beginners, especially those who are hesitant to use the command line. There are also lessons on advanced Git/GitHub topics for those looking to further improve their skills.\nReproducible Approaches to Arctic Research Using R: A course designed by the NCEAS Learning Hub to teach researchers techniques and tools to make their work more reproducible. Particularly relevant are the Introduction to Git and GitHub and Collaborating with Git and GitHub lessons, where the Git workflow and Git commands are explained thoroughly.\nHappy Git and GitHub for the userR: A comprehensive guide by Jenny Bryan on using Git, GitHub, and RStudio that covers everything from the basics to the more intricate topics. This book has a dedicated page for troubleshooting common problems a user might run into while setting up their computer.",
    "crumbs": [
      "Resources",
      "Coding"
    ]
  },
  {
    "objectID": "resources.html#tidyverse",
    "href": "resources.html#tidyverse",
    "title": "Additional Resources: Coding",
    "section": "Tidyverse",
    "text": "Tidyverse\n\nCoding in the Tidyverse: A workshop on using dplyr, tidyr, and ggplot2, which are R packages that are part of the Tidyverse. The workshop covers the fundamental tools for tidying data in R, for example, wrangling, summarizing, reshaping, joining, and visualizing data. This workshop was created by the LTER Network for their synthesis working groups.\nReproducible Approaches to Arctic Research Using R: A course designed by the NCEAS Learning Hub to teach researchers techniques and tools to make their work more reproducible. Particularly relevant are the Data Modeling Essentials and Cleaning and Wrangling Data lessons, where the principles of tidy data and basic Tidyverse functions are explained.\nR for Data Science: An online textbook by written by Hadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund that goes over the entire Tidyverse workflow from start to finish.\nPosit Cheatsheets: A collection of cheatsheets created by Posit (formerly RStudio Inc.) to serve as helpful reminders on how to use the most important functions from a wide array of R packages. Here are the cheatsheets for dplyr, tidyr, and ggplot2.",
    "crumbs": [
      "Resources",
      "Coding"
    ]
  },
  {
    "objectID": "resources.html#spatial-data",
    "href": "resources.html#spatial-data",
    "title": "Additional Resources: Coding",
    "section": "Spatial Data",
    "text": "Spatial Data\n\nIntroduction to Geospatial Concepts: A workshop from the Data Carpentry on core geospatial concepts such as raster data, vector data, and coordinate reference systems. This workshop is a prerequisite to their Introduction to Geospatial Raster and Vector Data with R workshop and is a great resource for reviewing the different kinds of geospatial data.\nIntroduction to Geospatial Raster and Vector Data with R: A workshop from the Data Carpentry that focuses on manipulating and plotting raster and vector data using the terra, sf, and ggplot2 R packages. The functions and operations involved in a basic geospatial data workflow are taught in this workshop.\nNCEAS coreR for Delta Science Program: A course designed by the NCEAS Learning Hub to teach researchers techniques and tools to make their work more reproducible. Particularly relevant is the Working with Spatial Data lesson that works with the sf, dplyr, ggplot2, and leaflet R packages to manipulate and visualize vector data by following the Tidyverse principles. Interactive maps are also covered.\nSpatial manipulation with sf cheatsheet: A handy cheatsheet that serves as a reminder on how to use common functions from the sf R package.\nSpatial data with terra: A tutorial showcasing functions from the terra R package to manipulate both raster and vector geospatial data. The tutorial was written by the team behind the development of terra.",
    "crumbs": [
      "Resources",
      "Coding"
    ]
  },
  {
    "objectID": "spatial_projections.html",
    "href": "spatial_projections.html",
    "title": "Spatial Projections of a randomForest model",
    "section": "",
    "text": "In this workshop, we will create a spatial projection of our random forest model for monthly methane exchange from natural ecosystems.\nTo date, we have completed model calibration, validation, and sensitivity analysis. Next, we can apply the model to a landscape to estimate natural methane emissions. For this workshop, we will calculate Connecticut’s natural emissions.\n\nIn this workshop, we will:\n\nMake a list of the variables, their units, and the exact name and class of each variable in your model.\nDetermine where you can get a spatial version of each variable in your model.\nFormat each spatial layer to match the exact conditions of the data used to fit the model.\nMake spatial predictions.\nUse predictions to calculate an annual budget.\n\n\n\n(1) Make a list of the variables, their units, the exact name and class of each variable in your model.\nDatasets: data.zip\nLoad the datasets and the model.\n\nrm(list=ls())\nload(file=\"data/final_model.RDATA\" )\n\nThere are four items in this.RDATA file.\n(1) the randomForest model,\n(2) the flux net dataset,\n(3) the training data, and\n(4) the testing data.\n\nLook at the model to determine which variables are in it:\n\nlibrary(randomForest)\n\n\nFCH4_F_gC.rf\n\n\nCall:\n randomForest(formula = FCH4_F_gC ~ P_F + TA_F + Upland, data = train,      keep.forest = T, importance = TRUE, mtry = 1, ntree = 500,      keep.inbag = TRUE) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n\n          Mean of squared residuals: 4.40718\n                    % Var explained: 32.54\n\n\nThe model includes precipitation in mm (P_F), mean air temperature in degrees Celsius (TA_F), and an indicator for upland ecosystems (Upland).\nCheck the class of each variable.\n\nclass(train$P_F)\n\n[1] \"numeric\"\n\nclass(train$TA_F)\n\n[1] \"numeric\"\n\nclass(train$Upland)\n\n[1] \"factor\"\n\n\nTo project this model in space, we need the following variables:\n\nMonthly total precipitation in mm and the name of the layer needs to be “P_F”\nMonthly mean air temperature in degrees Celsius and the layer name needs to be “TA_F”\nWe need an indicator for upland ecosystems called Upland. All inundated ecosystems (+ snow) are given the value “0” and non-inundated ecosystems are given the value “1”. Croplands and urban areas should be filtered out of this layer.\n\n\n\n\n(2) Determine where you can get a spatial version of each variable in your model.\nWe will spatialize the model for Connecticut in the year 2021.\n\nMonthly total precipitation (mm): Terra climate (getTerraClim()).\nMonthly mean air temperature temperature in degrees Celsius: Terra climate (getTerraClim()).\nIndicator for Upland ecosystems (Upland): MODIS Land Cover Data (Majority_Land_Cover_Type_1). downloaded from: (2001 - 2022) https://lpdaac.usgs.gov/products/mcd12c1v061/ the user guide is available here: https://lpdaac.usgs.gov/documents/101/MCD12_User_Guide_V6.pdf.\n\n\nTo use raster layers with the predict function, they must have the same CRS, resolution, and extent!\n#(3) Format each spatial layer. Download the climate layers needed for P_F and T_F using getTerraClim().\n\nlibrary(AOI)\nlibrary(climateR)\nlibrary(terra)\nlibrary(tidyverse)\n\nCreate an AOI for Connecticut.\n\nct &lt;- AOI::aoi_get(state=\"CT\")\nplot(ct$geometry)\n\n\n\n\n\n\n\n\nDownload terra climate data (Precipitation and air temperature) for 2021.\n\nglobal.clim.N &lt;- ct %&gt;% getTerraClim(varname = c(\"ppt\", \"tmin\", \"tmax\"), \n                                     startDate = \"2021-01-01\",\n                                     endDate = \"2021-12-31\")\n\nSubset the data for each variable.\n\nglobal.clim.ppt &lt;- global.clim.N$ppt\nglobal.clim.tmin &lt;-global.clim.N$tmin\nglobal.clim.tmax &lt;- global.clim.N$tmax \n\nWe need mean air temperature. Calculate the mean using the maximum and minimum air temperature.\n\nglobal.clim.tmean &lt;-   mean(global.clim.tmin, global.clim.tmax)\nglobal.clim.tmean\n\nclass       : SpatRaster \ndimensions  : 28, 48, 13  (nrow, ncol, nlyr)\nresolution  : 0.04166674, 0.04166679  (x, y)\nextent      : -73.75, -71.75, 40.91667, 42.08334  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=longlat +ellps=WGS84 +no_defs \nsource(s)   : memory\nnames       : tmin_~total, tmin_~total, tmin_~total, tmin_~total, tmin_~total, tmin_~total, ... \nmin values  :       -5.00,        -5.9,         1.3,        7.05,       12.20,       18.65, ... \nmax values  :        1.55,         1.3,         6.0,       11.15,       15.75,       22.00, ... \ntime        : 2021-01-01 to 2022-01-01 UTC \n\n\nRemove the layers you no longer need.\n\nrm(global.clim.tmin, global.clim.tmax)\n\nSave the layers.\n\nwriteRaster(global.clim.tmean, \"data/TERRA_TMEAN_2021_CT.tif\", overwrite=TRUE )\nwriteRaster(global.clim.ppt, \"data/TERRA_PPT_2021_CT.tif\", overwrite=TRUE )\n\nNow, we need to get the MODIS IGBP layers. The dataset provided was developed from MODIS Land Cover Data (Majority_Land_Cover_Type_1) downloaded from: (2001 - 2022) https://lpdaac.usgs.gov/products/mcd12c1v061/. This dataset was downloaded for the entire globe and cropped to include only Connecticut.\nLoad the data.\n\nigbp.ct &lt;- terra::rast(\"data/MODIS_IGBP_2001-2022_CT.tif\")\nigbp.ct \n\nclass       : SpatRaster \ndimensions  : 22, 39, 22  (nrow, ncol, nlyr)\nresolution  : 0.05, 0.05  (x, y)\nextent      : -73.75, -71.8, 40.95, 42.05  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat Unknown datum based upon the Clarke 1866 ellipsoid \nsource      : MODIS_IGBP_2001-2022_CT.tif \nnames       : Major~ype_1, Major~ype_1, Major~ype_1, Major~ype_1, Major~ype_1, Major~ype_1, ... \nmin values  :           0,           0,           0,           0,           0,           0, ... \nmax values  :          14,          13,          14,          14,          14,          13, ... \ntime (raw)  : 992563200 to 1655251200 \n\nigbp.ct[[1]] %&gt;% plot\n\n\n\n\n\n\n\n\nThis layer needs to be reformatted. Using the User Guide we can determine what each numerical value represents: https://lpdaac.usgs.gov/documents/1409/MCD12_User_Guide_V61.pdf\n1: ENF. 2: EBF. 3: DNF. 4: DBF. 5: MF. 6: CS. 7: OS. 8: WS. 9 : SAV. 10 : GRA. 11: WET. 12 : CRO. 13 : URB. 14 : CRO. 15 : SNO. 16: Barren. 17 : WAT. 0: Unclassified.\nlook at the layer. Here I use”[[1]]” to see only the first layer, which is for the year 2001.\n\nterra::plot(igbp.ct[[1]])\n\n\n\n\n\n\n\n\nReclassify each value, one at a time, and think about how you should reclassify each. We want to give all uplands the value “1” and all inundated systems the value “0”.\nFirst, make a copy of the raters (igbp.ct) and call it igbp.ct.r:\n\nigbp.ct.r &lt;- igbp.ct\n\nReclassify 0 value to NA.\n\nigbp.ct.r[ igbp.ct.r == 0] &lt;- NA \nterra::plot(igbp.ct.r[[1]] )\n\n\n\n\n\n\n\n\nReclassify the other values:\n\nigbp.ct.r[ igbp.ct.r == 1] &lt;- 1 \nigbp.ct.r[ igbp.ct.r == 2] &lt;- 1\nigbp.ct.r[ igbp.ct.r == 3] &lt;- 1\nigbp.ct.r[ igbp.ct.r == 4] &lt;- 1\nigbp.ct.r[ igbp.ct.r == 5] &lt;- 1\nigbp.ct.r[ igbp.ct.r == 6] &lt;- 1\nigbp.ct.r[ igbp.ct.r == 7] &lt;- 1\nigbp.ct.r[ igbp.ct.r == 8] &lt;- 1\nigbp.ct.r[ igbp.ct.r == 9] &lt;- 1\nigbp.ct.r[ igbp.ct.r == 10] &lt;- 1\nigbp.ct.r[ igbp.ct.r == 11] &lt;- 0\nigbp.ct.r[ igbp.ct.r == 12] &lt;- NA\nigbp.ct.r[ igbp.ct.r == 13] &lt;- NA\nigbp.ct.r[ igbp.ct.r == 14] &lt;- NA\nigbp.ct.r[ igbp.ct.r == 15] &lt;- 0\nigbp.ct.r[ igbp.ct.r == 16] &lt;- 1\nigbp.ct.r[ igbp.ct.r == 17] &lt;- 0\n\nLook at the final raster to ensure everything is reclassified to upland since Connecticut doesn’t have anything else at the resolution of MODIS.\n\nterra::plot(igbp.ct.r[[1]], col='red' ) \n\n\n\n\n\n\n\n\nFormat the upland layer as a factor by first making a data frame that has the raster values 0 and 1 and the corresponding factor level.\n\nfactors.df &lt;- data.frame(id=c(1, 0), cover=c(\"upland\", \"inundated\"))\n\nCreate a for loop to assign the factor levels to each raster layer one at a time:\n\nfor ( i in 1:22){\n  print(i)\n  levels(igbp.ct.r[[i]]) &lt;- factors.df\n  is.factor(igbp.ct.r)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n[1] 11\n[1] 12\n[1] 13\n[1] 14\n[1] 15\n[1] 16\n[1] 17\n[1] 18\n[1] 19\n[1] 20\n[1] 21\n[1] 22\n\nterra::plot(igbp.ct.r[[1]], col=\"red\" )\n\n\n\n\n\n\n\n\nWe only need the layer for 2021. Subset the 2021 layer.\n\nigbp.ct.r.2021 &lt;- igbp.ct.r[[21]]\n\nWe will use the CRS of the terra climate layers and make everything match this.\n\nigbp.ct.r.2021 &lt;- terra::project( igbp.ct.r.2021, global.clim.ppt)\n\nAll the resolutions must be the same to combine the rasters into one item. We will set the terra climate layers to match the igbp.ct.r layer:\n\nglobal.clim.tmean.resample &lt;- resample( global.clim.tmean, igbp.ct.r.2021)\nglobal.clim.ppt.resample &lt;- resample( global.clim.ppt, igbp.ct.r.2021)\n\nNow, export the files to save a version processed as needed.\n\nwriteRaster(global.clim.tmean.resample, \"data/TERRA_TMEAN_2021_CT_rs.tif\", overwrite=TRUE )\nwriteRaster(global.clim.ppt.resample, \"data/TERRA_PPT_2021_CT_rs.tif\", overwrite=TRUE )\nwriteRaster(igbp.ct.r.2021, \"data/MODIS_Upland_2021_CT.tif\", overwrite=TRUE )\n\n\n\n\n(4) Make predictions\nCombine all the variables into a raster stack, only including one month since igbp.ct.r.2021 has one layer and the climate has 12, one for each month.\n\nmodel.rasters.m1 &lt;- c(igbp.ct.r.2021, global.clim.tmean.resample[[1]], global.clim.ppt.resample[[1]] )\n\nIf you have any issues combining the raster layers into one object, you may not have made everything the same resolution or extent.\nMake the names of the raster layers match the dataframe.\n\nnames(model.rasters.m1 ) &lt;- c(\"Upland\", \"TA_F\", \"P_F\" )\nmodel.rasters.m1\n\nclass       : SpatRaster \ndimensions  : 28, 48, 3  (nrow, ncol, nlyr)\nresolution  : 0.04166674, 0.04166679  (x, y)\nextent      : -73.75, -71.75, 40.91667, 42.08334  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=longlat +ellps=WGS84 +no_defs \nsource(s)   : memory\nnames       : Upland,  TA_F,  P_F \nmin values  : upland, -5.00, 41.2 \nmax values  : upland,  1.55, 65.6 \nunit        :       ,      ,   mm \n\n\nCheck the dataframe again to ensure you don’t need to make additional changes to the raster.\n\nclass(train$Upland )\n\n[1] \"factor\"\n\nsummary(train$Upland )\n\ninundated    upland \n      974       697 \n\nlevels(train$Upland )\n\n[1] \"inundated\" \"upland\"   \n\nmodel.rasters.m1$Upland\n\nclass       : SpatRaster \ndimensions  : 28, 48, 1  (nrow, ncol, nlyr)\nresolution  : 0.04166674, 0.04166679  (x, y)\nextent      : -73.75, -71.75, 40.91667, 42.08334  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=longlat +ellps=WGS84 +no_defs \nsource(s)   : memory\ncategories  : cover \nname        : Upland \nmin value   : upland \nmax value   : upland \n\n\nYou are ready to use the predict function to predict you model in space.\n\nmodel.rasters.m1.pred &lt;- terra::predict(  object= model.rasters.m1, model=FCH4_F_gC.rf)\n\nplot(model.rasters.m1.pred)\n\n\n\n\n\n\n\n\nWe can do this in a for loop to get all 12 months.\nFirst, determine where you want to export the files to, and make a new folder there called predictions.\n\nsetwd('data') # sets the working directory to your data folder\ndirectory &lt;- getwd() # saves the path to directory\nsubDir &lt;- 'predictions' # You will use this to make the folder called predictions\n\ndir.create(file.path(directory , subDir)) # this makes the new folder in data called predictions\n\nsetwd(subDir) # sets the working directory to your new folder\n\nMake the forloop to make predictions for all 12 months.\n\nfor ( i in 1:12){\n \n  print(i)\n  \n  model.rasters &lt;- c(igbp.ct.r.2021, global.clim.tmean.resample[[i]], global.clim.ppt.resample[[i]] )\n  names(model.rasters) &lt;- c(\"Upland\", \"TA_F\", \"P_F\" )\n  pred &lt;- terra::predict(  object= model.rasters, model=FCH4_F_gC.rf)\n  \n  units(pred) &lt;- 'g C m-2 month-1' # Add the units\n  names(pred ) &lt;- \"F_CH4\" # Name the layer\n  \n  writeRaster(pred ,paste(\"MODEL_PRED_m\",i,\".tif\", sep =\"\"), overwrite=TRUE )\n}\n\nDelete the json files created:\n\ndelete &lt;- list.files( pattern=\".json\")\nfile.remove(delete)\n\nMake of list of all the files in a directory that you want to import, and import all the files in the list with rast().\n\npred &lt;- list.files( pattern=\"MODEL_PRED_m\")\npredictions &lt;- rast(pred)\n\n\n\n(5) Use predictions\nCreate the 2021 methane budget. To get an annual budget, sum the total monthly fluxes.\n\npredictions.2021.total &lt;- sum(predictions )\nunits(predictions.2021.total ) &lt;- 'g C m-2 year-1' # add the units\nnames(predictions.2021.total ) &lt;- \"F_CH4\"\n\nTo determine the total amount of methane in 2021 for natural areas we need to consider the area:\n\nct.area = cellSize(predictions.2021.total, unit=\"m\")\npredictions.2021.total$F_CH4_total &lt;- (predictions.2021.total$F_CH4 * ct.area)/1000000000000 \nunits(predictions.2021.total$F_CH4_total) &lt;- \"Gigatons of carbon per year\"\n\n# Total emissions in 2021:\nglobal( predictions.2021.total$F_CH4_total, \"sum\", na.rm=T)\n\n                  sum\nF_CH4_total 0.1154527\n\n\nNow you are ready to follow the same workflow for your model. (1) For your final project, determine where you will project your model.\n(2) Make a list of the variables, their units, the exact name and class of each variable in your model.\n(3) Determine where you can get a spatial version of each variable in your model.\n(4) Format each spatial layer. (5) Make predictions. (6) Use predictions.\nEnsure your raster layers all have the same CRS and resolution!",
    "crumbs": [
      "Dynamic Models in R",
      "Spatial Model Visualization"
    ]
  },
  {
    "objectID": "data_basics.html",
    "href": "data_basics.html",
    "title": "Data Basics",
    "section": "",
    "text": "In this workshop you will:\n\nDownload climate data with daymetr\nManipulate data with tidyverse:\n\nFormat date elements\nUse select() to choose variables from a data frame.\nUse filter() to choose data based on values.\nUse mutate() to create new variables.\nUse reframe() to summarize datasets.\nUse full_join() to merge datasets\n\nVisualize data with ggplot2\nLearn to write basic functions"
  },
  {
    "objectID": "data_basics.html#learn-the-basics-of-manipulating-data-in-r",
    "href": "data_basics.html#learn-the-basics-of-manipulating-data-in-r",
    "title": "Data Basics",
    "section": "",
    "text": "In this workshop you will:\n\nDownload climate data with daymetr\nManipulate data with tidyverse:\n\nFormat date elements\nUse select() to choose variables from a data frame.\nUse filter() to choose data based on values.\nUse mutate() to create new variables.\nUse reframe() to summarize datasets.\nUse full_join() to merge datasets\n\nVisualize data with ggplot2\nLearn to write basic functions"
  },
  {
    "objectID": "data_basics.html#calling-a-function-in-r",
    "href": "data_basics.html#calling-a-function-in-r",
    "title": "Data Basics",
    "section": "Calling a Function in R",
    "text": "Calling a Function in R\nIn all the above examples, we called the created functions by using the function name and adding the necessary arguments inside the parenthesis. In R, function arguments can be passed by position, by name (so-called named arguments), by mixing position-based and name-based matching, or by omitting the arguments altogether.\nIf we pass the arguments by position, we need to follow the same sequence of arguments as defined in the function:\n\nsubtract_two_nums &lt;- function(x, y){\n    x - y\n}\n\nprint(subtract_two_nums(3, 1))\n\n[1] 2\n\n\nIn the above example, x is equal to 3 and y to 1, and not vice versa.\nIf we pass the arguments by name, i.e., explicitly specify what value each parameter defined in the function takes, the order of the arguments doesn’t matter:\n\nsubtract_two_nums &lt;- function(x, y){\n  x - y\n}\nprint(subtract_two_nums(x=3, y=1))\n\n[1] 2\n\nprint(subtract_two_nums(y=1, x=3))\n\n[1] 2\n\n\nThings to remember when using functions inside other functions: - Functions can have default values for arguments. - Functions can return multiple values using a list or other data structures. - You can use the return() statement to specify the value to be returned by the function. - If you want to be able to use the function independent of another function, it should be created outside a function instead of nesting the functions."
  },
  {
    "objectID": "data_integration.html",
    "href": "data_integration.html",
    "title": "Introduction to Data Integration in R",
    "section": "",
    "text": "Data analysis often requires combining data from multiple sources, such as files, databases, APIs, or web scraping. R is a powerful and flexible tool for data integration, but it can also pose some challenges and pitfalls. In this workshop, you will learn some of the best ways to integrate data from multiple sources in R.\n\nWorkshop Goals:\n\nUnderstand techniques for data Integration.\nCombine information from tabular data sources.\nCombine tabular and vector data.\nCombine tabular and raster data.\nCombine tabular data by an ID and time.\n\n\n\nChoose the right package\nR has many packages that can help you import, merge, and manipulate data from different sources. Some of the most popular and useful ones for tables include readr, dplyr, tidyr, and purrr. These packages are part of the tidyverse, a collection of packages that share a consistent and coherent syntax and philosophy for data analysis. For spatial data, the sf and terra packages are useful.\n\n\n\nType of Data\nLibrary\n\n\n\n\nTabular\ntidyverse\n\n\nVector\nsf\n\n\nRaster\nterra\n\n\n\n\n\nUse pipes and functions\nOne of the best features of the tidyverse is the pipe operator (%&gt;%), which allows you to chain multiple functions together and pass the output of one function as the input of the next one. This way, you can create a data integration pipeline that is clear and logical, and that avoids intermediate variables and nested functions.\n\n\nCheck and validate data\nData integration can introduce errors or inconsistencies in your data, such as duplicates, mismatches, outliers, or invalid values. Therefore, it is important to check and validate your data before and after you integrate it from multiple sources. You can use various tools and techniques to do this, such as summary statistics, data visualization, data profiling, or data quality rules.\nIn this workshop you will begin to extract information about the FLUXNET CH4 tower sites.\n\n\nLoad Libraries:\n\nlibrary(sf)\nlibrary(terra)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(AOI)\n\n\n\nIntegrating information from simple features\nDataset: Data.zip\nImport the file FluxNet_Sites_2024.csv and call it FluxNet\n\nFluxNet &lt;- read.csv('Data/FluxNet_Sites_2024.csv')\n\n\n\n\nColumn Name\nDescription\n\n\n\n\nSITE_ID\nUnique site id\n\n\nLOCATION_LAT\nLocation information\n\n\nLOCATION_LONG\nLocation information\n\n\n\nConvert FluxNet to a sf and call it FLUXNET.CH4.shp:\n\nFLUXNET.CH4.shp &lt;- st_as_sf(x = FluxNet,                         \n           coords = c(\"LOCATION_LONG\",  \"LOCATION_LAT\"),\n           crs = 4326)\n\nggplot(data=FLUXNET.CH4.shp ) + geom_sf()\n\n\n\n\n\n\n\n\ncheck the class and that the geometry is valid:\n\nclass(FLUXNET.CH4.shp)\n\n[1] \"sf\"         \"data.frame\"\n\nst_is_valid(FLUXNET.CH4.shp)\n\n [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[16] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[31] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[46] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[61] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[76] TRUE TRUE TRUE TRUE\n\n\nCreate a global sf and extract the country into the sf\n\nglobal &lt;- aoi_get(country= c(\"Europe\",\"Asia\" ,\"North America\", \"South America\", \"Australia\",\"Africa\", \"New Zealand\"))\n\nst_is_valid(global)\n\n  [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [13] FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [25]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [37]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [49]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [61]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [73]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [85]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [97]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[109]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[121]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[133]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[145]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[157]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[169]  TRUE  TRUE  TRUE\n\n\nMake the CRS match:\n\nFLUXNET.CH4.shp = st_transform(FLUXNET.CH4.shp, crs= '+init=epsg:4087')\n\nWarning in CPL_crs_from_input(x): GDAL Message 1: +init=epsg:XXXX syntax is\ndeprecated. It might return a CRS with a non-EPSG compliant axis order.\n\nglobal = st_transform(global, crs= '+init=epsg:4087')\n\nggplot() + geom_sf(data = global) + geom_sf(data = FLUXNET.CH4.shp) \n\n\n\n\n\n\n\n\nUse the st_intersect to extract the country of each tower site:\n\nFLUXNET.CH4.shp$Country &lt;- st_intersection( global, FLUXNET.CH4.shp)$name\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\nFLUXNET.CH4.shp$Country \n\n [1] \"Austria\"                  \"Brazil\"                  \n [3] \"Botswana\"                 \"Botswana\"                \n [5] \"Canada\"                   \"Canada\"                  \n [7] \"Switzerland\"              \"Switzerland\"             \n [9] \"Switzerland\"              \"China\"                   \n[11] \"Germany\"                  \"Germany\"                 \n[13] \"Germany\"                  \"Germany\"                 \n[15] \"Finland\"                  \"Finland\"                 \n[17] \"Finland\"                  \"Finland\"                 \n[19] \"France\"                   \"China\"                   \n[21] \"Indonesia\"                \"Italy\"                   \n[23] \"Italy\"                    \"Japan\"                   \n[25] \"Japan\"                    \"Japan\"                   \n[27] \"South Korea\"              \"Malaysia\"                \n[29] \"Netherlands\"              \"New Zealand\"             \n[31] \"Philippines\"              \"Russia\"                  \n[33] \"Russia\"                   \"Russia\"                  \n[35] \"Russia\"                   \"Sweden\"                  \n[37] \"United Kingdom\"           \"United States of America\"\n[39] \"United States of America\" \"United States of America\"\n[41] \"United States of America\" \"United States of America\"\n[43] \"United States of America\" \"United States of America\"\n[45] \"United States of America\" \"United States of America\"\n[47] \"United States of America\" \"United States of America\"\n[49] \"United States of America\" \"United States of America\"\n[51] \"United States of America\" \"United States of America\"\n[53] \"United States of America\" \"United States of America\"\n[55] \"United States of America\" \"United States of America\"\n[57] \"United States of America\" \"United States of America\"\n[59] \"United States of America\" \"United States of America\"\n[61] \"United States of America\" \"United States of America\"\n[63] \"United States of America\" \"United States of America\"\n[65] \"United States of America\" \"United States of America\"\n[67] \"United States of America\" \"United States of America\"\n[69] \"United States of America\" \"United States of America\"\n[71] \"United States of America\" \"United States of America\"\n[73] \"United States of America\" \"United States of America\"\n[75] \"United States of America\" \"United States of America\"\n[77] \"United States of America\" \"United States of America\"\n[79] \"United States of America\"\n\n\n\n\nIntegrating information from rasters\nImport the file GlobalSoil_grids.tif :\n\nsoil &lt;- terra::rast(\"Data/GlobalSoil_grids.tif\" )\ncrs(soil)\n\n[1] \"GEOGCRS[\\\"WGS 84\\\",\\n    ENSEMBLE[\\\"World Geodetic System 1984 ensemble\\\",\\n        MEMBER[\\\"World Geodetic System 1984 (Transit)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G730)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G873)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1150)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1674)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1762)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G2139)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G2296)\\\"],\\n        ELLIPSOID[\\\"WGS 84\\\",6378137,298.257223563,\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n        ENSEMBLEACCURACY[2.0]],\\n    PRIMEM[\\\"Greenwich\\\",0,\\n        ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    CS[ellipsoidal,2],\\n        AXIS[\\\"geodetic latitude (Lat)\\\",north,\\n            ORDER[1],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        AXIS[\\\"geodetic longitude (Lon)\\\",east,\\n            ORDER[2],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    USAGE[\\n        SCOPE[\\\"Horizontal component of 3D system.\\\"],\\n        AREA[\\\"World.\\\"],\\n        BBOX[-90,-180,90,180]],\\n    ID[\\\"EPSG\\\",4326]]\"\n\n\nTransform FLUXNET.CH4.shp to the same CRS as soils:\n\nFLUXNET.CH4.shp = st_transform(FLUXNET.CH4.shp, crs= crs(soil))\n\nExtract soil information to FLUXNET.CH4.shp:\n\nFLUXNET.CH4.shp$SOIL_BulkDensity = terra::extract(soil, FLUXNET.CH4.shp)$BulkDensity\n\nFLUXNET.CH4.shp$SOIL_PH = terra::extract(soil, FLUXNET.CH4.shp)$PH\n\nFLUXNET.CH4.shp$SOIL_Nitrogen = terra::extract(soil, FLUXNET.CH4.shp)$Nitrogen\n\nImport the climate information (GlobalClimate.tif) :\n\nclimate &lt;- terra::rast(\"Data/GlobalClimate.tif\" )\ncrs(climate)\n\n[1] \"GEOGCRS[\\\"unknown\\\",\\n    DATUM[\\\"unknown\\\",\\n        ELLIPSOID[\\\"WGS 84\\\",6378137,298.257223563,\\n            LENGTHUNIT[\\\"metre\\\",1,\\n                ID[\\\"EPSG\\\",9001]]]],\\n    PRIMEM[\\\"unknown\\\",0,\\n        ANGLEUNIT[\\\"degree\\\",0.0174532925199433,\\n            ID[\\\"EPSG\\\",9122]]],\\n    CS[ellipsoidal,2],\\n        AXIS[\\\"latitude\\\",north,\\n            ORDER[1],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433,\\n                ID[\\\"EPSG\\\",9122]]],\\n        AXIS[\\\"longitude\\\",east,\\n            ORDER[2],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433,\\n                ID[\\\"EPSG\\\",9122]]]]\"\n\n\nTransform FLUXNET.CH4.shp to the same CRS as climate:\n\nFLUXNET.CH4.shp = st_transform(FLUXNET.CH4.shp, crs= crs(climate))\n\nLook at the data that is available in climate:\n\nnames(climate)\n\n[1] \"MAP\"  \"TMIN\" \"TMAX\" \"MAT\" \n\n\nExtract climate information to FLUXNET.CH4.shp:\n\nFLUXNET.CH4.shp$MAP = terra::extract(climate, FLUXNET.CH4.shp)$MAP\nFLUXNET.CH4.shp$TMIN = terra::extract(climate, FLUXNET.CH4.shp)$TMIN\nFLUXNET.CH4.shp$TMAX = terra::extract(climate, FLUXNET.CH4.shp)$TMAX\nFLUXNET.CH4.shp$MAT = terra::extract(climate, FLUXNET.CH4.shp)$MAT\n\nImport elevation information (Elevation.tif):\n\nelevation &lt;- terra::rast(\"Data/Elevation.tif\" )\ncrs(elevation)\n\n[1] \"GEOGCRS[\\\"WGS 84\\\",\\n    ENSEMBLE[\\\"World Geodetic System 1984 ensemble\\\",\\n        MEMBER[\\\"World Geodetic System 1984 (Transit)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G730)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G873)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1150)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1674)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G1762)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G2139)\\\"],\\n        MEMBER[\\\"World Geodetic System 1984 (G2296)\\\"],\\n        ELLIPSOID[\\\"WGS 84\\\",6378137,298.257223563,\\n            LENGTHUNIT[\\\"metre\\\",1]],\\n        ENSEMBLEACCURACY[2.0]],\\n    PRIMEM[\\\"Greenwich\\\",0,\\n        ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    CS[ellipsoidal,2],\\n        AXIS[\\\"geodetic latitude (Lat)\\\",north,\\n            ORDER[1],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        AXIS[\\\"geodetic longitude (Lon)\\\",east,\\n            ORDER[2],\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n    USAGE[\\n        SCOPE[\\\"Horizontal component of 3D system.\\\"],\\n        AREA[\\\"World.\\\"],\\n        BBOX[-90,-180,90,180]],\\n    ID[\\\"EPSG\\\",4326]]\"\n\n\nTransform FLUXNET.CH4.shp to the same CRS as elevation:\n\nFLUXNET.CH4.shp = st_transform(FLUXNET.CH4.shp, crs= crs(elevation))\n\nLook at the data that is available in elevation:\n\nnames(elevation)\n\n[1] \"wc2.1_2.5m_elev\"\n\n\nExtract elevation information to FLUXNET.CH4.shp:\n\nFLUXNET.CH4.shp$ELEVATION = terra::extract(elevation, FLUXNET.CH4.shp)$wc2.1_2.5m_elev\n\n\n\nJoining tables\nWe can combine columns from two (or more) tables together. This can be achieved using the join family of functions in dplyr. There are different types of joins that will result in different outcomes.\ninner_join() includes all rows that appear in both the first data frame (x) and the second data frame (y).\nleft_join() returns all rows from x based on matching rows on shared columns in y. right_join() is the companion to left_join(), but returns all rows included in y based on matching rows on shared columns in x.\nImport APPEEARS file where I requested MODIS NDVI and EVI data for all FLUXNET_sites (Data/ENV720-MOD13A3-061-results.csv):\n\nFLUXNET &lt;- read.csv(\"Data/ENV720-MOD13A3-061-results.csv\")\nnames(FLUXNET)\n\n [1] \"ID\"                                                                         \n [2] \"Latitude\"                                                                   \n [3] \"Longitude\"                                                                  \n [4] \"Date\"                                                                       \n [5] \"MODIS_Tile\"                                                                 \n [6] \"MOD13A3_061_Line_Y_1km\"                                                     \n [7] \"MOD13A3_061_Sample_X_1km\"                                                   \n [8] \"MOD13A3_061__1_km_monthly_EVI\"                                              \n [9] \"MOD13A3_061__1_km_monthly_NDVI\"                                             \n[10] \"MOD13A3_061__1_km_monthly_VI_Quality\"                                       \n[11] \"MOD13A3_061__1_km_monthly_VI_Quality_bitmask\"                               \n[12] \"MOD13A3_061__1_km_monthly_VI_Quality_MODLAND\"                               \n[13] \"MOD13A3_061__1_km_monthly_VI_Quality_MODLAND_Description\"                   \n[14] \"MOD13A3_061__1_km_monthly_VI_Quality_VI_Usefulness\"                         \n[15] \"MOD13A3_061__1_km_monthly_VI_Quality_VI_Usefulness_Description\"             \n[16] \"MOD13A3_061__1_km_monthly_VI_Quality_Aerosol_Quantity\"                      \n[17] \"MOD13A3_061__1_km_monthly_VI_Quality_Aerosol_Quantity_Description\"          \n[18] \"MOD13A3_061__1_km_monthly_VI_Quality_Adjacent_cloud_detected\"               \n[19] \"MOD13A3_061__1_km_monthly_VI_Quality_Adjacent_cloud_detected_Description\"   \n[20] \"MOD13A3_061__1_km_monthly_VI_Quality_Atmosphere_BRDF_Correction\"            \n[21] \"MOD13A3_061__1_km_monthly_VI_Quality_Atmosphere_BRDF_Correction_Description\"\n[22] \"MOD13A3_061__1_km_monthly_VI_Quality_Mixed_Clouds\"                          \n[23] \"MOD13A3_061__1_km_monthly_VI_Quality_Mixed_Clouds_Description\"              \n[24] \"MOD13A3_061__1_km_monthly_VI_Quality_Land.Water_Mask\"                       \n[25] \"MOD13A3_061__1_km_monthly_VI_Quality_Land.Water_Mask_Description\"           \n[26] \"MOD13A3_061__1_km_monthly_VI_Quality_Possible_snow.ice\"                     \n[27] \"MOD13A3_061__1_km_monthly_VI_Quality_Possible_snow.ice_Description\"         \n[28] \"MOD13A3_061__1_km_monthly_VI_Quality_Possible_shadow\"                       \n[29] \"MOD13A3_061__1_km_monthly_VI_Quality_Possible_shadow_Description\"           \n\n\nSubset the FLUXNET dataset to include only the columns of interest and rename them:\n\nFLUXNET.sub &lt;- FLUXNET %&gt;% select( \"ID\",\n\"Date\",\n\"MOD13A3_061__1_km_monthly_EVI\", \"MOD13A3_061__1_km_monthly_NDVI\", \"MOD13A3_061__1_km_monthly_VI_Quality\") %&gt;% \nrename( SITE_ID = ID,\nEVI = MOD13A3_061__1_km_monthly_EVI,\nNDVI = MOD13A3_061__1_km_monthly_NDVI, \nQAQC = MOD13A3_061__1_km_monthly_VI_Quality ) %&gt;% filter( QAQC &gt; 0)\n\nnames(FLUXNET.sub)\n\n[1] \"SITE_ID\" \"Date\"    \"EVI\"     \"NDVI\"    \"QAQC\"   \n\n\nMake the FLUXNET.CH4.shp vector a dataframe:\n\nFLUXNET_CH4 &lt;- as.data.frame( FLUXNET.CH4.shp)\n\nIdentify the column that you should use to join the datasets FLUXNET_CH4 and FLUXNET.sub:\n\nFLUXNET_CH4$SITE_ID\nFLUXNET.sub$SITE_ID\n\nUse left_join because you want to keep all the data from FLUXNET_CH4 and only the sites in FLUXNET.sub that match the sites in FLUXNET_CH4:\n\nFLUXNET_CH4_final &lt;- FLUXNET_CH4 %&gt;% left_join( FLUXNET.sub, by= 'SITE_ID')\n\nCheck to make sure the list of sites matches:\n\nlength( unique(FLUXNET_CH4$SITE_ID))\n\n[1] 79\n\nlength(unique(FLUXNET_CH4_final$SITE_ID))\n\n[1] 79\n\n\nImport the monthly FLUX data:\n\nload( \"Data/FLUXNET_FLUXES.RDATA\")\n\nLook at the flux file “FLUXNET.flux”:\n\nsummary( FLUXNET.flux)\n\n    YearMon         SITE                P_F               TA_F        \n Min.   :2006   Length:2370        Min.   :   0.00   Min.   :-37.188  \n 1st Qu.:2014   Class :character   1st Qu.:  14.42   1st Qu.:  2.863  \n Median :2015   Mode  :character   Median :  43.10   Median : 11.123  \n Mean   :2015                      Mean   :  69.08   Mean   :  9.543  \n 3rd Qu.:2017                      3rd Qu.:  84.12   3rd Qu.: 17.540  \n Max.   :2019                      Max.   :1094.02   Max.   : 31.280  \n     VPD_F           NEE_F_gC          FCH4_F_gC      \n Min.   : 0.000   Min.   :-383.426   Min.   :-1.8194  \n 1st Qu.: 1.635   1st Qu.: -24.661   1st Qu.: 0.1008  \n Median : 3.982   Median :   2.918   Median : 0.4034  \n Mean   : 4.763   Mean   :   3.815   Mean   : 1.3906  \n 3rd Qu.: 6.722   3rd Qu.:  19.432   3rd Qu.: 1.4318  \n Max.   :29.075   Max.   :1251.787   Max.   :28.3012  \n\nnames(FLUXNET.flux)\n\n[1] \"YearMon\"   \"SITE\"      \"P_F\"       \"TA_F\"      \"VPD_F\"     \"NEE_F_gC\" \n[7] \"FCH4_F_gC\"\n\n\n\n# Make SITE ID:\nFLUXNET.flux$SITE_ID &lt;- FLUXNET.flux$SITE\n\n# This is a good column to join based on:\nFLUXNET.flux$YearMon\n\nIn your site file, convert the date to Year-Month and use this column to join it with the flux file “FLUXNET.flux”:\n\nnames(FLUXNET_CH4_final)\n\n [1] \"SITE_ID\"          \"SITE_NAME\"        \"FLUXNET2015\"      \"geometry\"        \n [5] \"Country\"          \"SOIL_BulkDensity\" \"SOIL_PH\"          \"SOIL_Nitrogen\"   \n [9] \"MAP\"              \"TMIN\"             \"TMAX\"             \"MAT\"             \n[13] \"ELEVATION\"        \"Date\"             \"EVI\"              \"NDVI\"            \n[17] \"QAQC\"            \n\nsummary( FLUXNET_CH4_final$Date)\n\n   Length     Class      Mode \n    12264 character character \n\n\n\n# Format as a Date:\nFLUXNET_CH4_final$Date.f &lt;-FLUXNET_CH4_final$Date %&gt;% as.Date(format='%Y-%m-%d')\n\n# Format as a Yearmon:\nlibrary(zoo)\n\n\nAttaching package: 'zoo'\n\n\nThe following object is masked from 'package:terra':\n\n    time&lt;-\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\nFLUXNET_CH4_final$YearMon &lt;- FLUXNET_CH4_final$Date.f %&gt;% zoo::as.yearmon( \"%m-%Y\")\n\nclass(FLUXNET_CH4_final$YearMon)\n\n[1] \"yearmon\"\n\n\nJoin the two files:\n\nfluxes_month &lt;- FLUXNET.flux %&gt;% left_join(FLUXNET_CH4_final , by = c ('YearMon', 'SITE_ID'))\n\nSave your file:\n\nsave(fluxes_month, file=\"Monthly_Fluxes.RDATA\" )\n\nYou are now prepared to take data from different sources to build a file to explore patterns in methane infrastructure.",
    "crumbs": [
      "Data Integration in R",
      "Data Integration"
    ]
  },
  {
    "objectID": "sf.html",
    "href": "sf.html",
    "title": "Introduction to Simple Features in R",
    "section": "",
    "text": "Install libraries for this workshop\n\ninstall.packages('sf')\ninstall.packages('devtools')\ndevtools::install_github(\"mikejohnson51/AOI\", force = TRUE)\ndevtools::install_github(\"valentinitnelav/plotbiomes\")\n\n\n\nLoad the required libraries for this workshop\n\nlibrary(sf)\nlibrary(AOI)\nlibrary(tidyverse)\nlibrary(ggplot2)\n\n\n\nGoals\nThe goals of this workshop are to: 1. Become familiar with simple features 2. Master simple feature manipulation 3. Visualize simple features\n\n\nData\nImport FluxNet_Sites_2024.csv. This table was created from the FLUXNET site list found at https://fluxnet.org/sites/site-list-and-pages/?view=table.\n\nFluxNet &lt;- read.csv('data/FluxNet_Sites_2024.csv')\n\nThis dataset includes:\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\nSITE_ID\nUnique site id\n\n\nSITE_NAME\nSite name\n\n\nFLUXNET2015\nLicense information for the data for the two FLUXNET Products\n\n\nFLUXNET-CH4\nLicense information for the data for the two FLUXNET Products\n\n\nLOCATION_LAT\nLocation information\n\n\nLOCATION_LONG\nLocation information\n\n\nLOCATION_ELEV\nElevation in meters\n\n\nIGBP\nVegetation type\n\n\nMAT\nMean annual temperature in Celsius\n\n\nMAP\nMean annual precipitation in mm\n\n\n\nTake a look at the file:\n\nView(FluxNet) \n\nLook at the tower site locations:\n\nFluxNet %&gt;% ggplot( ) + geom_point( aes( x=LOCATION_LONG , y=LOCATION_LAT))\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nLook at the elevation, mean annual temperature, and mean annual precipitation for the tower site locations:\n\nFluxNet %&gt;% ggplot( aes(x=LOCATION_ELEV)) + \n  geom_histogram(color=\"black\", fill=\"white\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 50 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\nFluxNet %&gt;% ggplot( aes(x=MAT)) + \n  geom_histogram(color=\"black\", fill=\"white\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 59 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\nFluxNet %&gt;% ggplot( aes(x=MAP)) + \n  geom_histogram(color=\"black\", fill=\"white\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 60 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nWe are interested in exploring the sites with methane data. Lets subset by FLUXNET-CH4.\n\nFLUXNET.CH4 &lt;- FluxNet %&gt;% filter( FLUXNET.CH4 != \"\")\n\n\nView(FLUXNET.CH4 )\n\nThis object is currently a dataframe.\n\nclass(FLUXNET.CH4)\n\n[1] \"data.frame\"\n\n\nLets make it a simple feature using st_as_sf().\n\nFLUXNET.CH4.shp &lt;- st_as_sf(x = FLUXNET.CH4,                         \n           coords = c(\"LOCATION_LONG\",  \"LOCATION_LAT\"),\n           crs = \"+init=epsg:4326\")\n\nWarning in CPL_crs_from_input(x): GDAL Message 1: +init=epsg:XXXX syntax is\ndeprecated. It might return a CRS with a non-EPSG compliant axis order.\n\nggplot(data=FLUXNET.CH4.shp ) + geom_sf()\n\n\n\n\n\n\n\n\ncheck the class:\n\nclass(FLUXNET.CH4.shp)\n\n[1] \"sf\"         \"data.frame\"\n\n\nSimple features describe how objects in the real world can be represented in computers. They have a geometry describing where on earth the feature is located, and they have attributes, which describe other properties about the feature.\nLook at the information about the geometry:\n\nFLUXNET.CH4.shp$geometry\n\nGeometry set for 81 features \nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -163.7002 ymin: -37.3879 xmax: 175.5539 ymax: 71.3242\nGeodetic CRS:  WGS 84\nFirst 5 geometries:\n\n\nPOINT (11.3175 47.1167)\n\n\nPOINT (-56.412 -16.498)\n\n\nPOINT (22.3711 -18.9647)\n\n\nPOINT (23.1792 -19.5481)\n\n\nPOINT (-121.2984 61.3089)\n\n\nIf we print the first three features, we see their attribute values and an abridged version of the geometry.\n\nprint(FLUXNET.CH4.shp, n = 3)\n\nSimple feature collection with 81 features and 8 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -163.7002 ymin: -37.3879 xmax: 175.5539 ymax: 71.3242\nGeodetic CRS:  WGS 84\nFirst 3 features:\n  SITE_ID                 SITE_NAME FLUXNET2015 FLUXNET.CH4 LOCATION_ELEV IGBP\n1  AT-Neu                  Neustift   CC-BY-4.0   CC-BY-4.0           970  GRA\n2  BR-Npw Northern Pantanal Wetland               CC-BY-4.0           120  WSA\n3  BW-Gum                      Guma               CC-BY-4.0           950  WET\n   MAT  MAP                 geometry\n1  6.5  852  POINT (11.3175 47.1167)\n2 24.9 1486  POINT (-56.412 -16.498)\n3 21.0  460 POINT (22.3711 -18.9647)\n\n\n\n\nGeometrical Operations\nThere are many geometrical operations that can be used to achieve simple feature manipulation.\n\nmethods(class = \"sf\")\n\n  [1] [                            [[&lt;-                        \n  [3] [&lt;-                          $&lt;-                         \n  [5] aggregate                    anti_join                   \n  [7] arrange                      as.data.frame               \n  [9] cbind                        coerce                      \n [11] dbDataType                   dbWriteTable                \n [13] distinct                     dplyr_reconstruct           \n [15] drop_na                      duplicated                  \n [17] filter                       full_join                   \n [19] gather                       group_by                    \n [21] group_split                  identify                    \n [23] initialize                   inner_join                  \n [25] left_join                    merge                       \n [27] mutate                       nest                        \n [29] pivot_longer                 pivot_wider                 \n [31] plot                         points                      \n [33] print                        rbind                       \n [35] rename_with                  rename                      \n [37] right_join                   rowwise                     \n [39] sample_frac                  sample_n                    \n [41] select                       semi_join                   \n [43] separate_rows                separate                    \n [45] show                         slice                       \n [47] slotsFromS3                  spread                      \n [49] st_agr                       st_agr&lt;-                    \n [51] st_area                      st_as_s2                    \n [53] st_as_sf                     st_as_sfc                   \n [55] st_bbox                      st_boundary                 \n [57] st_break_antimeridian        st_buffer                   \n [59] st_cast                      st_centroid                 \n [61] st_collection_extract        st_concave_hull             \n [63] st_convex_hull               st_coordinates              \n [65] st_crop                      st_crs                      \n [67] st_crs&lt;-                     st_difference               \n [69] st_drop_geometry             st_exterior_ring            \n [71] st_filter                    st_geometry                 \n [73] st_geometry&lt;-                st_inscribed_circle         \n [75] st_interpolate_aw            st_intersection             \n [77] st_intersects                st_is_full                  \n [79] st_is_valid                  st_is                       \n [81] st_join                      st_line_merge               \n [83] st_m_range                   st_make_valid               \n [85] st_minimum_rotated_rectangle st_nearest_points           \n [87] st_node                      st_normalize                \n [89] st_point_on_surface          st_polygonize               \n [91] st_precision                 st_reverse                  \n [93] st_sample                    st_segmentize               \n [95] st_set_precision             st_shift_longitude          \n [97] st_simplify                  st_snap                     \n [99] st_sym_difference            st_transform                \n[101] st_triangulate_constrained   st_triangulate              \n[103] st_union                     st_voronoi                  \n[105] st_wrap_dateline             st_write                    \n[107] st_z_range                   st_zm                       \n[109] summarise                    text                        \n[111] transform                    transmute                   \n[113] ungroup                      unite                       \n[115] unnest                      \nsee '?methods' for accessing help and source code\n\n\nBelow we will explore a few methods:\nst_is_valid and st_is_simple return a boolean indicating whether a geometry is valid or simple.\n\nst_is_valid(FLUXNET.CH4.shp)\n\n [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[16] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[31] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[46] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[61] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[76] TRUE TRUE TRUE TRUE TRUE TRUE\n\n\n\nChange the CRS to a projected EPGS with st_transform.\n Coordinate reference systems (CRS)  are like measurement units for coordinates: they specify which location on Earth a particular coordinate pair refers to. Simple feature have two attributes to store a CRS: epsg and proj4string. This implies that all geometries in a geometry list-column must have the same CRS. Both may be NA, e.g. in case the CRS is unknown, or when we work with local coordinate systems (e.g. inside a building, a body, or an abstract space).\nproj4string is a generic, string-based description of a CRS, understood by the PROJ library. It defines projection types and (often) defines parameter values for particular projections, and hence can cover an infinite amount of different projections. This library (also used by GDAL) provides functions to convert or transform between different CRS. epsg is the integer ID for a particular, known CRS that can be resolved into a proj4string. Some proj4string values can be resolved back into their corresponding epsg ID, but this does not always work.\nThe importance of having epsg values stored with data besides proj4string values is that the epsg refers to particular, well-known CRS, whose parameters may change (improve) over time; fixing only the proj4string may remove the possibility to benefit from such improvements, and limit some of the provenance of datasets, but may help reproducibility. Coordinate reference system transformations can be carried out using st_transform\n\nFLUXNET.CH4.shp = st_transform(FLUXNET.CH4.shp, '+init=epsg:4087')\n\nCheck the CRS:\n\nst_crs(FLUXNET.CH4.shp)\n\nCoordinate Reference System:\n  User input: +init=epsg:4087 \n  wkt:\nPROJCRS[\"WGS 84 / World Equidistant Cylindrical\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"World Equidistant Cylindrical\",\n        METHOD[\"Equidistant Cylindrical\",\n            ID[\"EPSG\",1028]],\n        PARAMETER[\"Latitude of 1st standard parallel\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Longitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"False easting\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]],\n        ID[\"EPSG\",4085]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n    USAGE[\n        SCOPE[\"unknown\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]]]\n\n\nst_distance returns a dense numeric matrix with distances between geometries:\n\nFLUXNET.CH4.shp$MeanDistance_km &lt;- st_distance(FLUXNET.CH4.shp, FLUXNET.CH4.shp) %&gt;% rowMeans()/1000\n\nFLUXNET.CH4.shp %&gt;% ggplot() + geom_sf(aes(col = MeanDistance_km ))\n\n\n\n\n\n\n\nFLUXNET.CH4.shp %&gt;% ggplot( aes(x=MeanDistance_km)) + \n  geom_histogram(color=\"black\", fill=\"white\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\nThe package AOI\nThe AOI package in R stands for Area of Interest. It is primarily used for geographic or spatial data analysis, particularly in defining, visualizing, and working with specific regions or “areas of interest” on maps. The package facilitates interaction with various geographic data sources, allowing users to work with location-based data in a flexible and intuitive way.\n\nKey Feature of the AOI Package:\nDefine Areas of Interest (AOIs): Users can define specific geographic regions of interest using coordinates, addresses, or administrative boundaries (e.g., cities, countries).\n\n\nKey Function of the AOI Package:\naoi_get(): Defines an Area of Interest based on a variety of inputs such as bounding box coordinates, administrative boundaries, or addresses.\nExample:\n\nlibrary(AOI)\n\n# Define an area of interest by coordinates\n# aoi.NY.bb &lt;- aoi_get(\"New York\")\naoi.NY &lt;- aoi_get(state=\"New York\")\n\n# aoi.NY.bb %&gt;% ggplot() + geom_sf()\naoi.NY %&gt;% ggplot() + geom_sf()\n\n\n\n\n\n\n\n\nCreates a simple feature of South America:\n\ns.america &lt;- aoi_get(country= \"South America\", union=T)\ns.america %&gt;% ggplot() + geom_sf()\n\n\n\n\n\n\n\n\n\n\nWhat is union =T doing?\nRe-project the s.america to match Fluxnet.ch4:\n\naoi.SAmerica &lt;- st_transform( s.america , '+init=epsg:4087') \naoi.SAmerica  %&gt;% ggplot() + geom_sf()\n\n\n\n\n\n\n\n\nCreates a simple feature of Brazil and re-project it to match Fluxnet.ch4:\nThe commands st_intersects, st_disjoint, st_touches, st_crosses, st_within, st_contains, st_overlaps, st_equals, st_covers, st_covered_by, st_equals_exact and st_is_within_distance all return a sparse matrix with matching (TRUE) indexes, or a full logical matrix:\n\n\n\nHow many towers are in South America?\n\nst_intersects(aoi.SAmerica, FLUXNET.CH4.shp)\n\nSparse geometry binary predicate list of length 1, where the predicate\nwas `intersects'\n 1: 2\n\nst_intersects(aoi.SAmerica, FLUXNET.CH4.shp, sparse = FALSE)\n\n      [,1] [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10] [,11] [,12]\n[1,] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n     [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24]\n[1,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n     [,25] [,26] [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36]\n[1,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n     [,37] [,38] [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48]\n[1,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n     [,49] [,50] [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60]\n[1,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n     [,61] [,62] [,63] [,64] [,65] [,66] [,67] [,68] [,69] [,70] [,71] [,72]\n[1,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n     [,73] [,74] [,75] [,76] [,77] [,78] [,79] [,80] [,81]\n[1,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\n\nHow many towers are in Brazil?\nWhere possible geometric operations such as st_distance(), st_length() and st_area() report results with a units attribute appropriate for the CRS.\nCalculate the area of Brazil:\n\naoi.SAmerica$Area &lt;- st_area(aoi.SAmerica )\n\n\nVisualize the global distribution of towers:\nFirst create a simple feature for all large terrestrial regions in Europe, Asia, the Americas, Africa, Australia and New Zealand:\n\naoi.terrestrial &lt;- aoi_get(country= c(\"Europe\",\"Asia\" ,\"North America\", \"South America\", \"Australia\",\"Africa\", \"New Zealand\"))\n\naoi.terrestrial  %&gt;% ggplot() + geom_sf()\n\n\n\n\n\n\n\n\nLook at the CRS:\n\nst_crs(aoi.terrestrial)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\nRe-project the polygon to match FLUXNET.CH4.shp:\n\naoi.terrestrial &lt;- aoi_get(country= c(\"Europe\",\"Asia\" ,\"North America\", \"South America\", \"Australia\",\"Africa\", \"New Zealand\")) %&gt;% st_transform( 4087 ) \n\n\nst_crs(aoi.terrestrial)\n\nCoordinate Reference System:\n  User input: EPSG:4087 \n  wkt:\nPROJCRS[\"WGS 84 / World Equidistant Cylindrical\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"World Equidistant Cylindrical\",\n        METHOD[\"Equidistant Cylindrical\",\n            ID[\"EPSG\",1028]],\n        PARAMETER[\"Latitude of 1st standard parallel\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8823]],\n        PARAMETER[\"Longitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"False easting\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Graticule coordinates expressed in simple Cartesian form.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4087]]\n\n\nVisualize the shapefile you created:\n\naoi.terrestrial %&gt;% ggplot() + geom_sf()\n\n\n\n\n\n\n\n\nUse ggplot to visualize the global distribution of Fluxnet CH4 sites:\n\nggplot() + geom_sf(data = aoi.terrestrial) + geom_sf(data = FLUXNET.CH4.shp) \n\n\n\n\n\n\n\n\nExtract the country from the world simple feature into FLUXNET.CH4.shp:\n\nFLUXNET.CH4.shp$Country &lt;- st_intersection( aoi.terrestrial, FLUXNET.CH4.shp)$name\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n\nExplore the Fluxnet CH4 sites:\n\nnames(FLUXNET.CH4.shp)\n\n [1] \"SITE_ID\"         \"SITE_NAME\"       \"FLUXNET2015\"     \"FLUXNET.CH4\"    \n [5] \"LOCATION_ELEV\"   \"IGBP\"            \"MAT\"             \"MAP\"            \n [9] \"geometry\"        \"MeanDistance_km\" \"Country\"        \n\nggplot(data= FLUXNET.CH4.shp) + geom_point( aes(x=MAT, y=MAP))\n\nWarning: Removed 10 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nggplot(data= FLUXNET.CH4.shp) + geom_point( aes(x=MAT, y=MAP, col=IGBP))\n\nWarning: Removed 10 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nFLUXNET.CH4.shp$IGBP &lt;- as.factor( FLUXNET.CH4.shp$IGBP)\nsummary(FLUXNET.CH4.shp$IGBP)\n\nBSV CRO CSH EBF ENF GRA  MF OSH SNO URB WAT WET WSA \n  2  13   1   4   7   9   1   2   1   1   2  37   1 \n\n\n\n\n\nWriting files using st_write:\nWhen writing, you can use the following arguments to control update and delete: update=TRUE causes an existing data source to be updated, if it exists; this option is by default TRUE for all database drivers, where the database is updated by adding a table.\ndelete_layer=TRUE causes st_write try to open the data source and delete the layer; no errors are given if the data source is not present, or the layer does not exist in the data source.\ndelete_dsn=TRUE causes st_write to delete the data source when present, before writing the layer in a newly created data source. No error is given when the data source does not exist. This option should be handled with care, as it may wipe complete directories or databases.\n\nwrite_sf(FLUXNET.CH4.shp, \"data/FLUXNET_CH4.shp\") \n\nIt is possible to create data.frame objects with geometry list-columns that are not of class sf by:\n\nFluxnet.ch4.df &lt;- as.data.frame(FLUXNET.CH4.shp)\n\nCheck the class:\n\nclass(Fluxnet.ch4.df)\n\n[1] \"data.frame\"\n\n\nSuch objects: no longer register which column is the geometry list-column no longer have a plot method, and lack all of the other dedicated methods listed above for class sf. To write this object:\n\nwrite.csv(Fluxnet.ch4.df, \"data/Fluxnet.ch4.df\") \n\n\n\nAdditional Reading:\nS. Scheider, B. Gräler, E. Pebesma, C. Stasch, 2016. Modelling spatio-temporal information generation. Int J of Geographic Information Science, 30 (10), 1980-2008. (open access) Stasch, C., S. Scheider, E. Pebesma, W. Kuhn, 2014. Meaningful Spatial Prediction and Aggregation. Environmental Modelling & Software, 51, (149–165, open access).\n\n\n\nPost Workshop Assessment:\nWrite a 3-page report on the distribution of tower sites discussing the strengths and weakness of the current tower representation. Please create 2 visualizations. You are welcome to use any additional data.",
    "crumbs": [
      "Introduction to R",
      "Vector Data"
    ]
  },
  {
    "objectID": "terra.html",
    "href": "terra.html",
    "title": "Introduction to rasters with the terra package",
    "section": "",
    "text": "install.packages('sf')\ninstall.packages('terra')\ninstall.packages(\"remotes\")\ninstall.packages(\"tidyverse\")\ninstall.packages('tidyterra')\nremotes::install_github(\"mikejohnson51/AOI\")\nremotes::install_github(\"mikejohnson51/climateR\")",
    "crumbs": [
      "Introduction to R",
      "Raster Data"
    ]
  },
  {
    "objectID": "terra.html#rasters",
    "href": "terra.html#rasters",
    "title": "Introduction to rasters with the terra package",
    "section": "Rasters",
    "text": "Rasters\nA raster is a spatial data structure that subdivides an extent into rectangles known as “cells” (or “pixels”). Each cell has the capacity to store one or more values. This type of data structure is commonly referred to as a “grid” and is frequently juxtaposed with simple features.\nThe terra package offers functions designed for the creation, reading, manipulation, and writing of raster data. The terra package is built around a number of “classes” of which the SpatRaster and SpatVector are the most important.\n\nSpatRaster\nA SpatRaster object stores a number of fundamental parameters that describe it. These include the number of columns and rows, the coordinates of its spatial extent, and the coordinate reference system. In addition, a SpatRaster can store information about the file(s) in which the raster cell values are stored.\n\n\nSpatVector\nA SpatVector represents “vector” data, that is, points, lines or polygon geometries and their tabular attributes.\n\n\nWorking with climate data:\nTo become familiar with working with rasters, we will download climate data for an area of interest (AOI).\n\n# First create an AOI\naoi.global &lt;- aoi_get(country= c(\"Europe\",\"Asia\" ,\"North America\", \"South America\", \"Australia\",\"Africa\", \"New Zealand\"))\n\nVisualize your AOI:\nWe will use TerraClimate, a dataset of high-spatial resolution (1/24°, ~4-km) monthly climate and climatic water balance for global terrestrial surfaces from 1958–2015 (Abatzoglou, 2018).\nDownload climate data using the library climateR for the AOI. For this exercise we will use climate normals, multi-decadal averages for climate variables like temperature and precipitation. They provide a baseline that allows us to understand the location’s average condition.\nYou can access monthly precipitation (“ppt”), monthly temperature minimum (“tmin”), monthly temperature maximum (“tmax”) climate normals and more.\nDownload monthly precipitation (ppt)\n\nnormals.ppt &lt;- aoi.global  %&gt;% getTerraClimNormals(varname =\"ppt\")\n\nWhat is this object:\n\nclass(normals.ppt)\n\n[1] \"list\"\n\nclass(normals.ppt$ppt)\n\n[1] \"SpatRaster\"\nattr(,\"package\")\n[1] \"terra\"\n\n\nA RasterStack is a collection of RasterLayer objects with the same spatial extent and resolution. In essence it is a list of RasterLayer objects.\nTo access the raster stack:\n\nnormals.ppt$ppt\n\nclass       : SpatRaster \ndimensions  : 3343, 8640, 12  (nrow, ncol, nlyr)\nresolution  : 0.04166667, 0.04166667  (x, y)\nextent      : -180, 180, -55.625, 83.66667  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=longlat +ellps=WGS84 +no_defs \nsource(s)   : memory\nnames       : ppt_1~12010, ppt_1~12010, ppt_1~12010, ppt_1~12010, ppt_1~12010, ppt_1~12010, ... \nmin values  :         0.0,         0.0,         0.0,         0.0,         0.0,         0.0, ... \nmax values  :       945.8,       994.4,       827.2,       974.5,      2383.5,      2470.6, ... \nunit        :          mm,          mm,          mm,          mm,          mm,          mm, ... \ntime        : 1961-01-01 to 1961-12-01 UTC \n\nnormals.ppt$ppt %&gt;% names()\n\n [1] \"ppt_1961-01-01_19812010\" \"ppt_1961-02-01_19812010\"\n [3] \"ppt_1961-03-01_19812010\" \"ppt_1961-04-01_19812010\"\n [5] \"ppt_1961-05-01_19812010\" \"ppt_1961-06-01_19812010\"\n [7] \"ppt_1961-07-01_19812010\" \"ppt_1961-08-01_19812010\"\n [9] \"ppt_1961-09-01_19812010\" \"ppt_1961-10-01_19812010\"\n[11] \"ppt_1961-11-01_19812010\" \"ppt_1961-12-01_19812010\"\n\nnormals.ppt$ppt %&gt;% time()\n\n [1] \"1961-01-01 UTC\" \"1961-02-01 UTC\" \"1961-03-01 UTC\" \"1961-04-01 UTC\"\n [5] \"1961-05-01 UTC\" \"1961-06-01 UTC\" \"1961-07-01 UTC\" \"1961-08-01 UTC\"\n [9] \"1961-09-01 UTC\" \"1961-10-01 UTC\" \"1961-11-01 UTC\" \"1961-12-01 UTC\"\n\nggplot() + geom_spatraster(data=normals.ppt$ppt[[1]])\n\n&lt;SpatRaster&gt; resampled to 500280 cells.\n\n\n\n\n\n\n\n\n\n\n\nRaster algebra\nMany generic functions that allow for simple and elegant raster algebra have been implemented for SpatRaster objects, including the normal algebraic operators such as +, -, *, /, logical operators such as &gt;, &gt;=, &lt;, ==, !} and functions such as abs, round, ceiling, floor, trunc, sqrt, log, log10, exp, cos, sin, max, min, range, prod, sum, any, all. In these functions you can mix terra objects with numbers, as long as the first argument is a terra object. If you use multiple SpatRaster objects, all objects must have the same resolution and origin.\nLets summarize monthly data to annual normals:\n\nnormals.ppt.annual &lt;- normals.ppt$ppt %&gt;% sum(na.rm = TRUE)\n\n# look at the object\nnormals.ppt.annual %&gt;% plot()\n\n\n\n\n\n\n\n# Check the name of the layers:\nnames(normals.ppt.annual)\n\n[1] \"sum\"\n\n# re-name the layers:\nnames(normals.ppt.annual) &lt;- \"ppt\"\n\nSummary functions (min, max, mean, prod, sum, median, cv, range, any, all) always return a SpatRaster object.\nUse global if instead of a SpatRaster you want a single number summarizing the cell values of each layer.\n\nnormals.ppt.annual %&gt;% global( na.rm=T, mean)\n\n        mean\nppt 703.3037\n\n\n\n\nSpatial Summaries\nYou might also find it useful to create zonal summaries for each polygon within the simple feature. To do this we can use the function zonal, which takes a SpatRast and a SpatVect.\n\nnormals.ppt.annual.country &lt;- zonal(x = normals.ppt.annual, \nz= vect(aoi.global) , fun = \"mean\", as.polygons=TRUE,  na.rm=TRUE)\n\nWhat did the function return?\n\nclass( normals.ppt.annual.country)\n\n[1] \"SpatVector\"\nattr(,\"package\")\n[1] \"terra\"\n\n\nConvert the SpatVect back to a simple feature and plot it.\n\nnormals.ppt.annual.country.sf &lt;- st_as_sf(normals.ppt.annual.country)   \n\nggplot( data=normals.ppt.annual.country.sf ) + geom_sf(aes(fill= ppt))\n\n\n\n\n\n\n\n\n\n\nExtracting information to a point file:\nFor this exercise you will use your FLUXNET.CH4 vector file you saved last week and the monthly ppt data downloaded (normals.ppt.NAmerica$ppt).\nImport your point file FLUXNET.ch4:\n\nFLUXNET.ch4 &lt;- st_read(dsn=\"Data\", layer=\"FLUXNET_CH4\")\n\nReading layer `FLUXNET_CH4' from data source \n  `/Users/ac3656/GitHub/EDS_course/data' using driver `ESRI Shapefile'\nSimple feature collection with 81 features and 10 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -18223020 ymin: -4162002 xmax: 19542570 ymax: 7939774\nProjected CRS: WGS 84 / World Equidistant Cylindrical\n\n\nEnsure both files have the same coordinate reference system (CRS):\n\nFLUXNET.ch4 \n\nSimple feature collection with 81 features and 10 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -18223020 ymin: -4162002 xmax: 19542570 ymax: 7939774\nProjected CRS: WGS 84 / World Equidistant Cylindrical\nFirst 10 features:\n   SITE_ID                   SITE_NA  FLUXNET2  FLUXNET_ LOCATIO IGBP  MAT\n1   AT-Neu                  Neustift CC-BY-4.0 CC-BY-4.0     970  GRA  6.5\n2   BR-Npw Northern Pantanal Wetland      &lt;NA&gt; CC-BY-4.0     120  WSA 24.9\n3   BW-Gum                      Guma      &lt;NA&gt; CC-BY-4.0     950  WET 21.0\n4   BW-Nxr                   Nxaraga      &lt;NA&gt; CC-BY-4.0     950  GRA 21.0\n5   CA-SCB          Scotty Creek Bog      &lt;NA&gt; CC-BY-4.0     280  WET -2.8\n6   CA-SCC    Scotty Creek Landscape      &lt;NA&gt; CC-BY-4.0     285  ENF -2.8\n7   CH-Cha                    Chamau CC-BY-4.0 CC-BY-4.0     393  GRA  9.5\n8   CH-Dav                     Davos CC-BY-4.0 CC-BY-4.0    1639  ENF  2.8\n9   CH-Oe2            Oensingen crop CC-BY-4.0 CC-BY-4.0     452  CRO  9.8\n10  CN-Hgu                  Hongyuan      &lt;NA&gt; CC-BY-4.0    3500  GRA  1.5\n      MAP  MnDstn_     Country                  geometry\n1   852.0 10829.38     Austria   POINT (1259858 5245007)\n2  1486.0 12208.78      Brazil POINT (-6279755 -1836549)\n3   460.0 13930.19    Botswana  POINT (2490339 -2111141)\n4   460.0 14000.38    Botswana  POINT (2580297 -2176085)\n5   388.0 11237.65      Canada POINT (-13502876 6824876)\n6   387.6 11237.65      Canada POINT (-13502965 6824764)\n7  1136.0 10719.05 Switzerland  POINT (936241.4 5255415)\n8  1062.0 10771.80 Switzerland   POINT (1097154 5211455)\n9  1155.0 10698.00 Switzerland  POINT (860911.5 5263898)\n10  747.0 17202.71       China  POINT (11420267 3656322)\n\nnormals.ppt$ppt\n\nclass       : SpatRaster \ndimensions  : 3343, 8640, 12  (nrow, ncol, nlyr)\nresolution  : 0.04166667, 0.04166667  (x, y)\nextent      : -180, 180, -55.625, 83.66667  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=longlat +ellps=WGS84 +no_defs \nsource(s)   : memory\nnames       : ppt_1~12010, ppt_1~12010, ppt_1~12010, ppt_1~12010, ppt_1~12010, ppt_1~12010, ... \nmin values  :         0.0,         0.0,         0.0,         0.0,         0.0,         0.0, ... \nmax values  :       945.8,       994.4,       827.2,       974.5,      2383.5,      2470.6, ... \nunit        :          mm,          mm,          mm,          mm,          mm,          mm, ... \ntime        : 1961-01-01 to 1961-12-01 UTC \n\n\nYou can transform one of the files if they dont match. Here I transform th vector:\n\nFLUXNET.ch4  &lt;- st_transform(FLUXNET.ch4, crs= crs(normals.ppt$ppt ))\n\nTo check to see if everything lines up, I plot the files together:\n\nggplot() + geom_spatraster( data=normals.ppt$ppt[[1]]) +geom_sf( data =FLUXNET.ch4 )\n\n&lt;SpatRaster&gt; resampled to 500280 cells.\n\n\n\n\n\n\n\n\n\nExtract information from your raster stack using terra::extract()\n\nFLUXNET.ch4.ppt &lt;-terra::extract( normals.ppt$ppt, FLUXNET.ch4)\n\nWhat did the extract function return?\n\nclass(FLUXNET.ch4.ppt)\n\n[1] \"data.frame\"\n\n\nCombine extracted information to the simple feature:\n\nFLUXNET.ch4.ppt.sf &lt;- FLUXNET.ch4 %&gt;% cbind(FLUXNET.ch4.ppt)\n\nVisualize your work:\n\nggplot()+  geom_sf(data = aoi.global) + geom_sf( data = FLUXNET.ch4.ppt.sf, aes( col= ppt_1961.09.01_19812010)) \n\n\n\n\n\n\n\n\nFLUXNET data can be used to understand patterns in natural methane fluxes. Evaluating the conditions where measurements are taken is essential to designing a useful model.",
    "crumbs": [
      "Introduction to R",
      "Raster Data"
    ]
  },
  {
    "objectID": "terra.html#download-monthly-temperature-tmin-and-tmax-to-understand-the-differences-in-temperature-for-the-tower-locations.-use-precipitation-and-temperature-summaries-in-assessment-23.",
    "href": "terra.html#download-monthly-temperature-tmin-and-tmax-to-understand-the-differences-in-temperature-for-the-tower-locations.-use-precipitation-and-temperature-summaries-in-assessment-23.",
    "title": "Introduction to rasters with the terra package",
    "section": "Download monthly temperature (tmin and tmax) to understand the differences in temperature for the tower locations. Use precipitation and temperature summaries in Assessment 2/3.",
    "text": "Download monthly temperature (tmin and tmax) to understand the differences in temperature for the tower locations. Use precipitation and temperature summaries in Assessment 2/3.",
    "crumbs": [
      "Introduction to R",
      "Raster Data"
    ]
  },
  {
    "objectID": "terra.html#assessment-23",
    "href": "terra.html#assessment-23",
    "title": "Introduction to rasters with the terra package",
    "section": "Assessment 2/3",
    "text": "Assessment 2/3\nWe will use data from FLUXNET CH4 to explore patterns in natural methane emissions. Explore the distribution of tower sites and create visualizations that may be helpful to understand in the design and development of models. You are welcome to use any additional data or just new plot types.\n\nReferences\nAbatzoglou, J., Dobrowski, S., Parks, S. et al. TerraClimate, a high-resolution global dataset of monthly climate and climatic water balance from 1958–2015. Sci Data 5, 170191 (2018). https://doi.org/10.1038/sdata.2017.191",
    "crumbs": [
      "Introduction to R",
      "Raster Data"
    ]
  },
  {
    "objectID": "sensitivity.html",
    "href": "sensitivity.html",
    "title": "Sensitivity Analysis",
    "section": "",
    "text": "A sensitivity analysis is a technique used to understand how changes in the inputs of a mathematical model or system affect the output. In the context of a model, such as a simulation or a statistical model, sensitivity analysis helps to identify which input parameters have the most significant influence on the results.\nTo begin load the model and dataset we will use for this workshop:\n\nload( 'SensitivityProducts.RDATA')\n\nWe will use the following packages:\n\nlibrary(randomForest)\nlibrary(tidyverse)\nlibrary(gtools)\nlibrary(ggplot2)\n\nIn this workshop, we will prepare a sensitivity analysis for the model FCH4_F_gC.rf. Take a look at the model:\n\nFCH4_F_gC.rf \n\n\nCall:\n randomForest(formula = FCH4_F_gC ~ P_F + TA_F + Upland, data = train,      keep.forest = T, importance = TRUE, mtry = 1, ntree = 500,      keep.inbag = TRUE) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n\n          Mean of squared residuals: 4.321792\n                    % Var explained: 35.76\n\n\nThe model includes monthly precipitation (P_f), mean temperature (TA_F), and a binary indicator for upland (1= upland ecosystem and 0 = aquatic ecosystem).\nExplore the conditions present within the dataset (fluxnet.new) used to build the model. First subset only the variables used in the final model:\n\nmodel.vars &lt;- fluxnet.new %&gt;% as.data.frame %&gt;%  select( P_F, TA_F,Upland)\n\nNext, we will summarise the conditions within the dataset by quantiles (0.25, 0.5, 0.75): NOTE THAT YOUR CATEGORICAL VARIABLE WILL NEED TO BE IN THE group_by() FUNCTION!\n\nmodel.vars.lower &lt;- model.vars %&gt;% group_by(Upland) %&gt;% summarise(P_F = quantile(P_F, 0.25),\n                                                                      TA_F = quantile(TA_F, 0.25 ),\n                                                                      Quantile = as.factor(0.25)) %&gt;% as.data.frame()\n\nmodel.vars.median &lt;- model.vars %&gt;% group_by(Upland) %&gt;% summarise(P_F = quantile(P_F, 0.5),\n                                                                   TA_F = quantile(TA_F, 0.5 ),\n                                                                   Quantile = as.factor(0.5)) %&gt;% as.data.frame()\n\nmodel.vars.upper &lt;- model.vars %&gt;% group_by(Upland) %&gt;% summarise(P_F = quantile(P_F, 0.75),\n                                                                   TA_F = quantile(TA_F, 0.75 ),\n                                                                   Quantile = as.factor(0.75)) %&gt;% as.data.frame()\n\nCombine the individual Summaries into one dataframe:\n\nsummary &lt;- smartbind( model.vars.lower, model.vars.median, model.vars.upper)\n\nNow, choose the variable you want to explore: TA_F\nLook at the conditions present witing the dataset for TA_F\n\nsummary(fluxnet.new$TA_F)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-37.188   1.814  10.595   8.874  17.124  31.280 \n\n\nLook at the range in values for TA_F:\n\nrange(fluxnet.new$TA_F)\n\n[1] -37.18842  31.28010\n\n\nAccess each individual range value:\n\nrange(fluxnet.new$TA_F)[1]\n\n[1] -37.18842\n\nrange(fluxnet.new$TA_F)[2]\n\n[1] 31.2801\n\n\nUse the range to generate a sequence of values going from the highest to lowest:\n\nTA_F.seq &lt;- seq(range(fluxnet.new$TA_F)[1], range(fluxnet.new$TA_F)[2], by=5 )\n\nCreate a dataframe:\n\nTA_F.seq.df &lt;- data.frame(TA_F = TA_F.seq )\n\nNext we will take the summary, remove the values for TA_F and replace is with the generated range:\n\nTA_F_1AT &lt;- summary %&gt;% select(-TA_F) %&gt;% cross_join(TA_F.seq.df)\n\nIn this dataframe, all other variables in the model are held at their quantile values and only temperature varies across the range of values observed. This approach is “one-at-a-time”. We choose one variable to vary and hold all others at a specific value. Often the mean is chosen but using the 0.25 , 0.5, and 0.75 quantile helps to visualize what the model would predict across conditions.\nNext, use the predict() function to predict values into the dataframe (TA_F_1AT) with the simulated conditions:\n\nTA_F_1AT$PRED &lt;-predict(FCH4_F_gC.rf , newdata =  TA_F_1AT)\n\nLets look at the predictions.\n\nggplot() + geom_point(data =TA_F_1AT , aes( x=TA_F, y=PRED) )\n\n\n\n\n\n\n\n\nUse geom_smooth to visualize the general relationship:\n\nggplot() + geom_smooth(data =TA_F_1AT , aes( x=TA_F, y=PRED) )\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nLook at the predictions by Quantile and by the Upland indicator:\n\nggplot() + geom_line(data =TA_F_1AT , aes( x=TA_F, y=PRED, color=Quantile, group=interaction(Quantile, Upland), linetype =Upland)  )\n\n\n\n\n\n\n\n\nNow explore the next variable: PA_f\n\nsummary(fluxnet.new$P_F)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00   15.49   43.31   69.37   82.85 1094.02 \n\nrange(fluxnet.new$P_F)\n\n[1]    0.000 1094.017\n\nrange(fluxnet.new$P_F)[1]\n\n[1] 0\n\nrange(fluxnet.new$P_F)[2]\n\n[1] 1094.017\n\nP_F.seq &lt;- seq(range(fluxnet.new$P_F)[1], range(fluxnet.new$P_F)[2], by=100 )\n\nP_F.seq.df &lt;- data.frame(P_F = P_F.seq )\n\nP_F_1AT &lt;- summary %&gt;% select(-P_F) %&gt;% cross_join(P_F.seq.df)\n\nP_F_1AT$PRED &lt;- predict(FCH4_F_gC.rf , newdata =  P_F_1AT)\n\nggplot() + geom_point(data =P_F_1AT , aes( x=P_F, y=PRED) )\n\n\n\n\n\n\n\nggplot() + geom_smooth(data =P_F_1AT , aes( x=P_F, y=PRED) )\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\nggplot() + geom_line(data =P_F_1AT , aes( x=P_F, y=PRED, color=Quantile, group=interaction(Quantile, Upland), linetype =Upland)  )\n\n\n\n\n\n\n\n\nUse this same workflow to explore the sensitivity of your groups project’s model. Please provide a presentation explaining how you fit your model (mtry?, ntree?) and how variables where selected (forward versus backward selection). Describe your final model results (variables in the final model, their importance, the %Var, observed versus predicted for testing and training data), including a correlation plot, variance importance plots, and sensitivity analyses. This report will develop into the methods and results portion of your final project.",
    "crumbs": [
      "Machine Learning in R",
      "Sensitivity Analysis"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ENV 730",
    "section": "",
    "text": "T 2:30pm-5:20pm in Sage 24",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#meeting-information",
    "href": "index.html#meeting-information",
    "title": "ENV 730",
    "section": "",
    "text": "T 2:30pm-5:20pm in Sage 24",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#instructor",
    "href": "index.html#instructor",
    "title": "ENV 730",
    "section": "Instructor",
    "text": "Instructor\nDr. Sparkle L. Malone\nsparkle.malone@yale.edu\nOffice Hours: TBD\nTeaching Fellow: TBD",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#limited-enrollment",
    "href": "index.html#limited-enrollment",
    "title": "ENV 730",
    "section": "Limited Enrollment",
    "text": "Limited Enrollment\nPrerequisites:\n\nR Series Training",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#format",
    "href": "index.html#format",
    "title": "ENV 730",
    "section": "Format",
    "text": "Format\nThe primary format is synchronous, in-person lectures, discussions, and workshops.\nCourse Description: In today’s world, understanding environmental data and making informed decisions based on it is crucial for addressing complex environmental challenges. This course serves as an introductory exploration into the integration of environmental data using R programming language, coupled with machine learning techniques. Participants will gain hands-on experience in handling, analyzing, and interpreting environmental datasets, with a focus on leveraging the power of R for data integration and predictive modeling.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#learning-objectives",
    "href": "index.html#learning-objectives",
    "title": "ENV 730",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nWe will use coding and reproducible best practices.\nWe will become familiar with different types of environmental data.\nWe will use machine learning to explore adaptive management solutions.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#required-materials",
    "href": "index.html#required-materials",
    "title": "ENV 730",
    "section": "Required Materials",
    "text": "Required Materials\n\nR for Data Science\nR For Earth System\nSpatial Data Science with R and “terra”\n\nComputing needs: Please bring a laptop to class. Your laptop must have both R and Rstudio installed and operational prior to the start of this course. Ideally you will have 16GB of RAM or higher, dual Core 2Ghz or higher (Intel Core i5 processor or equivalent) and your operating system should be Windows 10 or higher or OS X 10.14 or higher.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#course-policies",
    "href": "index.html#course-policies",
    "title": "ENV 730",
    "section": "Course Policies",
    "text": "Course Policies\nAttendance Policy: Regular in-person classroom attendance is required of all students. When extenuating circumstances require accommodation, Dr. Malone will arrange for remote attendance via zoom.\nLate Policy: All assignments submitted after the due date are subject to a late penalty. For the first 24 hour period beyond an assignment’s due date there is a 10% penalty, which increases by 10% for each additional 24 hour period.\nAcademic integrity is a core university value that ensures respect for the academic reputation of the University, its students, faculty and staff, and the degrees it confers. The University expects that students will conduct themselves in an honest and ethical manner and respect the intellectual work of others. Please ask about my expectations regarding permissible or encouraged forms of student collaboration if they are unclear. Any work that you submit at any stage of the writing process—thesis, outline, draft, bibliography, final submission, presentations, blog posts, and more—must be your own; you also may not use material generated by ChatGPT or any other AI writing software. In addition, any words, ideas, or data that you borrow from other people and include in your work must be properly documented. Failure to do either of these things is plagiarism. -Poorvu Center\nBefore collaborating with an AI chatbot on your work for this course, please request permission by sending me a note that describes (a) how you intend to use the tool and (b) how using it will enhance your learning. Any use of AI to complete an assignment must be acknowledged in a citation that includes the prompt you submitted to the bot, the date of access, and the URL of the program. -Poorvu Center\nDiversity and Disability Statement: Our institution values diversity and inclusion; we are committed to a climate of mutual respect and full participation. Our goal is to create learning environments that are usable, equitable, inclusive and welcoming. If there are aspects of the instruction or design of this course that result in barriers to your inclusion or accurate assessment or achievement, please notify Dr. Malone as soon as possible. Disabled students are also welcome to contact Student Accessibility ServicesLinks to an external site. to discuss a range of options to removing barriers in the course, including accommodations. -Poorvu Center\nUsability, Disability and Design: I am committed to creating a course that is inclusive in its design. If you encounter barriers, please let me know immediately so that we can determine if there is a design adjustment that can be made or if an accommodation might be needed to overcome the limitations of the design. I will consider creative solutions, as long as they do not compromise the intent of the assessment or learning activity. You are also welcome to contact Student Accessibility ServicesLinks to an external site. to begin this conversation or to establish accommodations for this or other courses. I welcome feedback that will assist me in improving the usability and experience for all students. -Poorvu Center",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#assignments-assessments-grading",
    "href": "index.html#assignments-assessments-grading",
    "title": "ENV 730",
    "section": "Assignments, Assessments & Grading",
    "text": "Assignments, Assessments & Grading\n\nExam (125 points): This exam is designed to evaluate basic knowledge of R and Rstudio. A score of 90% or higher signifies that students have a strong foundational knowledge of R and R studio and are prepared to focus on the content in this course.\nAttendance: Regular in-person classroom attendance is required of all students. When extenuating circumstances require accommodation, Dr. Malone will arrange for remote attendance via zoom.\nPost-workshop Assessments (~150 points): Assessments are assignments that require direct application of tools and analysis approaches.\nMajor Assessments (200 points): Activities designed to test mastery of concepts.\nFinal Project (200 points):\n\nProposal Presentation (50 points)\nFinal Report (100 points)\nFinal Project Presentation (50 points)",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#course-ouline",
    "href": "index.html#course-ouline",
    "title": "ENV 730",
    "section": "Course Ouline:",
    "text": "Course Ouline:\n\n\n\n\n\n\n\nWeek\nWorkshop Topic\n\n\n\n\n1-5\nIntroduction to R\n\nOpen Science Best Practices\nData Basics (Importing, visualizing, manipulating data)\nVector Data Manipulation & Visualization (sf)\nRaster Data Manipulation & Visualization (terra)\n\n\n\n6-7\nData Integration in R\n\nTechniques for Data Integration\n\n\n\n7-10\nMachine Learning in R\n\nRandomForest Model Development: Fitting models\nRandomForest Model Development: Validating models\nRandomForest Model Development: Sensitivity Analysis\n\n\n\n11-12\nDynamic Models in R\n\nSpatial Model Visualization\nModel Competition\n\n\n\n13-16\nFinal Project (Independent)\n\nProposal Presentation\nFinal Report\nFinal Project Presentation",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "random_forest.html",
    "href": "random_forest.html",
    "title": "Introduction to Machine Learning with RandomForest",
    "section": "",
    "text": "Install Packages:\n\ninstall.packages(\"randomForest\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"GGally\")\n\n\n\nLoad Libraries:\n\nlibrary(randomForest)\nlibrary(tidyverse)\nlibrary(GGally)\n\nBoosted regression trees (BRT) represent a versatile machine learning technique applicable to both classification and regression tasks. This approach facilitates the assessment of the relative significance of numerous variables associated with a target response variable. In this workshop, our focus will be on utilizing BRT to develop a model for monthly methane fluxes originating from natural ecosystems. We’ll leverage climate and moisture conditions within these ecosystems to enhance predictive accuracy and understanding.\nRead in the data:\n\nload('RANDOMFOREST_DATASET.RDATA' )\n\nOur ultimate interest is in predicting monthly methane fluxes using both dynamic and static attribute of ecosystems. Before we start modeling with the data, it is a good practice to first visualize the variables. The ggpairs() function from the GGally package is a useful tool that visualizes the distribution and correlation between variables:\n\nggpairs(fluxnet, columns = c(3:7, 12:13))\n\n\n\n\n\n\n\n\nNext we need to divide the data into testing (20%) and training (80%) sets in a reproducible way:\n\nset.seed(111) # set the randomnumber generator\n\n#create ID column\nfluxnet$id &lt;- 1:nrow(fluxnet)\n\n#use 80% of dataset as training set and 30% as test set \ntrain &lt;- fluxnet %&gt;% dplyr::sample_frac(0.80)\ntest  &lt;- dplyr::anti_join(fluxnet, train, by = 'id')\n\nWe will use the randomForest() function to predict monthly natural methane efflux using several variables in the dataset. A few other key statements to use in the randomForest() function are:\n\nkeep.forest = T: This will save the random forest output, which will be helpful in summarizing the results.\nimportance = TRUE: This will assess the importance of each of the predictors, essential output in random forests.\nmtry = 1: This tells the function to randomly sample one variable at each split in the random forest. For applications in regression, the default value is the number of predictor variables divided by three (and rounded down). In the modeling, several small samples of the entire data set are taken. Any observations that are not taken are called “out-of-bag” samples.\nntree = 500: This tells the function to grow 500 trees. Generally, a larger number of trees will produce more stable estimates. However, increasing the number of trees needs to be done with consideration of time and memory issues when dealing with large data sets.\n\nOur response variable in the random forests model is FCH4_F_gC and predictors are P_F, TA_F, VPD_F, IGBP, NDVI, and EVI. We will only explore a few of these variables below:\n\nFCH4_F_gC.rf &lt;- randomForest(FCH4_F_gC ~ P_F + TA_F + VPD_F ,\n                        data = train,\n                        keep.forest = T,\n                        importance = TRUE, \n                        mtry = 1,\n                        ntree = 500,\n                        keep.inbag=TRUE)\nFCH4_F_gC.rf\n\n\nCall:\n randomForest(formula = FCH4_F_gC ~ P_F + TA_F + VPD_F, data = train,      keep.forest = T, importance = TRUE, mtry = 1, ntree = 500,      keep.inbag = TRUE) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n\n          Mean of squared residuals: 4.887725\n                    % Var explained: 22.83\n\n\nNote the mean of squared residuals and the percent variation explained (analogous to R-squared) provided in the output.\nVisualize the out-of-bag error rates of the random forests models using the plot() function. In this application, although we specified 500 trees, the out-of-bag error generally stabilizes after 100 trees:\n\nplot(FCH4_F_gC.rf)\n\n\n\n\n\n\n\n\nSome of the most helpful output in random forests is the importance of each of the predictor variables. The importance score is calculated by evaluating the regression tree with and without that variable. When evaluating the regression tree, the mean square error (MSE) will go up, down, or stay the same.\nIf the percent increase in MSE after removing the variable is large, it indicates an important variable. If the percent increase in MSE after removing the variable is small, it’s less important.\nThe importance() function prints the importance scores for each variable and the varImpPlot() function plots them:\n\nimportance(FCH4_F_gC.rf)\n\n         %IncMSE IncNodePurity\nP_F   11.7269452      2762.568\nTA_F  29.1106456      4454.298\nVPD_F -0.7084245      3396.256\n\nvarImpPlot(FCH4_F_gC.rf)\n\n\n\n\n\n\n\n\nAnother aspect of model evaluation is comparing predictions. Although random forests models are often considered a “black box” method because their results are not easily interpreted, the predict() function provides predictions of total tree mass:\n\ntrain$PRED.TPVPD &lt;- predict(FCH4_F_gC.rf, train)\n\nCompare the observed (FCH4_F_gC) versus predicted (PRED.TPVPD):\n\nggplot() + geom_point( data = train, aes( x=FCH4_F_gC, y= PRED.TPVPD )) +\n  geom_smooth(method='lm')\n\n\n\n\n\n\n\nsummary(lm(data=train,  PRED.TPVPD~FCH4_F_gC))\n\n\nCall:\nlm(formula = PRED.TPVPD ~ FCH4_F_gC, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8501 -0.3881 -0.2159  0.2347  4.1151 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.519090   0.015808   32.84   &lt;2e-16 ***\nFCH4_F_gC   0.639029   0.005478  116.65   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6003 on 1894 degrees of freedom\nMultiple R-squared:  0.8778,    Adjusted R-squared:  0.8777 \nF-statistic: 1.361e+04 on 1 and 1894 DF,  p-value: &lt; 2.2e-16\n\n\nSee how well the model performs on data that was not used to train the model:\n\ntest$PRED.TPVPD &lt;- predict(FCH4_F_gC.rf, test)\n  \nggplot() + geom_point( data = test, aes( x=FCH4_F_gC, y= PRED.TPVPD )) +\n  geom_smooth(method='lm')\n\n\n\n\n\n\n\nsummary(lm(data=test,  PRED.TPVPD~FCH4_F_gC))\n\n\nCall:\nlm(formula = PRED.TPVPD ~ FCH4_F_gC, data = test)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.6482 -0.8116 -0.4033  0.4567  5.9730 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.06617    0.06553   16.27   &lt;2e-16 ***\nFCH4_F_gC    0.32516    0.02412   13.48   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.235 on 464 degrees of freedom\nMultiple R-squared:  0.2814,    Adjusted R-squared:  0.2799 \nF-statistic: 181.7 on 1 and 464 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nFinal Model Development:\nThe current model includes only climate variables from the tower. Use either a forward or backward selection method to develop your final model using your own data sets.\nThe forward selection approach starts with no variables and adds each new variable incrementally, testing for statistical significance, while the backward elimination method begins with a full model and then removes the least statistically significant variables one at a time.\nSave your final model and datasets in a .Rdata object for next class where we will perform sensitivity analyses on the models.\n\nsave( FCH4_F_gC.rf , file=\"FinalModel.RDATA\")",
    "crumbs": [
      "Machine Learning in R",
      "Random Forest"
    ]
  },
  {
    "objectID": "data_repos.html",
    "href": "data_repos.html",
    "title": "Additional Resources: Data Repositories",
    "section": "",
    "text": "Here is a partial list of data repositories where you can find datasets on nearly every subject in environmental science. This list is credited to the LTER Network’s Synthesis Skills for Early Career Researchers (SSECR) course and Ecological Data Synthesis: A Primer on Essential Methods course. The data repository’s associated R package is listed as well. The R packages can be installed from either CRAN or GitHub.\n\n\n\nName\nDescription\nR Package\n\n\n\n\nAmeriFlux\nProvides data on carbon, water, and energy fluxes in ecosystems across the Americas, aiding in climate change and carbon cycle research.\namerifluxr\n\n\nDataONE\nA network of around 60 data repositories. Aggregates environmental and ecological data from global sources, focusing on biodiversity, climate, and ecosystem research.\ndataone\n\n\nDaymet\nDaymet provides long-term, continuous, gridded estimates of daily weather and climatology variables for North America.\ndaymetr\n\n\nEDI\nContains a wide range of ecological and environmental datasets, including long-term observational data, experimental results, and field studies from diverse ecosystems.\nEDIutils\n\n\nEES-DIVE\nThe Environmental System Science Data Infrastructure for a Virtual Ecosystem (ESS-DIVE) includes a variety of observational, experimental, modeling and other data products from a wide range of ecological and urban systems.\n–\n\n\nGBIF\nThe Global Biodiversity Information Facility (GBIF) aggregates global species occurrence data and biodiversity records, supporting research in species distribution and conservation.\nrgbif\n\n\nGoogle Earth Engine\nGoogle Earth Engine is a cloud-based geospatial analysis platform that provides access to vast amounts of satellite imagery and environmental data for monitoring and understanding changes in the Earth’s surface.\nrgee\n\n\nKNB\nAn open-source data repository hosting international ecological and environmental research. A member of the DataOne network.\ndataone\n\n\nMicrosoft Planetary Computer\nThe Microsoft Planetary Computer is a cloud-based platform that combines global environmental datasets with advanced analytical tools to support sustainability and ecological research.\nrstac\n\n\nNASA\nProvides data on earth science, space exploration, and climate, including satellite imagery and observational data for both terrestrial and extraterrestrial studies. Nice GUI-based data download via AppEEARS.\nnasadata\n\n\nNCBI\nHosts genomic and biological data, including DNA, RNA, and protein sequences, supporting genomics and molecular biology research.\nrentrez\n\n\nNEON\nProvides ecological data from U.S. field sites, covering biodiversity, ecosystems, and environmental changes, supporting large-scale ecological research.\nneonUtilities\n\n\nNOAA\nOffers meteorological, oceanographic, and climate data, essential for understanding atmospheric conditions, marine environments, and long-term climate trends.\nGitHub: EpiNOAA-R\n\n\nOpen Traits Network\nWhile not a repository per se, the Open Traits Network has compiled an extensive lists of repositories for trait data. Check out their repository inventory for trait data\n–\n\n\nPhenoCam Network\nA network of digital cameras that tracks vegetation phenology through images across North America and around the world.\nphenocamapi\n\n\nUS Census Bureau\nCensus data containing information about education, employment, health, and housing across America.\ntidycensus\n\n\nUSGS\nHosts data on geology, hydrology, biology, and geography, including topographical maps and natural resource assessments.\ndataRetrieval\n\n\nUS National Park Service\nProvides geospatial and tabular data products, which includes national park boundaries, vegetation maps, geology maps, air quality, and water quality.\nGitHub: NPSutils",
    "crumbs": [
      "Resources",
      "Data Repositories"
    ]
  }
]