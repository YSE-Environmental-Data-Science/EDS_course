[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Additional Resources: Coding",
    "section": "",
    "text": "Here are some additional resources that serve as a good starting point to learning more about the different types of tools involved in environmental data science. All listed resources are beginner-friendly so please feel free to check them out.",
    "crumbs": [
      "Resources",
      "Coding"
    ]
  },
  {
    "objectID": "resources.html#gitgithub",
    "href": "resources.html#gitgithub",
    "title": "Additional Resources: Coding",
    "section": "Git/GitHub",
    "text": "Git/GitHub\n\nCollaborative Coding with GitHub: An introductory workshop on how to use Git with GitHub and RStudio. This workshop was created by the LTER Network for their synthesis working groups but remains a great resource for all beginners, especially those who are hesitant to use the command line. There are also lessons on advanced Git/GitHub topics for those looking to further improve their skills.\nReproducible Approaches to Arctic Research Using R: A course designed by the NCEAS Learning Hub to teach researchers techniques and tools to make their work more reproducible. Particularly relevant are the Introduction to Git and GitHub and Collaborating with Git and GitHub lessons, where the Git workflow and Git commands are explained thoroughly.\nHappy Git and GitHub for the userR: A comprehensive guide by Jenny Bryan on using Git, GitHub, and RStudio that covers everything from the basics to the more intricate topics. This book has a dedicated page for troubleshooting common problems a user might run into while setting up their computer.",
    "crumbs": [
      "Resources",
      "Coding"
    ]
  },
  {
    "objectID": "resources.html#tidyverse",
    "href": "resources.html#tidyverse",
    "title": "Additional Resources: Coding",
    "section": "Tidyverse",
    "text": "Tidyverse\n\nCoding in the Tidyverse: A workshop on using dplyr, tidyr, and ggplot2, which are R packages that are part of the Tidyverse. The workshop covers the fundamental tools for tidying data in R, for example, wrangling, summarizing, reshaping, joining, and visualizing data. This workshop was created by the LTER Network for their synthesis working groups.\nReproducible Approaches to Arctic Research Using R: A course designed by the NCEAS Learning Hub to teach researchers techniques and tools to make their work more reproducible. Particularly relevant are the Data Modeling Essentials and Cleaning and Wrangling Data lessons, where the principles of tidy data and basic Tidyverse functions are explained.\nR for Data Science: An online textbook by written by Hadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund that goes over the entire Tidyverse workflow from start to finish.\nPosit Cheatsheets: A collection of cheatsheets created by Posit (formerly RStudio Inc.) to serve as helpful reminders on how to use the most important functions from a wide array of R packages. Here are the cheatsheets for dplyr, tidyr, and ggplot2.",
    "crumbs": [
      "Resources",
      "Coding"
    ]
  },
  {
    "objectID": "resources.html#spatial-data",
    "href": "resources.html#spatial-data",
    "title": "Additional Resources: Coding",
    "section": "Spatial Data",
    "text": "Spatial Data\n\nIntroduction to Geospatial Concepts: A workshop from the Data Carpentry on core geospatial concepts such as raster data, vector data, and coordinate reference systems. This workshop is a prerequisite to their Introduction to Geospatial Raster and Vector Data with R workshop and is a great resource for reviewing the different kinds of geospatial data.\nIntroduction to Geospatial Raster and Vector Data with R: A workshop from the Data Carpentry that focuses on manipulating and plotting raster and vector data using the terra, sf, and ggplot2 R packages. The functions and operations involved in a basic geospatial data workflow are taught in this workshop.\nNCEAS coreR for Delta Science Program: A course designed by the NCEAS Learning Hub to teach researchers techniques and tools to make their work more reproducible. Particularly relevant is the Working with Spatial Data lesson that works with the sf, dplyr, ggplot2, and leaflet R packages to manipulate and visualize vector data by following the Tidyverse principles. Interactive maps are also covered.\nSpatial manipulation with sf cheatsheet: A handy cheatsheet that serves as a reminder on how to use common functions from the sf R package.\nSpatial data with terra: A tutorial showcasing functions from the terra R package to manipulate both raster and vector geospatial data. The tutorial was written by the team behind the development of terra.",
    "crumbs": [
      "Resources",
      "Coding"
    ]
  },
  {
    "objectID": "data_repos.html",
    "href": "data_repos.html",
    "title": "Additional Resources: Data Repositories",
    "section": "",
    "text": "Here is a partial list of data repositories where you can find datasets on nearly every subject in environmental science. This list is credited to the LTER Network’s Synthesis Skills for Early Career Researchers (SSECR) course and Ecological Data Synthesis: A Primer on Essential Methods course. The data repository’s associated R package is listed as well. The R packages can be installed from either CRAN or GitHub.\n\n\n\nName\nDescription\nR Package\n\n\n\n\nAmeriFlux\nProvides data on carbon, water, and energy fluxes in ecosystems across the Americas, aiding in climate change and carbon cycle research.\namerifluxr\n\n\nDataONE\nA network of around 60 data repositories. Aggregates environmental and ecological data from global sources, focusing on biodiversity, climate, and ecosystem research.\ndataone\n\n\nDaymet\nDaymet provides long-term, continuous, gridded estimates of daily weather and climatology variables for North America.\ndaymetr\n\n\nEDI\nContains a wide range of ecological and environmental datasets, including long-term observational data, experimental results, and field studies from diverse ecosystems.\nEDIutils\n\n\nEES-DIVE\nThe Environmental System Science Data Infrastructure for a Virtual Ecosystem (ESS-DIVE) includes a variety of observational, experimental, modeling and other data products from a wide range of ecological and urban systems.\n–\n\n\nGBIF\nThe Global Biodiversity Information Facility (GBIF) aggregates global species occurrence data and biodiversity records, supporting research in species distribution and conservation.\nrgbif\n\n\nGoogle Earth Engine\nGoogle Earth Engine is a cloud-based geospatial analysis platform that provides access to vast amounts of satellite imagery and environmental data for monitoring and understanding changes in the Earth’s surface.\nrgee\n\n\nKNB\nAn open-source data repository hosting international ecological and environmental research. A member of the DataOne network.\ndataone\n\n\nMicrosoft Planetary Computer\nThe Microsoft Planetary Computer is a cloud-based platform that combines global environmental datasets with advanced analytical tools to support sustainability and ecological research.\nrstac\n\n\nNASA\nProvides data on earth science, space exploration, and climate, including satellite imagery and observational data for both terrestrial and extraterrestrial studies. Nice GUI-based data download via AppEEARS.\nnasadata\n\n\nNCBI\nHosts genomic and biological data, including DNA, RNA, and protein sequences, supporting genomics and molecular biology research.\nrentrez\n\n\nNEON\nProvides ecological data from U.S. field sites, covering biodiversity, ecosystems, and environmental changes, supporting large-scale ecological research.\nneonUtilities\n\n\nNOAA\nOffers meteorological, oceanographic, and climate data, essential for understanding atmospheric conditions, marine environments, and long-term climate trends.\nGitHub: EpiNOAA-R\n\n\nOpen Traits Network\nWhile not a repository per se, the Open Traits Network has compiled an extensive lists of repositories for trait data. Check out their repository inventory for trait data\n–\n\n\nPhenoCam Network\nA network of digital cameras that tracks vegetation phenology through images across North America and around the world.\nphenocamapi\n\n\nUS Census Bureau\nCensus data containing information about education, employment, health, and housing across America.\ntidycensus\n\n\nUSGS\nHosts data on geology, hydrology, biology, and geography, including topographical maps and natural resource assessments.\ndataRetrieval\n\n\nUS National Park Service\nProvides geospatial and tabular data products, which includes national park boundaries, vegetation maps, geology maps, air quality, and water quality.\nGitHub: NPSutils",
    "crumbs": [
      "Resources",
      "Data Repositories"
    ]
  },
  {
    "objectID": "random_forest.html",
    "href": "random_forest.html",
    "title": "Introduction to Machine Learning with RandomForest",
    "section": "",
    "text": "Install Packages:\n\ninstall.packages(\"randomForest\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"GGally\")\n\n\n\nLoad Libraries:\n\nlibrary(randomForest)\nlibrary(tidyverse)\nlibrary(GGally)\n\nBoosted regression trees (BRT) represent a versatile machine learning technique applicable to both classification and regression tasks. This approach facilitates the assessment of the relative significance of numerous variables associated with a target response variable. In this workshop, our focus will be on utilizing BRT to develop a model for monthly methane fluxes originating from natural ecosystems. We’ll leverage climate and moisture conditions within these ecosystems to enhance predictive accuracy and understanding.\nRead in the data:\n\nload('data/RANDOMFOREST_DATASET.RDATA' )\n\nOur ultimate interest is in predicting monthly methane fluxes using both dynamic and static attribute of ecosystems. Before we start modeling with the data, it is a good practice to first visualize the variables. The ggpairs() function from the GGally package is a useful tool that visualizes the distribution and correlation between variables:\n\nggpairs(fluxnet, columns = c(3:7, 12:13))\n\n\n\n\n\n\n\n\nNext we need to divide the data into testing (20%) and training (80%) sets in a reproducible way:\n\nset.seed(111) # set the randomnumber generator\n\n#create ID column\nfluxnet$id &lt;- 1:nrow(fluxnet)\n\n#use 80% of dataset as training set and 30% as test set \ntrain &lt;- fluxnet %&gt;% dplyr::sample_frac(0.80)\ntest  &lt;- dplyr::anti_join(fluxnet, train, by = 'id')\n\nWe will use the randomForest() function to predict monthly natural methane efflux using several variables in the dataset. A few other key statements to use in the randomForest() function are:\n\nkeep.forest = T: This will save the random forest output, which will be helpful in summarizing the results.\nimportance = TRUE: This will assess the importance of each of the predictors, essential output in random forests.\nmtry = 1: This tells the function to randomly sample one variable at each split in the random forest. For applications in regression, the default value is the number of predictor variables divided by three (and rounded down). In the modeling, several small samples of the entire data set are taken. Any observations that are not taken are called “out-of-bag” samples.\nntree = 500: This tells the function to grow 500 trees. Generally, a larger number of trees will produce more stable estimates. However, increasing the number of trees needs to be done with consideration of time and memory issues when dealing with large data sets.\n\nOur response variable in the random forests model is FCH4_F_gC and predictors are P_F, TA_F, VPD_F, IGBP, NDVI, and EVI. We will only explore a few of these variables below:\n\nFCH4_F_gC.rf &lt;- randomForest(FCH4_F_gC ~ P_F + TA_F + VPD_F ,\n                        data = train,\n                        keep.forest = T,\n                        importance = TRUE, \n                        mtry = 1,\n                        ntree = 500,\n                        keep.inbag=TRUE)\nFCH4_F_gC.rf\n\n\nCall:\n randomForest(formula = FCH4_F_gC ~ P_F + TA_F + VPD_F, data = train,      keep.forest = T, importance = TRUE, mtry = 1, ntree = 500,      keep.inbag = TRUE) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n\n          Mean of squared residuals: 4.887725\n                    % Var explained: 22.83\n\n\nNote the mean of squared residuals and the percent variation explained (analogous to R-squared) provided in the output.\nVisualize the out-of-bag error rates of the random forests models using the plot() function. In this application, although we specified 500 trees, the out-of-bag error generally stabilizes after 100 trees:\n\nplot(FCH4_F_gC.rf)\n\n\n\n\n\n\n\n\nSome of the most helpful output in random forests is the importance of each of the predictor variables. The importance score is calculated by evaluating the regression tree with and without that variable. When evaluating the regression tree, the mean square error (MSE) will go up, down, or stay the same.\nIf the percent increase in MSE after removing the variable is large, it indicates an important variable. If the percent increase in MSE after removing the variable is small, it’s less important.\nThe importance() function prints the importance scores for each variable and the varImpPlot() function plots them:\n\nimportance(FCH4_F_gC.rf)\n\n         %IncMSE IncNodePurity\nP_F   11.7269452      2762.568\nTA_F  29.1106456      4454.298\nVPD_F -0.7084245      3396.256\n\nvarImpPlot(FCH4_F_gC.rf)\n\n\n\n\n\n\n\n\nAnother aspect of model evaluation is comparing predictions. Although random forests models are often considered a “black box” method because their results are not easily interpreted, the predict() function provides predictions of total tree mass:\n\ntrain$PRED.TPVPD &lt;- predict(FCH4_F_gC.rf, train)\nhead(train$PRED.TPVPD)\n\n        1         2         3         4         5         6 \n0.3509667 0.4292789 0.3139104 0.4189123 1.1068833 2.2023186 \n\n\nCompare the observed (FCH4_F_gC) versus predicted (PRED.TPVPD):\n\nggplot() + geom_point( data = train, aes( x=FCH4_F_gC, y= PRED.TPVPD )) +\n  geom_smooth(method='lm')\n\n\n\n\n\n\n\nsummary(lm(data=train,  PRED.TPVPD~FCH4_F_gC))\n\n\nCall:\nlm(formula = PRED.TPVPD ~ FCH4_F_gC, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8501 -0.3881 -0.2159  0.2347  4.1151 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 0.519090   0.015808   32.84   &lt;2e-16 ***\nFCH4_F_gC   0.639029   0.005478  116.65   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6003 on 1894 degrees of freedom\nMultiple R-squared:  0.8778,    Adjusted R-squared:  0.8777 \nF-statistic: 1.361e+04 on 1 and 1894 DF,  p-value: &lt; 2.2e-16\n\n\nSee how well the model performs on data that was not used to train the model:\n\ntest$PRED.TPVPD &lt;- predict(FCH4_F_gC.rf, test)\n  \nggplot() + geom_point( data = test, aes( x=FCH4_F_gC, y= PRED.TPVPD )) +\n  geom_smooth(method='lm')\n\n\n\n\n\n\n\nsummary(lm(data=test,  PRED.TPVPD~FCH4_F_gC))\n\n\nCall:\nlm(formula = PRED.TPVPD ~ FCH4_F_gC, data = test)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.6482 -0.8116 -0.4033  0.4567  5.9730 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.06617    0.06553   16.27   &lt;2e-16 ***\nFCH4_F_gC    0.32516    0.02412   13.48   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.235 on 464 degrees of freedom\nMultiple R-squared:  0.2814,    Adjusted R-squared:  0.2799 \nF-statistic: 181.7 on 1 and 464 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nFinal Model Development:\nThe current model includes only climate variables from the tower. Use either a forward or backward selection method to develop your final model using your own data sets.\nThe forward selection approach starts with no variables and adds each new variable incrementally, testing for statistical significance, while the backward elimination method begins with a full model and then removes the least statistically significant variables one at a time.\nSave your final model and datasets in a .Rdata object for next class where we will perform sensitivity analyses on the models.\n\nsave( FCH4_F_gC.rf , file=\"data/products/FinalModel.RDATA\")",
    "crumbs": [
      "Machine Learning in R",
      "Random Forest"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ENV 730",
    "section": "",
    "text": "Th 2:30pm-5:20pm in Kroon 319",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#meeting-information",
    "href": "index.html#meeting-information",
    "title": "ENV 730",
    "section": "",
    "text": "Th 2:30pm-5:20pm in Kroon 319",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#instructor",
    "href": "index.html#instructor",
    "title": "ENV 730",
    "section": "Instructor",
    "text": "Instructor\nDr. Sparkle L. Malone\nsparkle.malone@yale.edu\nOffice Hours: Mondays 12pm-3pm\nTeaching Fellow: TBD",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#limited-enrollment",
    "href": "index.html#limited-enrollment",
    "title": "ENV 730",
    "section": "Limited Enrollment",
    "text": "Limited Enrollment\nPrerequisites:\n\nR Series Training",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#format",
    "href": "index.html#format",
    "title": "ENV 730",
    "section": "Format",
    "text": "Format\nThe primary format is synchronous, in-person lectures, discussions, and workshops.\nCourse Description: In today’s world, understanding environmental data and making informed decisions based on it is crucial for addressing complex environmental challenges. This course serves as an introductory exploration into the integration of environmental data using R programming language, coupled with machine learning techniques. Participants will gain hands-on experience in handling, analyzing, and interpreting environmental datasets, with a focus on leveraging the power of R for data integration and predictive modeling.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#learning-objectives",
    "href": "index.html#learning-objectives",
    "title": "ENV 730",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nWe will use coding and reproducible best practices.\nWe will become familiar with different types of environmental data.\nWe will use machine learning to explore adaptive management solutions.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#required-materials",
    "href": "index.html#required-materials",
    "title": "ENV 730",
    "section": "Required Materials",
    "text": "Required Materials\n\nR for Data Science\nR For Earth System\nSpatial Data Science with R and “terra”\n\nComputing needs: Please bring a laptop to class. Your laptop must have both R and Rstudio installed and operational prior to the start of this course. Ideally you will have 16GB of RAM or higher, dual Core 2Ghz or higher (Intel Core i5 processor or equivalent) and your operating system should be Windows 10 or higher or OS X 10.14 or higher.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#course-policies",
    "href": "index.html#course-policies",
    "title": "ENV 730",
    "section": "Course Policies",
    "text": "Course Policies\nAttendance Policy: Regular in-person classroom attendance is required of all students. When extenuating circumstances require accommodation, Dr. Malone will arrange for remote attendance via zoom.\nLate Policy: All assignments submitted after the due date are subject to a late penalty. For the first 24 hour period beyond an assignment’s due date there is a 10% penalty, which increases by 10% for each additional 24 hour period.\nAcademic integrity is a core university value that ensures respect for the academic reputation of the University, its students, faculty and staff, and the degrees it confers. The University expects that students will conduct themselves in an honest and ethical manner and respect the intellectual work of others. Please ask about my expectations regarding permissible or encouraged forms of student collaboration if they are unclear. Any work that you submit at any stage of the writing process—thesis, outline, draft, bibliography, final submission, presentations, blog posts, and more—must be your own; you also may not use material generated by ChatGPT or any other AI writing software. In addition, any words, ideas, or data that you borrow from other people and include in your work must be properly documented. Failure to do either of these things is plagiarism. -Poorvu Center\nBefore collaborating with an AI chatbot on your work for this course, please request permission by sending me a note that describes (a) how you intend to use the tool and (b) how using it will enhance your learning. Any use of AI to complete an assignment must be acknowledged in a citation that includes the prompt you submitted to the bot, the date of access, and the URL of the program. -Poorvu Center\nDiversity and Disability Statement: Our institution values diversity and inclusion; we are committed to a climate of mutual respect and full participation. Our goal is to create learning environments that are usable, equitable, inclusive and welcoming. If there are aspects of the instruction or design of this course that result in barriers to your inclusion or accurate assessment or achievement, please notify Dr. Malone as soon as possible. Disabled students are also welcome to contact Student Accessibility ServicesLinks to an external site. to discuss a range of options to removing barriers in the course, including accommodations. -Poorvu Center\nUsability, Disability and Design: I am committed to creating a course that is inclusive in its design. If you encounter barriers, please let me know immediately so that we can determine if there is a design adjustment that can be made or if an accommodation might be needed to overcome the limitations of the design. I will consider creative solutions, as long as they do not compromise the intent of the assessment or learning activity. You are also welcome to contact Student Accessibility ServicesLinks to an external site. to begin this conversation or to establish accommodations for this or other courses. I welcome feedback that will assist me in improving the usability and experience for all students. -Poorvu Center",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#assignments-assessments-grading",
    "href": "index.html#assignments-assessments-grading",
    "title": "ENV 730",
    "section": "Assignments, Assessments & Grading",
    "text": "Assignments, Assessments & Grading\n\nExam (125 points): This exam is designed to evaluate basic knowledge of R and Rstudio. A score of 90% or higher signifies that students have a strong foundational knowledge of R and R studio and are prepared to focus on the content in this course.\nAttendance: Regular in-person classroom attendance is required of all students. When extenuating circumstances require accommodation, Dr. Malone will arrange for remote attendance via zoom.\nPost-workshop Assessments (~150 points): Assessments are assignments that require direct application of tools and analysis approaches.\nMajor Assessments (200 points): Activities designed to test mastery of concepts.\nFinal Project (200 points):\n\nProposal Presentation (50 points)\nFinal Report (100 points)\nFinal Project Presentation (50 points)",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "index.html#course-ouline",
    "href": "index.html#course-ouline",
    "title": "ENV 730",
    "section": "Course Ouline:",
    "text": "Course Ouline:\n\n\n\n\n\n\n\n~Week\nWorkshop Topic\n\n\n\n\n1-6\nIntroduction to R\n\nOpen Science Best Practices\nData Basics (Importing, visualizing, manipulating data)\nVector Data Manipulation & Visualization (sf)\nRaster Data Manipulation & Visualization (terra)\n\n\n\n7-8\nData Integration in R\n\nTechniques for Data Integration\n\n\n\n9-10\nMachine Learning in R\n\nRandomForest Model Development: Fitting models\nRandomForest Model Development: Validating models\nRandomForest Model Development: Sensitivity Analysis\n\n\n\n11-12\nDynamic Models in R\n\nSpatial Model Visualization\nModel Competition\n\n\n\n13-16\nFinal Project (Independent)\n\nProposal Presentation\nFinal Report\nFinal Project Presentation",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "sensitivity.html",
    "href": "sensitivity.html",
    "title": "Sensitivity Analysis",
    "section": "",
    "text": "A sensitivity analysis is a technique used to understand how changes in the inputs of a mathematical model or system affect the output. In the context of a model, such as a simulation or a statistical model, sensitivity analysis helps to identify which input parameters have the most significant influence on the results.\nTo begin load the model and dataset we will use for this workshop:\n\nload( 'data/SensitivityProducts.RDATA')\n\nWe will use the following packages:\n\nlibrary(randomForest)\nlibrary(tidyverse)\nlibrary(gtools)\nlibrary(ggplot2)\n\nIn this workshop, we will prepare a sensitivity analysis for the model FCH4_F_gC.rf. Take a look at the model:\n\nFCH4_F_gC.rf \n\n\nCall:\n randomForest(formula = FCH4_F_gC ~ P_F + TA_F + Upland, data = train,      keep.forest = T, importance = TRUE, mtry = 1, ntree = 500,      keep.inbag = TRUE) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n\n          Mean of squared residuals: 4.321792\n                    % Var explained: 35.76\n\n\nThe model includes monthly precipitation (P_f), mean temperature (TA_F), and a binary indicator for upland (1= upland ecosystem and 0 = aquatic ecosystem).\nExplore the conditions present within the dataset (fluxnet.new) used to build the model. First subset only the variables used in the final model:\n\nmodel.vars &lt;- fluxnet.new %&gt;% as.data.frame %&gt;%  select( P_F, TA_F,Upland)\n\nNext, we will summarize the conditions within the dataset by quantiles (0.25, 0.5, 0.75): note that your categorical varibles need to be listed in .by=c().\n\nmodel.vars.lower &lt;- model.vars %&gt;% reframe( .by= Upland, P_F = quantile(P_F, 0.25),\n                                                                      TA_F = quantile(TA_F, 0.25 ),\n                                                                      Quantile = as.factor(0.25)) %&gt;% as.data.frame()\n\nmodel.vars.median &lt;- model.vars %&gt;% reframe( .by= Upland,P_F = quantile(P_F, 0.5),\n                                                                   TA_F = quantile(TA_F, 0.5 ),\n                                                                   Quantile = as.factor(0.5)) %&gt;% as.data.frame()\n\nmodel.vars.upper &lt;- model.vars %&gt;% reframe( .by= Upland,P_F = quantile(P_F, 0.75),\n                                                                   TA_F = quantile(TA_F, 0.75 ),\n                                                                   Quantile = as.factor(0.75)) %&gt;% as.data.frame()\n\nCombine the individual Summaries into one dataframe:\n\nsummary &lt;- smartbind( model.vars.lower, model.vars.median, model.vars.upper)\nsummary \n\n    Upland     P_F      TA_F Quantile\n1:1      0 14.5905  1.609448     0.25\n1:2      1 16.2000  2.099640     0.25\n2:1      0 41.7950 10.157892      0.5\n2:2      1 46.6680 10.859068      0.5\n3:1      0 78.3985 17.060485     0.75\n3:2      1 91.6260 17.147735     0.75\n\n\nNow, choose the variable you want to explore: TA_F\nLook at the conditions present witing the dataset for TA_F\n\nsummary(fluxnet.new$TA_F)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-37.188   1.814  10.595   8.874  17.124  31.280 \n\n\nLook at the range in values for TA_F:\n\nrange(fluxnet.new$TA_F)\n\n[1] -37.18842  31.28010\n\n\nAccess each individual range value:\n\nrange(fluxnet.new$TA_F)[1]\n\n[1] -37.18842\n\nrange(fluxnet.new$TA_F)[2]\n\n[1] 31.2801\n\n\nUse the range to generate a sequence of values going from the highest to lowest:\n\nTA_F.seq &lt;- seq(range(fluxnet.new$TA_F)[1], range(fluxnet.new$TA_F)[2], by=5 )\n\nCreate a dataframe:\n\nTA_F.seq.df &lt;- data.frame(TA_F = TA_F.seq )\n\nNext we will take the summary, remove the values for TA_F and replace is with the generated range:\n\nTA_F_1AT &lt;- summary %&gt;% select(-TA_F) %&gt;% cross_join(TA_F.seq.df)\n\nIn this dataframe, all other variables in the model are held at their quantile values and only temperature varies across the range of values observed. This approach is “one-at-a-time”. We choose one variable to vary and hold all others at a specific value. Often the mean is chosen but using the 0.25 , 0.5, and 0.75 quantile helps to visualize what the model would predict across conditions.\nNext, use the predict() function to predict values into the dataframe (TA_F_1AT) with the simulated conditions:\n\nTA_F_1AT$PRED &lt;-predict(FCH4_F_gC.rf , newdata =  TA_F_1AT)\nTA_F_1AT$PRED\n\n [1] 0.7865243 0.7849535 0.7849535 0.7863125 0.7915485 0.7938221 0.7999815\n [8] 0.8049531 0.8573231 1.0604947 1.5308568 3.7154677 4.4405273 5.5544100\n[15] 0.2840847 0.2840847 0.2840847 0.2841673 0.2833294 0.2859127 0.2846756\n[22] 0.2908040 0.3133371 0.4528563 0.4795861 0.7387009 1.2891996 2.3624352\n[29] 0.9599767 0.9599767 0.9599767 0.9601269 0.9602617 0.9624025 0.9571700\n[36] 0.9776828 1.0225842 1.2624965 1.9266091 3.7791711 5.2229525 5.5206278\n[43] 0.4368570 0.4368570 0.4368570 0.4368570 0.4368613 0.4474668 0.3249978\n[50] 0.3315233 0.3450498 0.4880674 0.5654118 0.8090723 1.1766667 1.5957777\n[57] 0.9608859 0.9608859 0.9608859 0.9610361 0.9618341 0.9640564 0.9618201\n[64] 0.9784727 1.0238619 1.2011973 1.6773282 3.2341428 4.0571488 4.3363714\n[71] 0.3662076 0.3662076 0.3662076 0.3662076 0.3662119 0.3768174 0.2958994\n[78] 0.2985805 0.3224997 0.4540422 0.5588751 0.6855452 0.9679276 1.1272437\n\n\nLets look at the predictions.\n\nggplot() + geom_point(data =TA_F_1AT , aes( x=TA_F, y=PRED) )\n\n\n\n\n\n\n\n\nUse geom_smooth to visualize the general relationship:\n\nggplot() + geom_smooth(data =TA_F_1AT , aes( x=TA_F, y=PRED) )\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nLook at the predictions by Quantile and by the Upland indicator:\n\nggplot() + geom_line(data =TA_F_1AT , aes( x=TA_F, y=PRED, color=Quantile, group=interaction(Quantile, Upland), linetype =Upland)  )\n\n\n\n\n\n\n\n\nNow explore the next variable: PA_f\n\nsummary(fluxnet.new$P_F)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00   15.49   43.31   69.37   82.85 1094.02 \n\nrange(fluxnet.new$P_F)\n\n[1]    0.000 1094.017\n\nrange(fluxnet.new$P_F)[1]\n\n[1] 0\n\nrange(fluxnet.new$P_F)[2]\n\n[1] 1094.017\n\nP_F.seq &lt;- seq(range(fluxnet.new$P_F)[1], range(fluxnet.new$P_F)[2], by=100 )\n\nP_F.seq.df &lt;- data.frame(P_F = P_F.seq )\n\nP_F_1AT &lt;- summary %&gt;% select(-P_F) %&gt;% cross_join(P_F.seq.df)\n\nP_F_1AT$PRED &lt;- predict(FCH4_F_gC.rf , newdata =  P_F_1AT)\n\nggplot() + geom_point(data =P_F_1AT , aes( x=P_F, y=PRED) )\n\n\n\n\n\n\n\nggplot() + geom_smooth(data =P_F_1AT , aes( x=P_F, y=PRED) )\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\nggplot() + geom_line(data =P_F_1AT , aes( x=P_F, y=PRED, color=Quantile, group=interaction(Quantile, Upland), linetype =Upland)  )\n\n\n\n\n\n\n\n\nUse this same workflow to explore the sensitivity of your groups project’s model. Please provide a presentation explaining how you fit your model (mtry?, ntree?) and how variables where selected (forward versus backward selection). Describe your final model results (variables in the final model, their importance, the %Var, observed versus predicted for testing and training data), including a correlation plot, variance importance plots, and sensitivity analyses. This report will develop into the methods and results portion of your final project.",
    "crumbs": [
      "Machine Learning in R",
      "Sensitivity Analysis"
    ]
  },
  {
    "objectID": "data_pub.html",
    "href": "data_pub.html",
    "title": "Additional Resources: Data Publication",
    "section": "",
    "text": "A research project can involve multiple moving parts that could get lost if there isn’t a plan in place to manage how your research will be preserved and reused. This page will get you familiarized with the best practices of data management/publication, and serves as a quick-start guide to the EDI and KNB data repositories.\nSome content on this page have been adapted from NCEAS’ coreR for Delta Science Program.",
    "crumbs": [
      "Resources",
      "Data Publication"
    ]
  },
  {
    "objectID": "data_pub.html#overview",
    "href": "data_pub.html#overview",
    "title": "Additional Resources: Data Publication",
    "section": "",
    "text": "A research project can involve multiple moving parts that could get lost if there isn’t a plan in place to manage how your research will be preserved and reused. This page will get you familiarized with the best practices of data management/publication, and serves as a quick-start guide to the EDI and KNB data repositories.\nSome content on this page have been adapted from NCEAS’ coreR for Delta Science Program.",
    "crumbs": [
      "Resources",
      "Data Publication"
    ]
  },
  {
    "objectID": "data_pub.html#data-life-cycle",
    "href": "data_pub.html#data-life-cycle",
    "title": "Additional Resources: Data Publication",
    "section": "Data Life Cycle",
    "text": "Data Life Cycle\n\n\n\nSource: Faundeen et al 2013, USGS\nAs you begin a research project, your data may go through lots of stages in its life cycle. Like the diagram from Faundeen et al 2013 shows, the first stage starts with planning out goals you want to accomplish or research questions you want to answer. Consider the resources (data, funding, personnel) you will need.\nWith a plan in mind, the second stage of the data life cycle involves identifying and evaluating new or existing data sources to be collected/generated/re-used. You might acquire datasets at one of the many data repositories out there. At the next stage, these data inputs will be processed through data cleaning and wrangling.\nOnce the data is processed, it will be ready for analysis, where new insights are generated. Be sure to document the processes that were involved, for example, recording the methods and organizing files to ensure that your workflow is reproducible.\nAs the research project nears its end, consider how you will preserve your data products for the long-term, especially if it’s required by your funding. If you are planning to submit your data/code for archival at a data repository, plan what metadata and documentation you will need to ensure your results will be accessible and reusable for the future. Each repository will have slightly different requirements, but check out the next few sections to learn more about what data publication at EDI and KNB entails.\nLast but not least, once your data is published with a DOI (digital object identifier), other researchers will be able to use and cite your work in the future, which in turn, will expose your research to an even wider audience.\nYou may have also noticed three arrows cutting across the diagram. These steps should always be kept in mind when you’re progressing your research in order to manage your data effectively.\nAs you’re working, describe your data products by documenting its metadata. Metadata, or data about data, is a critical component of data reusability. More on metadata below. Additionally, have some quality control techniques in place so that the values of your output are expected. This helps to ensure that your workflow will run as intended. Finally, it never hurts to backup your data regularly to protect it from unexpected data losses due to software failures or natural disasters.",
    "crumbs": [
      "Resources",
      "Data Publication"
    ]
  },
  {
    "objectID": "data_pub.html#metadata",
    "href": "data_pub.html#metadata",
    "title": "Additional Resources: Data Publication",
    "section": "Metadata",
    "text": "Metadata\nAs mentioned, metadata plays an important role in the data life cycle, yet it’s often overlooked. Metadata is crucial to understanding, interpreting, and reusing datasets. It contains information on what was measured, who made the measurements, when and where were measurements made, and why was the data collected. The point is to have enough metadata so that anyone (even you) can reuse the data far into the future.\nIf you are submitting your dataset/code to a data repository, you will need to create metadata that documents all the details about the data itself. One popular, standard format for metadata files in the field of environmental sciences is EML, Ecological Metadata Language. This format arranges the metadata into an organized, machine-readable set of elements.\nThere are many ways to create this EML file, such as programmatically through R or by filling out a web form offered by a data repository. EDI and KNB are two data repositories that have the web form option for researchers to generate their metadata file.",
    "crumbs": [
      "Resources",
      "Data Publication"
    ]
  },
  {
    "objectID": "data_pub.html#edi-quick-start-guide",
    "href": "data_pub.html#edi-quick-start-guide",
    "title": "Additional Resources: Data Publication",
    "section": "EDI Quick-Start Guide",
    "text": "EDI Quick-Start Guide\nEDI, or the Environmental Data Initiative, is a DataONE data repository that hosts ecological and environmental data for the long-term. They have staff to support curation efforts and work closely with the LTER Network to promote best practices for FAIR (Findable, Accessible, Interoperable, and Reusable) data.\nTo get started with your data package submission, navigate to the Publish Data page in EDI. Click to login to their ezEML metadata editor. ezEML is EDI’s easy-to-use web-based tool for creating metadata.\n\n\n\nChoose one of the many options to login to ezEML. You will want to stay consistent with the account that you’re using to login because your submissions will only be associated to the account you used to login.\n\n\n\nOnce you’ve logged in, click on EML Documents then New.. to start a new EML document.\n\n\n\nYou will be prompted to enter a name for your metadata document. Enter a unique, memorable name.\n\n\n\nThen you will be presented with a form to fill out all the information about your data, such as the Title, Data Tables, Creators, Contacts, etc. Be as descriptive as you can. Don’t worry about filling it all out in one sitting. The information you enter will be saved as you go, and you can always return to your metadata document to continue where you’ve left off.\n\n\n\nIf you’re not sure how to fill out a section, click on the question mark to get helpful tips.\n\n\n\nIn the screenshot below, a pop-up appeared on how to best come up with a Title for your data package.\n\n\n\nIf you have tabular data in CSV files, you can upload them in Data Tables and EDI will automatically infer many attributes about your data, like column names, so you can save some time on manually filling out information. Be sure to double-check the metadata so everything looks correct.\n\n\n\nFor all other types of data, you can upload them in Other Entities and fill out the metadata as appropriate.\n\n\n\nFor extra help, check out the ezEML User Guide or email EDI for support. You can also watch their YouTube tutorial below.\n\n\nOnce you’re done filling out the sections to the best of your ability, click on Submit/Share Package to submit your data package for the EDI team to review. When you’re submitting for review, there’s an option to add a message to your submission. If you wish for your data to be embargoed, you should mention it here.\nThe EDI team will respond back to you via email with any additional suggestions/edits or the greenlight to publish your data package with a DOI if everything looks good.\n\n\n\nThere’s also an option on this page to invite your colleagues to collaborate on editing the metadata. Be aware that only one person can edit the metadata at a time.\nWhen your data is published with a DOI, the landing page for your data package may look something like this:\n\n\n\nThe full metadata will be available under View Full Metadata. With this, your data will be available for reuse!",
    "crumbs": [
      "Resources",
      "Data Publication"
    ]
  },
  {
    "objectID": "data_pub.html#knb-quick-start-guide",
    "href": "data_pub.html#knb-quick-start-guide",
    "title": "Additional Resources: Data Publication",
    "section": "KNB Quick-Start Guide",
    "text": "KNB Quick-Start Guide\nJust like EDI, KNB, or the Knowledge Network for Biocomplexity, is another DataONE data repository that preserves ecological and environmental data for the long-term. They are hosted by NCEAS and abide by the FAIR (Findable, Accessible, Interoperable, and Reusable) data principles.\nNavigate to the Submit page to get started on your submission. It will prompt you to sign-in through your ORCID account if you’re not already signed-in. Be sure to create an ORCID account if you don’t have one already. It will help you keep track of the products of your research.\n\n\n\n\n\n\nOnce you’ve signed-in, the Submit page will direct you to fill out a web form. The top half of the form is for uploading and describing your data files while the bottom half is for the other metadata fields like Overview, People, Dates, Locations, etc.\n\n\n\nClick on + Add Files to add any data files you wish to submit. Once you have them uploaded, there are options to either Describe, Replace, or Remove the file.\n\n\n\nClick on Describe to enter metadata about the data files. Provide a file description and information about the attributes/variables in this data file.\n\n\n\n\n\n\nOnce you’re done entering the metadata for the data files, you will also need to fill out the rest of the metadata fields in the bottom half of the web form. Fill out the Overview, People, Dates, Locations, Taxa, and Methods sections as best as you can.\n\n\n\nLike with EDI, you do not need to worry about submitting all your metadata in one sitting. Utilize the Save dataset button to save your progress and feel free to continue another day.\n\n\n\nAfter you’ve saved your dataset once, you can come back to the same submission to edit it however you like. Unlike EDI, you can publish your data package in KNB by assigning it a DOI yourself. Click on Publish with DOI whenever you’re ready. Of course, you can continue to edit your data even after assigning it a DOI, but be aware that a new DOI will be issued for every version of your submission.\n\n\n\nFor extra assistance, email KNB (knb-help@nceas.ucsb.edu) or browse other data packages on KNB to get an idea of what the final product should look like.",
    "crumbs": [
      "Resources",
      "Data Publication"
    ]
  },
  {
    "objectID": "data_pub.html#accessing-data-from-repositories-with-r",
    "href": "data_pub.html#accessing-data-from-repositories-with-r",
    "title": "Additional Resources: Data Publication",
    "section": "Accessing Data from Repositories with R",
    "text": "Accessing Data from Repositories with R\nNow that you have your data published, you can access it via R. Below is an example script on how to get data from EDI and KNB.\n\n## --------------------------------------------- ##\n#      How to pull data from EDI or KNB\n## --------------------------------------------- ##\n# Script author(s): Angel Chen\n\n# Purpose:\n## This script shows you how to either read in data directly from EDI/KNB, \n## or download the data first and then read in\n\n## --------------------------------------------- ##\n#               Housekeeping -----\n## --------------------------------------------- ##\n\n# Load necessary libraries\nlibrary(\"tidyverse\") \nlibrary(\"EDIutils\")\n\n## --------------------------------------------- ##\n#               EDI example -----\n## --------------------------------------------- ##\n\n# Getting a single file ------------------------------------------\n\n# As an example, let's get ER_contour.csv from this EDI package:\n# https://doi.org/10.6073/pasta/9b0cce476cb9a7f96575ff1c3d3155c3\n\n# First specify the direct URL to the data file you want\n# You can copy any data file's URL by clicking on \"View Full Metadata\"\n# Scroll down to the \"Detailed Metadata\" -&gt; \"Data Entities\" section\n# Copy the URL associated with the data file you want\n\n# Paste the direct URL to ER_contour.csv\nedi_url &lt;- \"https://pasta.lternet.edu/package/data/eml/edi/1790/1/e64ec185b099968d9ae653d620e23d31\"\n\n# Reading directly \ndf &lt;- read_csv(edi_url)\n\n# Or you can download first, and then read in\ndownload.file(url = edi_url, destfile = \"ER_contour.csv\")\ndf &lt;- read_csv(\"ER_contour.csv\")\n\n# Getting multiple files -----------------------------------------\n\n# If you would like to download the entire data package as a zip file, \n# use EDIutils::read_data_package_archive()\n\n# Create an empty folder to store the zip file in\ndir.create(path = \"edi_data\")\n\n# Download the data package by specifying the package ID number\n# The package ID is listed on the landing page of every data package\nread_data_package_archive(\"edi.1790.1\", path = \"edi_data\")\n\n# Unzip the contents into a new \"edi.1790.1\" folder\nunzip(file.path(\"edi_data\", \"edi.1790.1.zip\"), exdir = file.path(\"edi_data\", \"edi.1790.1\"))\n\n## --------------------------------------------- ##\n#                KNB example -----\n## --------------------------------------------- ##\n\n# Getting a single file ------------------------------------------\n\n# This method works for KNB repositories as well!\n# Let's get Everglades_Aquatic_Metabolism_SRS_2.csv from this KNB package:\n# https://doi.org/10.5063/JQ0ZDM\n\n# First specify the direct URL to the data file you want\n# You can copy any data file's URL by right-clicking its Download button (appears as a cloud icon),\n# and then selecting \"Copy Link Address\"\n\n# Paste the direct URL to Everglades_Aquatic_Metabolism_SRS_2.csv\nknb_url &lt;- \"https://knb.ecoinformatics.org/knb/d1/mn/v2/object/urn%3Auuid%3A9e3cdf01-47b9-4025-8656-75dcf696a46c\"\n\n# Reading directly\ndf &lt;- read_csv(knb_url)\n\n# Or you can download first, and then read in\ndownload.file(url = knb_url, destfile = \"Everglades_Aquatic_Metabolism_SRS_2.csv\")\ndf &lt;- read_csv(\"Everglades_Aquatic_Metabolism_SRS_2.csv\")\n\n# Getting multiple files -----------------------------------------\n\n# If you would like to download the entire data package as a zip file,\n# right-click the data package's Download button (appears as a cloud icon),\n# and then select \"Copy Link Address\"\n\n# Paste the URL to the entire data package\nknb_pkg_url &lt;- \"https://knb.ecoinformatics.org/knb/d1/mn/v2/packages/application%2Fbagit-1.0/urn%3Auuid%3Ab9539caf-9133-4d93-8099-3a38b1c4d568\"\n\n# Create an empty folder to store the zip file in\ndir.create(path = \"knb_data\")\n\n# Download the data package as a zip file\ndownload.file(url = knb_pkg_url, destfile = file.path(\"knb_data\", \"example_knb_pkg.zip\"))\n\n# Unzip the contents into a new \"example_knb_pkg\" folder\nunzip(file.path(\"knb_data\", \"example_knb_pkg.zip\"), exdir = file.path(\"knb_data\", \"example_knb_pkg\"))",
    "crumbs": [
      "Resources",
      "Data Publication"
    ]
  },
  {
    "objectID": "terra.html",
    "href": "terra.html",
    "title": "Introduction to rasters with the terra package",
    "section": "",
    "text": "install.packages('sf')\ninstall.packages('terra')\ninstall.packages(\"remotes\")\ninstall.packages(\"tidyverse\")\ninstall.packages('tidyterra')\nremotes::install_github(\"mikejohnson51/AOI\")\nremotes::install_github(\"mikejohnson51/climateR\")",
    "crumbs": [
      "Introduction to R",
      "Raster Data"
    ]
  },
  {
    "objectID": "terra.html#rasters",
    "href": "terra.html#rasters",
    "title": "Introduction to rasters with the terra package",
    "section": "Rasters",
    "text": "Rasters\nA raster is a spatial data structure that subdivides an extent into rectangles known as “cells” (or “pixels”). Each cell has the capacity to store one or more values. This type of data structure is commonly referred to as a “grid” and is frequently juxtaposed with simple features.\nThe terra package offers functions designed for the creation, reading, manipulation, and writing of raster data. The terra package is built around a number of “classes” of which the SpatRaster and SpatVector are the most important.\n\nSpatRaster\nA SpatRaster object stores a number of fundamental parameters that describe it. These include the number of columns and rows, the coordinates of its spatial extent, and the coordinate reference system. In addition, a SpatRaster can store information about the file(s) in which the raster cell values are stored.\n\n\nSpatVector\nA SpatVector represents “vector” data, that is, points, lines or polygon geometries and their tabular attributes.\n\n\nWorking with climate data:\nTo become familiar with working with rasters, we will download climate data for an area of interest (AOI).\n\n# First create an AOI\naoi.global &lt;- aoi_get(country= c(\"Europe\",\"Asia\" ,\"North America\", \"South America\", \"Australia\",\"Africa\", \"New Zealand\"))\n\nVisualize your AOI:\nWe will use TerraClimate, a dataset of high-spatial resolution (1/24°, ~4-km) monthly climate and climatic water balance for global terrestrial surfaces from 1958–2015 (Abatzoglou, 2018).\nDownload climate data using the library climateR for the AOI. For this exercise we will use climate normals, multi-decadal averages for climate variables like temperature and precipitation. They provide a baseline that allows us to understand the location’s average condition.\nYou can access monthly precipitation (“ppt”), monthly temperature minimum (“tmin”), monthly temperature maximum (“tmax”) climate normals and more.\nDownload monthly precipitation (ppt)\n\nnormals.ppt &lt;- aoi.global  %&gt;% getTerraClimNormals(varname =\"ppt\")\n\nWhat is this object:\n\nclass(normals.ppt)\n\n[1] \"list\"\n\nclass(normals.ppt$ppt)\n\n[1] \"SpatRaster\"\nattr(,\"package\")\n[1] \"terra\"\n\n\nA RasterStack is a collection of RasterLayer objects with the same spatial extent and resolution. In essence it is a list of RasterLayer objects.\nTo access the raster stack:\n\nnormals.ppt$ppt\n\nclass       : SpatRaster \ndimensions  : 3343, 8640, 12  (nrow, ncol, nlyr)\nresolution  : 0.04166667, 0.04166667  (x, y)\nextent      : -180, 180, -55.625, 83.66667  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=longlat +ellps=WGS84 +no_defs \nsource(s)   : memory\nnames       : ppt_1~12010, ppt_1~12010, ppt_1~12010, ppt_1~12010, ppt_1~12010, ppt_1~12010, ... \nmin values  :         0.0,         0.0,         0.0,         0.0,         0.0,         0.0, ... \nmax values  :       945.8,       994.4,       827.2,       974.5,      2383.5,      2470.6, ... \nunit        :          mm,          mm,          mm,          mm,          mm,          mm, ... \ntime        : 1961-01-01 to 1961-12-01 UTC \n\nnormals.ppt$ppt %&gt;% names()\n\n [1] \"ppt_1961-01-01_19812010\" \"ppt_1961-02-01_19812010\"\n [3] \"ppt_1961-03-01_19812010\" \"ppt_1961-04-01_19812010\"\n [5] \"ppt_1961-05-01_19812010\" \"ppt_1961-06-01_19812010\"\n [7] \"ppt_1961-07-01_19812010\" \"ppt_1961-08-01_19812010\"\n [9] \"ppt_1961-09-01_19812010\" \"ppt_1961-10-01_19812010\"\n[11] \"ppt_1961-11-01_19812010\" \"ppt_1961-12-01_19812010\"\n\nnormals.ppt$ppt %&gt;% time()\n\n [1] \"1961-01-01 UTC\" \"1961-02-01 UTC\" \"1961-03-01 UTC\" \"1961-04-01 UTC\"\n [5] \"1961-05-01 UTC\" \"1961-06-01 UTC\" \"1961-07-01 UTC\" \"1961-08-01 UTC\"\n [9] \"1961-09-01 UTC\" \"1961-10-01 UTC\" \"1961-11-01 UTC\" \"1961-12-01 UTC\"\n\nggplot() + geom_spatraster(data=normals.ppt$ppt[[1]]) + scale_fill_gradient( na.value = \"transparent\")\n\n&lt;SpatRaster&gt; resampled to 500280 cells.\n\n\n\n\n\n\n\n\n\nWhat is the “+ scale_fill_gradient( na.value =”transparent”)” doing?\n\n\nRaster algebra\nMany generic functions that allow for simple and elegant raster algebra have been implemented for SpatRaster objects, including the normal algebraic operators such as +, -, *, /, logical operators such as &gt;, &gt;=, &lt;, ==, !} and functions such as abs, round, ceiling, floor, trunc, sqrt, log, log10, exp, cos, sin, max, min, range, prod, sum, any, all. In these functions you can mix terra objects with numbers, as long as the first argument is a terra object. If you use multiple SpatRaster objects, all objects must have the same resolution and origin.\nLets summarize monthly data to annual normals:\n\nnormals.ppt.annual &lt;- normals.ppt$ppt %&gt;% sum(na.rm = TRUE)\n\n# look at the object\nnormals.ppt.annual %&gt;% plot()\n\n\n\n\n\n\n\n# Check the name of the layers:\nnames(normals.ppt.annual)\n\n[1] \"sum\"\n\n# re-name the layers:\nnames(normals.ppt.annual) &lt;- \"ppt\"\n\nSummary functions (min(), max(), mean(), prod(), sum(), median(), cv(), range(), any(), all()) always return a SpatRaster object.\nUse global if instead of a SpatRaster you want a single number summarizing the cell values of each layer.\n\nnormals.ppt.annual %&gt;% global( na.rm=T, mean)\n\n        mean\nppt 703.3037\n\n\n\n\nSpatial Summaries\nYou might also find it useful to create zonal summaries for each polygon within the simple feature. To do this we can use the function zonal, which takes a SpatRast and a SpatVect.\n\nnormals.ppt.annual.country &lt;- zonal(x = normals.ppt.annual, \nz= vect(aoi.global) , fun = \"mean\", as.polygons=TRUE,  na.rm=TRUE)\n\nWhat did the function return?\n\nclass( normals.ppt.annual.country)\n\n[1] \"SpatVector\"\nattr(,\"package\")\n[1] \"terra\"\n\n\nConvert the SpatVect back to a simple feature and plot it.\n\nnormals.ppt.annual.country.sf &lt;- st_as_sf(normals.ppt.annual.country)   \n\nggplot( data=normals.ppt.annual.country.sf ) + geom_sf(aes(fill= ppt))\n\n\n\n\n\n\n\n\n\n\nExtracting information to a point file:\nFor this exercise you will use your FLUXNET.CH4 vector file you saved last week and the monthly ppt data downloaded (normals.ppt.NAmerica$ppt).\nImport your point file FLUXNET.ch4:\n\nFLUXNET.ch4 &lt;- st_read(dsn=\"data/products\", layer=\"FLUXNET_CH4\")\n\nReading layer `FLUXNET_CH4' from data source \n  `/Users/ac3656/GitHub/EDS_course/data/products' using driver `ESRI Shapefile'\nSimple feature collection with 81 features and 10 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -18223020 ymin: -4162002 xmax: 19542570 ymax: 7939774\nProjected CRS: WGS 84 / World Equidistant Cylindrical\n\n\nEnsure both files have the same coordinate reference system (CRS):\n\nFLUXNET.ch4 \n\nSimple feature collection with 81 features and 10 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -18223020 ymin: -4162002 xmax: 19542570 ymax: 7939774\nProjected CRS: WGS 84 / World Equidistant Cylindrical\nFirst 10 features:\n   SITE_ID                   SITE_NA  FLUXNET2  FLUXNET_ LOCATIO IGBP  MAT\n1   AT-Neu                  Neustift CC-BY-4.0 CC-BY-4.0     970  GRA  6.5\n2   BR-Npw Northern Pantanal Wetland      &lt;NA&gt; CC-BY-4.0     120  WSA 24.9\n3   BW-Gum                      Guma      &lt;NA&gt; CC-BY-4.0     950  WET 21.0\n4   BW-Nxr                   Nxaraga      &lt;NA&gt; CC-BY-4.0     950  GRA 21.0\n5   CA-SCB          Scotty Creek Bog      &lt;NA&gt; CC-BY-4.0     280  WET -2.8\n6   CA-SCC    Scotty Creek Landscape      &lt;NA&gt; CC-BY-4.0     285  ENF -2.8\n7   CH-Cha                    Chamau CC-BY-4.0 CC-BY-4.0     393  GRA  9.5\n8   CH-Dav                     Davos CC-BY-4.0 CC-BY-4.0    1639  ENF  2.8\n9   CH-Oe2            Oensingen crop CC-BY-4.0 CC-BY-4.0     452  CRO  9.8\n10  CN-Hgu                  Hongyuan      &lt;NA&gt; CC-BY-4.0    3500  GRA  1.5\n      MAP  MnDstn_     Country                  geometry\n1   852.0 10829.38     Austria   POINT (1259858 5245007)\n2  1486.0 12208.78      Brazil POINT (-6279755 -1836549)\n3   460.0 13930.19    Botswana  POINT (2490339 -2111141)\n4   460.0 14000.38    Botswana  POINT (2580297 -2176085)\n5   388.0 11237.65      Canada POINT (-13502876 6824876)\n6   387.6 11237.65      Canada POINT (-13502965 6824764)\n7  1136.0 10719.05 Switzerland  POINT (936241.4 5255415)\n8  1062.0 10771.80 Switzerland   POINT (1097154 5211455)\n9  1155.0 10698.00 Switzerland  POINT (860911.5 5263898)\n10  747.0 17202.71       China  POINT (11420267 3656322)\n\nnormals.ppt$ppt\n\nclass       : SpatRaster \ndimensions  : 3343, 8640, 12  (nrow, ncol, nlyr)\nresolution  : 0.04166667, 0.04166667  (x, y)\nextent      : -180, 180, -55.625, 83.66667  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=longlat +ellps=WGS84 +no_defs \nsource(s)   : memory\nnames       : ppt_1~12010, ppt_1~12010, ppt_1~12010, ppt_1~12010, ppt_1~12010, ppt_1~12010, ... \nmin values  :         0.0,         0.0,         0.0,         0.0,         0.0,         0.0, ... \nmax values  :       945.8,       994.4,       827.2,       974.5,      2383.5,      2470.6, ... \nunit        :          mm,          mm,          mm,          mm,          mm,          mm, ... \ntime        : 1961-01-01 to 1961-12-01 UTC \n\n\nYou can transform one of the files if they dont match. Here I transform th vector:\n\nFLUXNET.ch4  &lt;- st_transform(FLUXNET.ch4, crs= crs(normals.ppt$ppt ))\n\nTo check to see if everything lines up, I plot the files together:\n\nggplot() + geom_spatraster( data=normals.ppt$ppt[[1]]) +geom_sf( data =FLUXNET.ch4 ) +  scale_fill_gradient( na.value = \"transparent\")\n\n&lt;SpatRaster&gt; resampled to 500280 cells.\n\n\n\n\n\n\n\n\n\nExtract information from your raster stack using terra::extract()\n\nFLUXNET.ch4.ppt &lt;-terra::extract( normals.ppt$ppt, FLUXNET.ch4)\nhead(FLUXNET.ch4.ppt)\n\n  ID ppt_1961-01-01_19812010 ppt_1961-02-01_19812010 ppt_1961-03-01_19812010\n1  1                    43.3                    41.4                    64.0\n2  2                   231.2                   194.3                   170.2\n3  3                   111.7                    92.6                    66.1\n4  4                   106.9                    82.1                    64.0\n5  5                    20.4                    16.9                    16.2\n6  6                    20.4                    16.9                    16.2\n  ppt_1961-04-01_19812010 ppt_1961-05-01_19812010 ppt_1961-06-01_19812010\n1                    62.9                    98.5                   130.3\n2                   103.4                    48.3                    22.5\n3                    27.3                     1.4                     0.2\n4                    24.1                     1.4                     2.1\n5                    20.0                    38.6                    59.8\n6                    20.0                    38.6                    59.8\n  ppt_1961-07-01_19812010 ppt_1961-08-01_19812010 ppt_1961-09-01_19812010\n1                   139.1                   134.5                    92.3\n2                    19.2                    26.4                    45.7\n3                     0.0                     0.0                     4.2\n4                     0.0                     0.0                     3.9\n5                    75.9                    66.8                    36.9\n6                    75.9                    66.8                    36.9\n  ppt_1961-10-01_19812010 ppt_1961-11-01_19812010 ppt_1961-12-01_19812010\n1                    71.6                    66.2                    56.2\n2                   105.8                   155.4                   200.1\n3                    24.7                    46.1                    73.6\n4                    19.6                    42.8                    71.4\n5                    32.6                    25.4                    20.5\n6                    32.6                    25.4                    20.5\n\n\nWhat did the extract function return?\n\nclass(FLUXNET.ch4.ppt)\n\n[1] \"data.frame\"\n\n\nCombine extracted information to the simple feature:\n\nFLUXNET.ch4.ppt.sf &lt;- FLUXNET.ch4 %&gt;% cbind(FLUXNET.ch4.ppt)\n\nVisualize your work:\n\nggplot()+  geom_sf(data = aoi.global) + geom_sf( data = FLUXNET.ch4.ppt.sf, aes( col= ppt_1961.09.01_19812010)) \n\n\n\n\n\n\n\n\nFLUXNET data can be used to understand patterns in natural methane fluxes. Evaluating the conditions where measurements are taken is essential to designing a useful model.",
    "crumbs": [
      "Introduction to R",
      "Raster Data"
    ]
  },
  {
    "objectID": "terra.html#download-monthly-temperature-tmin-and-tmax-to-understand-the-differences-in-temperature-for-the-tower-locations.-use-precipitation-and-temperature-summaries-in-assignment-23.",
    "href": "terra.html#download-monthly-temperature-tmin-and-tmax-to-understand-the-differences-in-temperature-for-the-tower-locations.-use-precipitation-and-temperature-summaries-in-assignment-23.",
    "title": "Introduction to rasters with the terra package",
    "section": "Download monthly temperature (tmin and tmax) to understand the differences in temperature for the tower locations. Use precipitation and temperature summaries in Assignment 2/3.",
    "text": "Download monthly temperature (tmin and tmax) to understand the differences in temperature for the tower locations. Use precipitation and temperature summaries in Assignment 2/3.",
    "crumbs": [
      "Introduction to R",
      "Raster Data"
    ]
  },
  {
    "objectID": "terra.html#assignment-2",
    "href": "terra.html#assignment-2",
    "title": "Introduction to rasters with the terra package",
    "section": "Assignment 2",
    "text": "Assignment 2\nWe will use data from FLUXNET CH4 to explore patterns in natural methane emissions. Explore the distribution of tower sites and create visualizations that may be helpful to understand in the design and development of models. You are welcome to use any additional data or just new plot types.\n\nReferences\nAbatzoglou, J., Dobrowski, S., Parks, S. et al. TerraClimate, a high-resolution global dataset of monthly climate and climatic water balance from 1958–2015. Sci Data 5, 170191 (2018). https://doi.org/10.1038/sdata.2017.191",
    "crumbs": [
      "Introduction to R",
      "Raster Data"
    ]
  },
  {
    "objectID": "sf.html",
    "href": "sf.html",
    "title": "Introduction to Simple Features in R",
    "section": "",
    "text": "Install libraries for this workshop\n\ninstall.packages('sf')\ninstall.packages('devtools')\ndevtools::install_github(\"mikejohnson51/AOI\", force = TRUE)\ndevtools::install_github(\"valentinitnelav/plotbiomes\")\n\n\n\nLoad the required libraries for this workshop\n\nlibrary(sf)\nlibrary(AOI)\nlibrary(tidyverse)\nlibrary(ggplot2)\n\n\n\nGoals\nThe goals of this workshop are to: 1. Become familiar with simple features 2. Master simple feature manipulation 3. Visualize simple features\n\n\nData\nImport FluxNet_Sites_2024.csv. This table was created from the FLUXNET site list found at https://fluxnet.org/sites/site-list-and-pages/?view=table.\n\nFluxNet &lt;- read.csv('data/FluxNet_Sites_2024.csv')\n\nThis dataset includes:\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\nSITE_ID\nUnique site id\n\n\nSITE_NAME\nSite name\n\n\nFLUXNET2015\nLicense information for the data for the two FLUXNET Products\n\n\nFLUXNET-CH4\nLicense information for the data for the two FLUXNET Products\n\n\nLOCATION_LAT\nLocation information\n\n\nLOCATION_LONG\nLocation information\n\n\nLOCATION_ELEV\nElevation in meters\n\n\nIGBP\nVegetation type\n\n\nMAT\nMean annual temperature in Celsius\n\n\nMAP\nMean annual precipitation in mm\n\n\n\nTake a look at the file:\n\nView(FluxNet) \n\nLook at the tower site locations:\n\nFluxNet %&gt;% ggplot( ) + geom_point( aes( x=LOCATION_LONG , y=LOCATION_LAT))\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nLook at the elevation, mean annual temperature, and mean annual precipitation for the tower site locations:\n\nFluxNet %&gt;% ggplot( aes(x=LOCATION_ELEV)) + \n  geom_histogram(color=\"black\", fill=\"white\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 50 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\nFluxNet %&gt;% ggplot( aes(x=MAT)) + \n  geom_histogram(color=\"black\", fill=\"white\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 59 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\nFluxNet %&gt;% ggplot( aes(x=MAP)) + \n  geom_histogram(color=\"black\", fill=\"white\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 60 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nWe are interested in exploring the sites with methane data. Lets subset by FLUXNET-CH4.\n\nFLUXNET.CH4 &lt;- FluxNet %&gt;% filter( FLUXNET.CH4 != \"\")\n\n\nView(FLUXNET.CH4 )\n\nThis object is currently a dataframe.\n\nclass(FLUXNET.CH4)\n\n[1] \"data.frame\"\n\n\nLets make it a simple feature using st_as_sf().\n\nFLUXNET.CH4.shp &lt;- st_as_sf(x = FLUXNET.CH4,                         \n           coords = c(\"LOCATION_LONG\",  \"LOCATION_LAT\"),\n           crs = \"+init=epsg:4326\")\n\nWarning in CPL_crs_from_input(x): GDAL Message 1: +init=epsg:XXXX syntax is\ndeprecated. It might return a CRS with a non-EPSG compliant axis order.\n\nggplot(data=FLUXNET.CH4.shp ) + geom_sf()\n\n\n\n\n\n\n\n\ncheck the class:\n\nclass(FLUXNET.CH4.shp)\n\n[1] \"sf\"         \"data.frame\"\n\n\nSimple features describe how objects in the real world can be represented in computers. They have a geometry describing where on earth the feature is located, and they have attributes, which describe other properties about the feature.\nLook at the information about the geometry:\n\nFLUXNET.CH4.shp$geometry\n\nGeometry set for 81 features \nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -163.7002 ymin: -37.3879 xmax: 175.5539 ymax: 71.3242\nGeodetic CRS:  WGS 84\nFirst 5 geometries:\n\n\nPOINT (11.3175 47.1167)\n\n\nPOINT (-56.412 -16.498)\n\n\nPOINT (22.3711 -18.9647)\n\n\nPOINT (23.1792 -19.5481)\n\n\nPOINT (-121.2984 61.3089)\n\n\nIf we print the first three features, we see their attribute values and an abridged version of the geometry.\n\nprint(FLUXNET.CH4.shp, n = 3)\n\nSimple feature collection with 81 features and 8 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -163.7002 ymin: -37.3879 xmax: 175.5539 ymax: 71.3242\nGeodetic CRS:  WGS 84\nFirst 3 features:\n  SITE_ID                 SITE_NAME FLUXNET2015 FLUXNET.CH4 LOCATION_ELEV IGBP\n1  AT-Neu                  Neustift   CC-BY-4.0   CC-BY-4.0           970  GRA\n2  BR-Npw Northern Pantanal Wetland               CC-BY-4.0           120  WSA\n3  BW-Gum                      Guma               CC-BY-4.0           950  WET\n   MAT  MAP                 geometry\n1  6.5  852  POINT (11.3175 47.1167)\n2 24.9 1486  POINT (-56.412 -16.498)\n3 21.0  460 POINT (22.3711 -18.9647)\n\n\n\n\nGeometrical Operations\nThere are many geometrical operations that can be used to achieve simple feature manipulation.\n\nmethods(class = \"sf\")\n\n  [1] [                            [[&lt;-                        \n  [3] [&lt;-                          $&lt;-                         \n  [5] aggregate                    anti_join                   \n  [7] arrange                      as.data.frame               \n  [9] cbind                        coerce                      \n [11] dbDataType                   dbWriteTable                \n [13] distinct                     dplyr_reconstruct           \n [15] drop_na                      duplicated                  \n [17] filter                       full_join                   \n [19] gather                       group_by                    \n [21] group_split                  identify                    \n [23] initialize                   inner_join                  \n [25] left_join                    merge                       \n [27] mutate                       nest                        \n [29] pivot_longer                 pivot_wider                 \n [31] plot                         points                      \n [33] print                        rbind                       \n [35] rename_with                  rename                      \n [37] right_join                   rowwise                     \n [39] sample_frac                  sample_n                    \n [41] select                       semi_join                   \n [43] separate_rows                separate                    \n [45] show                         slice                       \n [47] slotsFromS3                  spread                      \n [49] st_agr                       st_agr&lt;-                    \n [51] st_area                      st_as_s2                    \n [53] st_as_sf                     st_as_sfc                   \n [55] st_bbox                      st_boundary                 \n [57] st_break_antimeridian        st_buffer                   \n [59] st_cast                      st_centroid                 \n [61] st_collection_extract        st_concave_hull             \n [63] st_convex_hull               st_coordinates              \n [65] st_crop                      st_crs                      \n [67] st_crs&lt;-                     st_difference               \n [69] st_drop_geometry             st_exterior_ring            \n [71] st_filter                    st_geometry                 \n [73] st_geometry&lt;-                st_inscribed_circle         \n [75] st_interpolate_aw            st_intersection             \n [77] st_intersects                st_is_full                  \n [79] st_is_valid                  st_is                       \n [81] st_join                      st_line_merge               \n [83] st_m_range                   st_make_valid               \n [85] st_minimum_rotated_rectangle st_nearest_points           \n [87] st_node                      st_normalize                \n [89] st_point_on_surface          st_polygonize               \n [91] st_precision                 st_reverse                  \n [93] st_sample                    st_segmentize               \n [95] st_set_precision             st_shift_longitude          \n [97] st_simplify                  st_snap                     \n [99] st_sym_difference            st_transform                \n[101] st_triangulate_constrained   st_triangulate              \n[103] st_union                     st_voronoi                  \n[105] st_wrap_dateline             st_write                    \n[107] st_z_range                   st_zm                       \n[109] summarise                    text                        \n[111] transform                    transmute                   \n[113] ungroup                      unite                       \n[115] unnest                      \nsee '?methods' for accessing help and source code\n\n\nBelow we will explore a few methods:\nst_is_valid() and st_is_simple() return a boolean indicating whether a geometry is valid or simple.\n\nst_is_valid(FLUXNET.CH4.shp)\n\n [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[16] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[31] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[46] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[61] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[76] TRUE TRUE TRUE TRUE TRUE TRUE\n\n\nIf there were any issues with the geometery you can use st_make_valid() to fix. Since this is a point file there will be no issues with the geometry.\n\nChange the CRS to a projected EPGS with st_transform.\n Coordinate reference systems (CRS)  are like measurement units for coordinates: they specify which location on Earth a particular coordinate pair refers to. Simple feature have two attributes to store a CRS: epsg and proj4string. This implies that all geometries in a geometry list-column must have the same CRS. Both may be NA, e.g. in case the CRS is unknown, or when we work with local coordinate systems (e.g. inside a building, a body, or an abstract space).\nproj4string is a generic, string-based description of a CRS, understood by the PROJ library. It defines projection types and (often) defines parameter values for particular projections, and hence can cover an infinite amount of different projections. This library (also used by GDAL) provides functions to convert or transform between different CRS. epsg is the integer ID for a particular, known CRS that can be resolved into a proj4string. Some proj4string values can be resolved back into their corresponding epsg ID, but this does not always work.\nThe importance of having epsg values stored with data besides proj4string values is that the epsg refers to particular, well-known CRS, whose parameters may change (improve) over time; fixing only the proj4string may remove the possibility to benefit from such improvements, and limit some of the provenance of datasets, but may help reproducibility. Coordinate reference system transformations can be carried out using st_transform\n\nFLUXNET.CH4.shp = st_transform(FLUXNET.CH4.shp, '+init=epsg:4087')\n\nFLUXNET.CH4.shp\n\nSimple feature collection with 81 features and 8 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -18223020 ymin: -4162002 xmax: 19542570 ymax: 7939774\nProjected CRS: WGS 84 / World Equidistant Cylindrical\nFirst 10 features:\n   SITE_ID                 SITE_NAME FLUXNET2015 FLUXNET.CH4 LOCATION_ELEV IGBP\n1   AT-Neu                  Neustift   CC-BY-4.0   CC-BY-4.0           970  GRA\n2   BR-Npw Northern Pantanal Wetland               CC-BY-4.0           120  WSA\n3   BW-Gum                      Guma               CC-BY-4.0           950  WET\n4   BW-Nxr                   Nxaraga               CC-BY-4.0           950  GRA\n5   CA-SCB          Scotty Creek Bog               CC-BY-4.0           280  WET\n6   CA-SCC    Scotty Creek Landscape               CC-BY-4.0           285  ENF\n7   CH-Cha                    Chamau   CC-BY-4.0   CC-BY-4.0           393  GRA\n8   CH-Dav                     Davos   CC-BY-4.0   CC-BY-4.0          1639  ENF\n9   CH-Oe2            Oensingen crop   CC-BY-4.0   CC-BY-4.0           452  CRO\n10  CN-Hgu                  Hongyuan               CC-BY-4.0          3500  GRA\n    MAT    MAP                  geometry\n1   6.5  852.0   POINT (1259858 5245007)\n2  24.9 1486.0 POINT (-6279755 -1836549)\n3  21.0  460.0  POINT (2490339 -2111141)\n4  21.0  460.0  POINT (2580297 -2176085)\n5  -2.8  388.0 POINT (-13502876 6824876)\n6  -2.8  387.6 POINT (-13502965 6824764)\n7   9.5 1136.0  POINT (936241.4 5255415)\n8   2.8 1062.0   POINT (1097154 5211455)\n9   9.8 1155.0  POINT (860911.5 5263898)\n10  1.5  747.0  POINT (11420267 3656322)\n\n\nNow the Projected CRS is WGS 84 / World Equidistant Cylindrical.\nst_distance() returns a dense numeric matrix with distances between geometries:\n\nFLUXNET.CH4.shp$MeanDistance_km &lt;- st_distance(FLUXNET.CH4.shp, FLUXNET.CH4.shp) %&gt;% rowMeans()/1000\n\nFLUXNET.CH4.shp %&gt;% ggplot() + geom_sf(aes(col = MeanDistance_km ))\n\n\n\n\n\n\n\nFLUXNET.CH4.shp %&gt;% ggplot( aes(x=MeanDistance_km)) + \n  geom_histogram(color=\"black\", fill=\"white\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\n\n\n\nThe package AOI\nThe AOI package in R stands for Area of Interest. It is primarily used for geographic or spatial data analysis, particularly in defining, visualizing, and working with specific regions or “areas of interest” on maps. The package facilitates interaction with various geographic data sources, allowing users to work with location-based data in a flexible and intuitive way.\n\nKey Feature of the AOI Package:\nDefine Areas of Interest (AOIs): Users can define specific geographic regions of interest using coordinates, addresses, or administrative boundaries (e.g., cities, countries).\n\n\nKey Function of the AOI Package:\naoi_get(): Defines an Area of Interest based on a variety of inputs such as bounding box coordinates, administrative boundaries, or addresses.\nExample:\n\nlibrary(AOI)\n\n# Define an area of interest by coordinates\n# aoi.NY.bb &lt;- aoi_get(\"New York\")\naoi.NY &lt;- aoi_get(state=\"New York\")\n\n# aoi.NY.bb %&gt;% ggplot() + geom_sf()\naoi.NY %&gt;% ggplot() + geom_sf()\n\n\n\n\n\n\n\n\nCreates a simple feature of South America:\n\ns.america &lt;- aoi_get(country= \"South America\", union=T)\ns.america %&gt;% ggplot() + geom_sf()\n\n\n\n\n\n\n\n\n\n\nWhat is union =T doing?\n\n# Explore what happens when union = F here\n# ...\n\nRe-project the s.america to match Fluxnet.ch4:\n\naoi.SAmerica &lt;- st_transform( s.america , '+init=epsg:4087') \naoi.SAmerica  %&gt;% ggplot() + geom_sf()\n\n\n\n\n\n\n\n\nCreate a simple feature of Brazil and re-project it to match Fluxnet.ch4:\n\n# Create your simple feature object here\n# ...\n\nThe functions st_intersects(), st_disjoint(), st_touches(), st_crosses(), st_within(), st_contains(), st_overlaps(), st_equals(), st_covers(), st_covered_by(), st_equals_exact() and st_is_within_distance() all return a sparse matrix with matching (TRUE) indexes, or a full logical matrix:\n\n\n\nHow many towers are in South America?\n\nst_intersects(aoi.SAmerica, FLUXNET.CH4.shp)\n\nSparse geometry binary predicate list of length 1, where the predicate\nwas `intersects'\n 1: 2\n\nst_intersects(aoi.SAmerica, FLUXNET.CH4.shp, sparse = FALSE)\n\n      [,1] [,2]  [,3]  [,4]  [,5]  [,6]  [,7]  [,8]  [,9] [,10] [,11] [,12]\n[1,] FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n     [,13] [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24]\n[1,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n     [,25] [,26] [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36]\n[1,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n     [,37] [,38] [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48]\n[1,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n     [,49] [,50] [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60]\n[1,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n     [,61] [,62] [,63] [,64] [,65] [,66] [,67] [,68] [,69] [,70] [,71] [,72]\n[1,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n     [,73] [,74] [,75] [,76] [,77] [,78] [,79] [,80] [,81]\n[1,] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\n\nHow many towers are in Brazil?\n\n# Find out how many towers are in Brazil here\n# ...\n\nWhere possible geometric operations such as st_distance(), st_length() and st_area() report results with a units attribute appropriate for the CRS.\nCalculate the area of South America:\n\naoi.SAmerica$Area &lt;- st_area(aoi.SAmerica )\naoi.SAmerica$Area\n\n1.918234e+13 [m^2]\n\n\nCalculate the area of Brazil:\n\n# Calculate the area here\n# ...\n\n\nVisualize the global distribution of towers:\nFirst create a simple feature for all large terrestrial regions in Europe, Asia, the Americas, Africa, Australia and New Zealand:\n\naoi.terrestrial &lt;- aoi_get(country= c(\"Europe\",\"Asia\" ,\"North America\", \"South America\", \"Australia\",\"Africa\", \"New Zealand\"))\n\naoi.terrestrial  %&gt;% ggplot() + geom_sf()\n\n\n\n\n\n\n\n\nLook at the CRS:\n\naoi.terrestrial\n\nSimple feature collection with 171 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -180 ymin: -55.61183 xmax: 180 ymax: 83.64513\nGeodetic CRS:  WGS 84\nFirst 10 features:\n                       name                       geometry\n2                  Tanzania MULTIPOLYGON (((33.90371 -0...\n3                 W. Sahara MULTIPOLYGON (((-8.66559 27...\n4                    Canada MULTIPOLYGON (((-122.84 49,...\n5  United States of America MULTIPOLYGON (((-122.84 49,...\n6                Kazakhstan MULTIPOLYGON (((87.35997 49...\n7                Uzbekistan MULTIPOLYGON (((55.96819 41...\n9                 Indonesia MULTIPOLYGON (((141.0002 -2...\n10                Argentina MULTIPOLYGON (((-68.63401 -...\n11                    Chile MULTIPOLYGON (((-68.63401 -...\n12          Dem. Rep. Congo MULTIPOLYGON (((29.34 -4.49...\n\n\nRe-project the polygon to match FLUXNET.CH4.shp:\n\naoi.terrestrial &lt;- aoi_get(country= c(\"Europe\",\"Asia\" ,\"North America\", \"South America\", \"Australia\",\"Africa\", \"New Zealand\")) %&gt;% st_transform( 4087 ) \n\n\naoi.terrestrial\n\nSimple feature collection with 171 features and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -20037510 ymin: -6190681 xmax: 20037510 ymax: 9311333\nProjected CRS: WGS 84 / World Equidistant Cylindrical\nFirst 10 features:\n                       name                       geometry\n2                  Tanzania MULTIPOLYGON (((3774144 -10...\n3                 W. Sahara MULTIPOLYGON (((-964649 307...\n4                    Canada MULTIPOLYGON (((-13674486 5...\n5  United States of America MULTIPOLYGON (((-13674486 5...\n6                Kazakhstan MULTIPOLYGON (((9724867 547...\n7                Uzbekistan MULTIPOLYGON (((6230351 459...\n9                 Indonesia MULTIPOLYGON (((15696072 -2...\n10                Argentina MULTIPOLYGON (((-7640303 -5...\n11                    Chile MULTIPOLYGON (((-7640303 -5...\n12          Dem. Rep. Congo MULTIPOLYGON (((3266114 -50...\n\n\nVisualize the shapefile you created:\n\naoi.terrestrial %&gt;% ggplot() + geom_sf()\n\n\n\n\n\n\n\n\nUse ggplot to visualize the global distribution of Fluxnet CH4 sites:\n\nggplot() + geom_sf(data = aoi.terrestrial) + geom_sf(data = FLUXNET.CH4.shp) \n\n\n\n\n\n\n\n\nExtract the country from the world simple feature into FLUXNET.CH4.shp:\n\nFLUXNET.CH4.shp$Country &lt;- st_intersection( aoi.terrestrial, FLUXNET.CH4.shp)$name\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\n\nExplore the Fluxnet CH4 sites:\n\nnames(FLUXNET.CH4.shp)\n\n [1] \"SITE_ID\"         \"SITE_NAME\"       \"FLUXNET2015\"     \"FLUXNET.CH4\"    \n [5] \"LOCATION_ELEV\"   \"IGBP\"            \"MAT\"             \"MAP\"            \n [9] \"geometry\"        \"MeanDistance_km\" \"Country\"        \n\nggplot(data= FLUXNET.CH4.shp) + geom_point( aes(x=MAT, y=MAP))\n\nWarning: Removed 10 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nggplot(data= FLUXNET.CH4.shp) + geom_point( aes(x=MAT, y=MAP, col=IGBP))\n\nWarning: Removed 10 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\nFLUXNET.CH4.shp$IGBP &lt;- as.factor( FLUXNET.CH4.shp$IGBP)\nsummary(FLUXNET.CH4.shp$IGBP)\n\nBSV CRO CSH EBF ENF GRA  MF OSH SNO URB WAT WET WSA \n  2  13   1   4   7   9   1   2   1   1   2  37   1 \n\n\n\n\n\nWriting files using st_write:\nWhen writing, you can use the following arguments to control update and delete: update=TRUE causes an existing data source to be updated, if it exists; this option is by default TRUE for all database drivers, where the database is updated by adding a table.\ndelete_layer=TRUE causes st_write try to open the data source and delete the layer; no errors are given if the data source is not present, or the layer does not exist in the data source.\ndelete_dsn=TRUE causes st_write to delete the data source when present, before writing the layer in a newly created data source. No error is given when the data source does not exist. This option should be handled with care, as it may wipe complete directories or databases.\n\nwrite_sf(FLUXNET.CH4.shp, \"data/products/FLUXNET_CH4.shp\") \n\nIt is possible to create data.frame objects with geometry list-columns that are not of class sf by:\n\nFluxnet.ch4.df &lt;- as.data.frame(FLUXNET.CH4.shp)\n\nCheck the class:\n\nclass(Fluxnet.ch4.df)\n\n[1] \"data.frame\"\n\n\nSuch objects: no longer register which column is the geometry list-column no longer have a plot method, and lack all of the other dedicated methods listed above for class sf. To write this object:\n\nwrite.csv(Fluxnet.ch4.df, \"data/Fluxnet.ch4.df\") \n\n\n\nAdditional Reading:\nS. Scheider, B. Gräler, E. Pebesma, C. Stasch, 2016. Modelling spatio-temporal information generation. Int J of Geographic Information Science, 30 (10), 1980-2008. (open access) Stasch, C., S. Scheider, E. Pebesma, W. Kuhn, 2014. Meaningful Spatial Prediction and Aggregation. Environmental Modelling & Software, 51, (149–165, open access).\n\n\n\nAssignment:\nWrite a 3-page report on the distribution of tower sites discussing the strengths and weakness of the current tower representation. Please create 2 visualizations. You are welcome to use any additional data.",
    "crumbs": [
      "Introduction to R",
      "Vector Data"
    ]
  },
  {
    "objectID": "data_integration.html",
    "href": "data_integration.html",
    "title": "Introduction to Data Integration in R",
    "section": "",
    "text": "Data analysis often requires combining data from multiple sources, such as files, databases, APIs, or web scraping. R is a powerful and flexible tool for data integration, but it can also pose some challenges and pitfalls. In this workshop, you will learn some of the best ways to integrate data from multiple sources in R.\n\nWorkshop Goals:\n\nUnderstand techniques for data Integration.\nObtain information from different data sources.\nCombine tabular and vector data.\nCombine tabular and raster data.\nCombine tabular data by an ID and time.\n\n\n\nChoose the right package\nR has many packages that can help you import, merge, and manipulate data from different sources. Some of the most popular and useful ones for tables include readr, dplyr, tidyr, and purrr. These packages are part of the tidyverse, a collection of packages that share a consistent and coherent syntax and philosophy for data analysis. For spatial data, the sf and terra packages are useful.\n\n\n\nType of Data\nLibrary\n\n\n\n\nTabular\ntidyverse\n\n\nVector\nsf\n\n\nRaster\nterra\n\n\n\n\n\nUse pipes and functions\nOne of the best features of the tidyverse is the pipe operator (%&gt;%), which allows you to chain multiple functions together and pass the output of one function as the input of the next one. This way, you can create a data integration pipeline that is clear and logical, and that avoids intermediate variables and nested functions.\n\n\nCheck and validate data\nData integration can introduce errors or inconsistencies in your data, such as duplicates, mismatches, outliers, or invalid values. Therefore, it is important to check and validate your data before and after you integrate it from multiple sources. You can use various tools and techniques to do this, such as summary statistics, data visualization, data profiling, or data quality rules.\nIn this workshop you will begin to extract information about the FLUXNET CH4 tower sites.\n\n\nInstall Packages:\n\ninstall.packages(zoo)\n\n\n\nLoad Libraries:\n\nlibrary(sf)\nlibrary(terra)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(AOI)\nlibrary(zoo)\n\n\n\nIntegrating information from simple features\nImport the file FluxNet_Sites_2024.csv and call it FluxNet\n\nFluxNet &lt;- read.csv('data/FluxNet_Sites_2024.csv')\n\nThis dataset includes:\n\n\n\n\n\n\n\nColumn Name\nDescription\n\n\n\n\nSITE_ID\nUnique site id\n\n\nSITE_NAME\nSite name\n\n\nFLUXNET2015\nLicense information for the data for the two FLUXNET Products\n\n\nFLUXNET-CH4\nLicense information for the data for the two FLUXNET Products\n\n\nLOCATION_LAT\nLocation information\n\n\nLOCATION_LONG\nLocation information\n\n\nLOCATION_ELEV\nElevation in meters\n\n\nIGBP\nVegetation type\n\n\nMAT\nMean annual temperature in Celsius\n\n\nMAP\nMean annual precipitation in mm\n\n\n\nSubset file to incluse only sites measuring methane:\n\nFLUXNET.CH4 &lt;- FluxNet %&gt;% filter( FLUXNET.CH4 == \"CC-BY-4.0\" )\n\nNow that you have all of the sites measuring methane, convert FLUXNET.CH4 to a sf and call it FLUXNET.CH4.shp:\n\nFLUXNET.CH4.shp &lt;- st_as_sf(x = FLUXNET.CH4,                         \n           coords = c(\"LOCATION_LONG\",  \"LOCATION_LAT\"),\n           crs = 4326)\n\nggplot(data=FLUXNET.CH4.shp ) + geom_sf()\n\n\n\n\n\n\n\n\ncheck the class and that the geometry is valid:\n\nclass(FLUXNET.CH4.shp)\n\n[1] \"sf\"         \"data.frame\"\n\nst_is_valid(FLUXNET.CH4.shp)\n\n [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[16] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[31] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[46] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[61] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n[76] TRUE TRUE TRUE TRUE\n\n\nCreate a global sf and extract the country into the sf\n\nglobal &lt;- aoi_get(country= c(\"Europe\",\"Asia\" ,\"North America\", \"South America\", \"Australia\",\"Africa\", \"New Zealand\"))\n\nst_is_valid(global)\n\n  [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [13] FALSE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [25]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [37]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [49]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [61]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [73]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [85]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [97]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[109]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[121]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[133]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[145]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[157]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[169]  TRUE  TRUE  TRUE\n\n\nMake the CRS match:\n\nFLUXNET.CH4.shp = st_transform(FLUXNET.CH4.shp, crs= '+init=epsg:4087')\n\nWarning in CPL_crs_from_input(x): GDAL Message 1: +init=epsg:XXXX syntax is\ndeprecated. It might return a CRS with a non-EPSG compliant axis order.\n\nglobal = st_transform(global, crs= '+init=epsg:4087') %&gt;% st_make_valid()\n\nggplot() + geom_sf(data = global) + geom_sf(data = FLUXNET.CH4.shp) \n\n\n\n\n\n\n\n\nUse the st_intersect to extract the country of each tower site:\n\nFLUXNET.CH4.shp$Country &lt;- st_intersection( global, FLUXNET.CH4.shp)$name\n\nWarning: attribute variables are assumed to be spatially constant throughout\nall geometries\n\nFLUXNET.CH4.shp$Country \n\n [1] \"Austria\"                  \"Brazil\"                  \n [3] \"Botswana\"                 \"Botswana\"                \n [5] \"Canada\"                   \"Canada\"                  \n [7] \"Switzerland\"              \"Switzerland\"             \n [9] \"Switzerland\"              \"China\"                   \n[11] \"Germany\"                  \"Germany\"                 \n[13] \"Germany\"                  \"Germany\"                 \n[15] \"Finland\"                  \"Finland\"                 \n[17] \"Finland\"                  \"Finland\"                 \n[19] \"France\"                   \"China\"                   \n[21] \"Indonesia\"                \"Italy\"                   \n[23] \"Italy\"                    \"Japan\"                   \n[25] \"Japan\"                    \"Japan\"                   \n[27] \"South Korea\"              \"Malaysia\"                \n[29] \"Netherlands\"              \"New Zealand\"             \n[31] \"Philippines\"              \"Russia\"                  \n[33] \"Russia\"                   \"Russia\"                  \n[35] \"Russia\"                   \"Sweden\"                  \n[37] \"United Kingdom\"           \"United States of America\"\n[39] \"United States of America\" \"United States of America\"\n[41] \"United States of America\" \"United States of America\"\n[43] \"United States of America\" \"United States of America\"\n[45] \"United States of America\" \"United States of America\"\n[47] \"United States of America\" \"United States of America\"\n[49] \"United States of America\" \"United States of America\"\n[51] \"United States of America\" \"United States of America\"\n[53] \"United States of America\" \"United States of America\"\n[55] \"United States of America\" \"United States of America\"\n[57] \"United States of America\" \"United States of America\"\n[59] \"United States of America\" \"United States of America\"\n[61] \"United States of America\" \"United States of America\"\n[63] \"United States of America\" \"United States of America\"\n[65] \"United States of America\" \"United States of America\"\n[67] \"United States of America\" \"United States of America\"\n[69] \"United States of America\" \"United States of America\"\n[71] \"United States of America\" \"United States of America\"\n[73] \"United States of America\" \"United States of America\"\n[75] \"United States of America\" \"United States of America\"\n[77] \"United States of America\" \"United States of America\"\n[79] \"United States of America\"\n\n\n\n\nIntegrating information from rasters\nImport the file GlobalSoil_grids.tif :\n\nsoil &lt;- terra::rast(\"data/GlobalSoil_grids.tif\" )\nsoil\n\nclass       : SpatRaster \ndimensions  : 18000, 43200, 3  (nrow, ncol, nlyr)\nresolution  : 0.008333333, 0.008333333  (x, y)\nextent      : -180, 180, -60, 90  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource      : GlobalSoil_grids.tif \nnames       : BulkDensity,   PH, Nitrogen \nmin values  :         0.1,  3.5,      0.0 \nmax values  :         1.8, 10.0,     28.3 \nunit        :     kg dm-3,  H2O,     kg-1 \n\n\nTransform FLUXNET.CH4.shp to the same CRS as soils:\n\nFLUXNET.CH4.shp = st_transform(FLUXNET.CH4.shp, crs= crs(soil))\nFLUXNET.CH4.shp\n\nSimple feature collection with 79 features and 9 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -163.7002 ymin: -37.3879 xmax: 175.5539 ymax: 71.3242\nGeodetic CRS:  WGS 84\nFirst 10 features:\n   SITE_ID                 SITE_NAME FLUXNET2015 FLUXNET.CH4 LOCATION_ELEV IGBP\n1   AT-Neu                  Neustift   CC-BY-4.0   CC-BY-4.0           970  GRA\n2   BR-Npw Northern Pantanal Wetland               CC-BY-4.0           120  WSA\n3   BW-Gum                      Guma               CC-BY-4.0           950  WET\n4   BW-Nxr                   Nxaraga               CC-BY-4.0           950  GRA\n5   CA-SCB          Scotty Creek Bog               CC-BY-4.0           280  WET\n6   CA-SCC    Scotty Creek Landscape               CC-BY-4.0           285  ENF\n7   CH-Cha                    Chamau   CC-BY-4.0   CC-BY-4.0           393  GRA\n8   CH-Dav                     Davos   CC-BY-4.0   CC-BY-4.0          1639  ENF\n9   CH-Oe2            Oensingen crop   CC-BY-4.0   CC-BY-4.0           452  CRO\n10  CN-Hgu                  Hongyuan               CC-BY-4.0          3500  GRA\n    MAT    MAP                  geometry     Country\n1   6.5  852.0   POINT (11.3175 47.1167)     Austria\n2  24.9 1486.0   POINT (-56.412 -16.498)      Brazil\n3  21.0  460.0  POINT (22.3711 -18.9647)    Botswana\n4  21.0  460.0  POINT (23.1792 -19.5481)    Botswana\n5  -2.8  388.0 POINT (-121.2984 61.3089)      Canada\n6  -2.8  387.6 POINT (-121.2992 61.3079)      Canada\n7   9.5 1136.0    POINT (8.4104 47.2102) Switzerland\n8   2.8 1062.0    POINT (9.8559 46.8153) Switzerland\n9   9.8 1155.0    POINT (7.7337 47.2864) Switzerland\n10  1.5  747.0    POINT (102.59 32.8453)       China\n\n\nExtract soil information to FLUXNET.CH4.shp:\n\nFLUXNET.CH4.shp$SOIL_BulkDensity = terra::extract(soil, FLUXNET.CH4.shp)$BulkDensity\nFLUXNET.CH4.shp$SOIL_BulkDensity\n\n [1] 1.0 1.3 1.3 1.4 1.0 1.0 1.2 1.0 1.2 1.2 1.1 1.0 1.1 1.2 0.6 0.6 0.6 0.6 1.2\n[20] 0.8 1.0 1.3 1.2 1.0 1.1  NA 1.3 1.1 0.9 0.9 1.2 0.9 0.9 0.9 0.6 0.6  NA 0.5\n[39] 0.5 0.3 0.4 0.4 0.4 0.4 0.4 1.3 1.2 1.5 1.2 1.4 0.6 1.3 1.3 1.1 0.3 0.4 1.1\n[58] 1.2 1.1 1.2 1.2 1.2 1.3 0.4 0.5 1.3 1.3 1.1 1.2 1.2 1.4 1.3 1.2 1.2 1.2 1.2\n[77] 1.2 0.6 1.3\n\nFLUXNET.CH4.shp$SOIL_PH = terra::extract(soil, FLUXNET.CH4.shp)$PH\nFLUXNET.CH4.shp$SOIL_PH\n\n [1] 5.9 5.6 6.4 6.6 6.3 6.3 6.3 5.0 6.3 6.0 5.3 5.9 5.8 6.4 4.4 4.8 4.8 4.8 5.3\n[20] 5.3 4.7 7.4 6.7 5.3 5.9  NA 6.5 4.6 6.2 5.4 6.1 6.0 6.0 6.0 5.1 4.6  NA 5.9\n[39] 5.6 5.1 5.6 5.6 5.6 5.4 5.4 6.2 6.1 6.4 5.6 7.1 5.1 5.8 5.8 4.6 5.1 4.9 5.5\n[58] 5.4 4.9 5.2 5.9 6.9 4.5 5.4 5.0 6.2 6.2 5.1 7.0 7.0 6.6 5.1 6.2 6.0 6.0 6.2\n[77] 6.1 5.8 6.8\n\nFLUXNET.CH4.shp$SOIL_Nitrogen = terra::extract(soil, FLUXNET.CH4.shp)$Nitrogen\nFLUXNET.CH4.shp$SOIL_Nitrogen \n\n [1]  8.8  2.6  1.1  1.7  2.6  2.7  8.5  9.0  6.9  7.1  7.8  9.5  7.4  8.7 11.1\n[16]  9.5 11.4 11.0  6.4  4.1  3.2  3.8  5.5  9.0  6.0   NA  5.7  4.4  7.5  6.1\n[31]  4.0  7.8  7.9  6.3 10.6  8.9   NA 12.3 11.5 10.6  8.4  8.5  8.4 11.2 11.1\n[46]  8.0  8.6  5.8 15.3  5.3 10.6  2.9  2.9 12.2 14.9 13.5 10.8  9.7  5.5 14.7\n[61]  6.9  6.9  8.5 11.2 13.3  6.0  6.9  6.5  6.3  6.3  5.2  7.5  7.7  8.4  7.6\n[76]  7.7  8.3  7.3  7.1\n\n\nImport the climate information (GlobalClimate.tif) :\n\nclimate &lt;- terra::rast(\"data/GlobalClimate.tif\" )\nclimate\n\nclass       : SpatRaster \ndimensions  : 3343, 8640, 4  (nrow, ncol, nlyr)\nresolution  : 0.04166667, 0.04166667  (x, y)\nextent      : -180, 180, -55.625, 83.66667  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=longlat +ellps=WGS84 +no_defs \nsource      : GlobalClimate.tif \nnames       :     MAP,      TMIN,      TMAX,       MAT \nmin values  :     0.0,     -50.4,      -9.3, -27.10833 \nmax values  : 12390.2,      26.6,      53.7,  34.14167 \nunit        :      mm, Degrees C, Degrees C, Degrees C \n\n\nTransform FLUXNET.CH4.shp to the same CRS as climate:\n\nFLUXNET.CH4.shp = st_transform(FLUXNET.CH4.shp, crs= crs(climate))\nFLUXNET.CH4.shp\n\nSimple feature collection with 79 features and 12 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -163.7002 ymin: -37.3879 xmax: 175.5539 ymax: 71.3242\nGeodetic CRS:  GEOGCRS[\"unknown\",\n    DATUM[\"unknown\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]],\n    PRIMEM[\"unknown\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433,\n            ID[\"EPSG\",9122]]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433,\n                ID[\"EPSG\",9122]]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433,\n                ID[\"EPSG\",9122]]]]\nFirst 10 features:\n   SITE_ID                 SITE_NAME FLUXNET2015 FLUXNET.CH4 LOCATION_ELEV IGBP\n1   AT-Neu                  Neustift   CC-BY-4.0   CC-BY-4.0           970  GRA\n2   BR-Npw Northern Pantanal Wetland               CC-BY-4.0           120  WSA\n3   BW-Gum                      Guma               CC-BY-4.0           950  WET\n4   BW-Nxr                   Nxaraga               CC-BY-4.0           950  GRA\n5   CA-SCB          Scotty Creek Bog               CC-BY-4.0           280  WET\n6   CA-SCC    Scotty Creek Landscape               CC-BY-4.0           285  ENF\n7   CH-Cha                    Chamau   CC-BY-4.0   CC-BY-4.0           393  GRA\n8   CH-Dav                     Davos   CC-BY-4.0   CC-BY-4.0          1639  ENF\n9   CH-Oe2            Oensingen crop   CC-BY-4.0   CC-BY-4.0           452  CRO\n10  CN-Hgu                  Hongyuan               CC-BY-4.0          3500  GRA\n    MAT    MAP                  geometry     Country SOIL_BulkDensity SOIL_PH\n1   6.5  852.0   POINT (11.3175 47.1167)     Austria              1.0     5.9\n2  24.9 1486.0   POINT (-56.412 -16.498)      Brazil              1.3     5.6\n3  21.0  460.0  POINT (22.3711 -18.9647)    Botswana              1.3     6.4\n4  21.0  460.0  POINT (23.1792 -19.5481)    Botswana              1.4     6.6\n5  -2.8  388.0 POINT (-121.2984 61.3089)      Canada              1.0     6.3\n6  -2.8  387.6 POINT (-121.2992 61.3079)      Canada              1.0     6.3\n7   9.5 1136.0    POINT (8.4104 47.2102) Switzerland              1.2     6.3\n8   2.8 1062.0    POINT (9.8559 46.8153) Switzerland              1.0     5.0\n9   9.8 1155.0    POINT (7.7337 47.2864) Switzerland              1.2     6.3\n10  1.5  747.0    POINT (102.59 32.8453)       China              1.2     6.0\n   SOIL_Nitrogen\n1            8.8\n2            2.6\n3            1.1\n4            1.7\n5            2.6\n6            2.7\n7            8.5\n8            9.0\n9            6.9\n10           7.1\n\n\nLook at the data that is available in climate:\n\nnames(climate)\n\n[1] \"MAP\"  \"TMIN\" \"TMAX\" \"MAT\" \n\n\nExtract climate information to FLUXNET.CH4.shp:\n\nFLUXNET.CH4.shp$MAP = terra::extract(climate, FLUXNET.CH4.shp)$MAP\nFLUXNET.CH4.shp$TMIN = terra::extract(climate, FLUXNET.CH4.shp)$TMIN\nFLUXNET.CH4.shp$TMAX = terra::extract(climate, FLUXNET.CH4.shp)$TMAX\nFLUXNET.CH4.shp$MAT = terra::extract(climate, FLUXNET.CH4.shp)$MAT\n\nFLUXNET.CH4.shp$MAP\n\n [1] 1000.3 1322.5  447.9  418.3  430.0  430.0 1208.7 1093.9 1151.7  696.5\n[11]  591.8  608.0 1104.9  620.9  676.6  551.0  673.7  678.9  709.1 1854.9\n[21] 2388.6 1047.4  715.0 1128.5 1353.4 1191.7 1289.0 3475.8  865.8 1299.9\n[31] 2002.7  175.6  175.6  199.7  715.2  627.6  657.5  142.9  115.9  144.4\n[41]  290.0  290.0  290.0  118.0  118.0  392.8  386.1  872.4 1233.6  406.5\n[51]  421.5 1275.6 1275.6 1091.0  193.3  259.5 1568.0 1577.7  829.3 1222.2\n[61] 1204.9  362.9 1322.4  118.0  427.1  968.5  921.8  822.6  346.8  346.8\n[71]  543.0 1130.0  377.1  377.1  377.1  377.1  377.1  293.5  902.0\n\nFLUXNET.CH4.shp$TMIN\n\n [1]  -7.2  14.7   8.2   9.0 -26.9 -26.9  -2.3  -7.8  -2.0 -17.8  -2.6  -1.3\n[13]  -4.0  -2.3 -11.6 -17.8 -11.7 -11.6   0.3  12.1  23.0   5.5  -2.3 -11.6\n[25]  -2.2  -7.3 -11.3  22.5   0.2   4.1  21.7 -40.2 -40.2 -37.6 -10.7 -13.4\n[37]   2.2 -32.9 -30.0 -29.6 -25.2 -25.2 -25.2 -30.0 -30.0   3.0   3.3  -7.9\n[49]   8.6   5.3 -22.2   0.1   0.1 -15.4 -28.1 -26.9   6.6   6.0 -16.8   9.7\n[61]  -4.2   3.6   2.0 -30.0 -20.6  -6.4  -7.1 -16.7   3.6   3.6   3.7  -3.4\n[73]   3.2   3.2   3.2   3.2   3.2 -25.0  -7.2\n\nFLUXNET.CH4.shp$TMAX\n\n [1] 19.6 34.1 35.6 35.5 23.2 23.2 24.7 16.1 24.5 18.8 22.6 21.0 23.6 21.9 21.5\n[16] 18.0 21.8 21.6 25.7 32.0 32.2 30.5 29.4 25.3 30.2 27.7 28.2 32.3 22.2 23.8\n[31] 33.1 20.1 20.1 15.4 23.1 20.1 23.5 12.1  8.7 13.3 22.4 22.4 22.4  9.4  9.4\n[46] 31.6 31.4 28.8 33.3 24.6 19.4 33.2 33.2 26.0 16.9 16.0 32.5 32.5 24.7 33.7\n[61] 29.7 30.6 30.9  9.4 16.2 29.4 28.3 24.6 30.9 30.9 29.8 30.2 31.2 31.2 31.2\n[76] 31.2 31.2 22.3 28.3\n\nFLUXNET.CH4.shp$MAT\n\n [1]   5.3000002  25.4583340  23.7000008  23.9291668  -2.4583333  -2.4583333\n [7]   9.8541670   3.4625001   9.5000000   2.7291667   8.5916662   8.8041668\n[13]   8.7208338   8.6374998   3.4875000  -0.9458333   3.5250001   3.4875000\n[19]  11.4333334  23.0208340  27.5333328  16.7250004  12.7208338   6.9958334\n[25]  14.4083338  10.0500002  10.3291664  27.1291676  10.0583334  13.9125004\n[31]  27.0041676 -11.6458330 -11.6458330 -13.5208330   4.7874999   2.1708333\n[37]  11.3500004 -11.0791664 -11.2916670  -9.6458330  -2.1958334  -2.1958334\n[43]  -2.1958334 -11.1958332 -11.1958332  15.5958338  15.6083336   9.9541664\n[49]  22.2041664  14.9666662  -2.7583334  16.9291668  16.9291668   5.9833331\n[55]  -8.2500000  -7.8458333  20.5458336  20.2458324   4.4291668  22.8458328\n[61]  12.2375002  15.5166664  16.7333336 -11.1958332  -2.6541667  11.1958332\n[67]  10.1333332   4.6458335  15.6875000  15.6875000  15.4750004  13.1999998\n[73]  15.4708338  15.4708338  15.4708338  15.4708338  15.4708338  -2.2583334\n[79]  10.1416664\n\n\nImport elevation information (Elevation.tif):\n\nelevation &lt;- terra::rast(\"data/Elevation.tif\" )\nelevation\n\nclass       : SpatRaster \ndimensions  : 4320, 8640, 1  (nrow, ncol, nlyr)\nresolution  : 0.04166667, 0.04166667  (x, y)\nextent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource      : Elevation.tif \nname        : wc2.1_2.5m_elev \nmin value   :            -415 \nmax value   :            7412 \nunit        :               m \n\n\nTransform FLUXNET.CH4.shp to the same CRS as elevation:\n\nFLUXNET.CH4.shp = st_transform(FLUXNET.CH4.shp, crs= crs(elevation))\n\nLook at the data that is available in elevation:\n\nnames(elevation)\n\n[1] \"wc2.1_2.5m_elev\"\n\n\nExtract elevation information to FLUXNET.CH4.shp:\n\nFLUXNET.CH4.shp$ELEVATION = terra::extract(elevation, FLUXNET.CH4.shp)$wc2.1_2.5m_elev\nFLUXNET.CH4.shp$ELEVATION \n\n [1] 1445  126  981  950  244  244  410 1738  462 3506   89    7  592   11  167\n[16]  320  172  180  157    5   15    3   84   15   18  853  174   21   -2    7\n[31]   33    4    4    5  262  239   32    3    2   17  120  120  120    5    5\n[46]   -4   -5  177   20   -1  657   64   64   67  923  575    3    1  486   11\n[61]    3    2   12    5   54  238  175  473    4    4    3    5   -4   -4   -4\n[76]   -4   -4  144  173\n\n\n\n\nJoining tables\nWe can combine columns from two (or more) tables together. This can be achieved using the join family of functions in dplyr. There are different types of joins that will result in different outcomes.\ninner_join() includes all rows that appear in both the first data frame (x) and the second data frame (y).\nleft_join() returns all rows from x based on matching rows on shared columns in y. right_join() is the companion to left_join(), but returns all rows included in y based on matching rows on shared columns in x.\nImport APPEEARS file where I requested MODIS NDVI and EVI data for all FLUXNET_sites (Data/ENV720-MOD13A3-061-results.csv):\n\nFLUXNET &lt;- read.csv(\"data/ENV720-MOD13A3-061-results.csv\")\nnames(FLUXNET)\n\n [1] \"ID\"                                                                         \n [2] \"Latitude\"                                                                   \n [3] \"Longitude\"                                                                  \n [4] \"Date\"                                                                       \n [5] \"MODIS_Tile\"                                                                 \n [6] \"MOD13A3_061_Line_Y_1km\"                                                     \n [7] \"MOD13A3_061_Sample_X_1km\"                                                   \n [8] \"MOD13A3_061__1_km_monthly_EVI\"                                              \n [9] \"MOD13A3_061__1_km_monthly_NDVI\"                                             \n[10] \"MOD13A3_061__1_km_monthly_VI_Quality\"                                       \n[11] \"MOD13A3_061__1_km_monthly_VI_Quality_bitmask\"                               \n[12] \"MOD13A3_061__1_km_monthly_VI_Quality_MODLAND\"                               \n[13] \"MOD13A3_061__1_km_monthly_VI_Quality_MODLAND_Description\"                   \n[14] \"MOD13A3_061__1_km_monthly_VI_Quality_VI_Usefulness\"                         \n[15] \"MOD13A3_061__1_km_monthly_VI_Quality_VI_Usefulness_Description\"             \n[16] \"MOD13A3_061__1_km_monthly_VI_Quality_Aerosol_Quantity\"                      \n[17] \"MOD13A3_061__1_km_monthly_VI_Quality_Aerosol_Quantity_Description\"          \n[18] \"MOD13A3_061__1_km_monthly_VI_Quality_Adjacent_cloud_detected\"               \n[19] \"MOD13A3_061__1_km_monthly_VI_Quality_Adjacent_cloud_detected_Description\"   \n[20] \"MOD13A3_061__1_km_monthly_VI_Quality_Atmosphere_BRDF_Correction\"            \n[21] \"MOD13A3_061__1_km_monthly_VI_Quality_Atmosphere_BRDF_Correction_Description\"\n[22] \"MOD13A3_061__1_km_monthly_VI_Quality_Mixed_Clouds\"                          \n[23] \"MOD13A3_061__1_km_monthly_VI_Quality_Mixed_Clouds_Description\"              \n[24] \"MOD13A3_061__1_km_monthly_VI_Quality_Land.Water_Mask\"                       \n[25] \"MOD13A3_061__1_km_monthly_VI_Quality_Land.Water_Mask_Description\"           \n[26] \"MOD13A3_061__1_km_monthly_VI_Quality_Possible_snow.ice\"                     \n[27] \"MOD13A3_061__1_km_monthly_VI_Quality_Possible_snow.ice_Description\"         \n[28] \"MOD13A3_061__1_km_monthly_VI_Quality_Possible_shadow\"                       \n[29] \"MOD13A3_061__1_km_monthly_VI_Quality_Possible_shadow_Description\"           \n\n\nSubset the FLUXNET dataset to include only the columns of interest and rename them:\n\nFLUXNET.sub &lt;- FLUXNET %&gt;% select( \"ID\",\n\"Date\",\n\"MOD13A3_061__1_km_monthly_EVI\", \"MOD13A3_061__1_km_monthly_NDVI\", \"MOD13A3_061__1_km_monthly_VI_Quality\") %&gt;% \nrename( SITE_ID = ID,\nEVI = MOD13A3_061__1_km_monthly_EVI,\nNDVI = MOD13A3_061__1_km_monthly_NDVI, \nQAQC = MOD13A3_061__1_km_monthly_VI_Quality ) %&gt;% filter( QAQC &gt; 0)\n\nnames(FLUXNET.sub)\n\n[1] \"SITE_ID\" \"Date\"    \"EVI\"     \"NDVI\"    \"QAQC\"   \n\n\nMake the FLUXNET.CH4.shp vector a dataframe:\n\nFLUXNET_CH4 &lt;- as.data.frame( FLUXNET.CH4.shp)\n\nIdentify the column that you should use to join the datasets FLUXNET_CH4 and FLUXNET.sub:\n\nFLUXNET_CH4$SITE_ID\nFLUXNET.sub$SITE_ID\n\nUse left_join because you want to keep all the data from FLUXNET_CH4 and only the sites in FLUXNET.sub that match the sites in FLUXNET_CH4:\n\nFLUXNET_CH4_final &lt;- FLUXNET_CH4 %&gt;% left_join( FLUXNET.sub, by= 'SITE_ID')\n\nCheck to make sure the list of sites matches:\n\nlength( unique(FLUXNET_CH4$SITE_ID))\n\n[1] 79\n\nlength(unique(FLUXNET_CH4_final$SITE_ID))\n\n[1] 79\n\n\nImport the monthly FLUX data:\n\nload( \"data/FLUXNET_FLUXES.RDATA\")\n\nLook at the flux file “FLUXNET.flux”:\n\nsummary( FLUXNET.flux)\n\n    YearMon         SITE                P_F               TA_F        \n Min.   :2006   Length:2370        Min.   :   0.00   Min.   :-37.188  \n 1st Qu.:2014   Class :character   1st Qu.:  14.42   1st Qu.:  2.863  \n Median :2015   Mode  :character   Median :  43.10   Median : 11.123  \n Mean   :2015                      Mean   :  69.08   Mean   :  9.543  \n 3rd Qu.:2017                      3rd Qu.:  84.12   3rd Qu.: 17.540  \n Max.   :2019                      Max.   :1094.02   Max.   : 31.280  \n     VPD_F           NEE_F_gC          FCH4_F_gC      \n Min.   : 0.000   Min.   :-383.426   Min.   :-1.8194  \n 1st Qu.: 1.635   1st Qu.: -24.661   1st Qu.: 0.1008  \n Median : 3.982   Median :   2.918   Median : 0.4034  \n Mean   : 4.763   Mean   :   3.815   Mean   : 1.3906  \n 3rd Qu.: 6.722   3rd Qu.:  19.432   3rd Qu.: 1.4318  \n Max.   :29.075   Max.   :1251.787   Max.   :28.3012  \n\nnames(FLUXNET.flux)\n\n[1] \"YearMon\"   \"SITE\"      \"P_F\"       \"TA_F\"      \"VPD_F\"     \"NEE_F_gC\" \n[7] \"FCH4_F_gC\"\n\n\n\n# Make SITE ID:\nFLUXNET.flux$SITE_ID &lt;- FLUXNET.flux$SITE\nFLUXNET.flux$SITE_ID\n# This is a good column to join based on:\nFLUXNET.flux$YearMon\nFLUXNET.flux$YearMon\n\nIn your site file, convert the date to Year-Month and use this column to join it with the flux file “FLUXNET.flux”:\n\nnames(FLUXNET_CH4_final)\n\n [1] \"SITE_ID\"          \"SITE_NAME\"        \"FLUXNET2015\"      \"FLUXNET.CH4\"     \n [5] \"LOCATION_ELEV\"    \"IGBP\"             \"MAT\"              \"MAP\"             \n [9] \"geometry\"         \"Country\"          \"SOIL_BulkDensity\" \"SOIL_PH\"         \n[13] \"SOIL_Nitrogen\"    \"TMIN\"             \"TMAX\"             \"ELEVATION\"       \n[17] \"Date\"             \"EVI\"              \"NDVI\"             \"QAQC\"            \n\nsummary( FLUXNET_CH4_final$Date)\n\n   Length     Class      Mode \n    12264 character character \n\n\n\n# Format as a Date:\nFLUXNET_CH4_final$Date.f &lt;-FLUXNET_CH4_final$Date %&gt;% as.Date(format='%Y-%m-%d')\n\n# Format as a Yearmon:\nFLUXNET_CH4_final$YearMon &lt;- FLUXNET_CH4_final$Date.f %&gt;% zoo::as.yearmon( \"%m-%Y\")\n\nclass(FLUXNET_CH4_final$YearMon)\n\n[1] \"yearmon\"\n\n\nJoin the two files:\n\nfluxes_month &lt;- FLUXNET.flux %&gt;% left_join(FLUXNET_CH4_final , by = c ('YearMon', 'SITE_ID'))\n\nSave your file:\n\nsave(fluxes_month, file=\"data/products/Monthly_Fluxes.RDATA\" )\n\nYou are now prepared to take data from different sources to build a file to explore patterns in methane infrastructure. Take note of the difference between joining site based static data versus site based data that changes over time. In your next in class assessment, you will build a file with all of the data you want to use to develop your methane model. To prepare for this, find and obtain your data sources.",
    "crumbs": [
      "Data Integration in R",
      "Data Integration"
    ]
  },
  {
    "objectID": "data_basics.html",
    "href": "data_basics.html",
    "title": "Data Basics",
    "section": "",
    "text": "In this workshop you will:\n\nDownload climate data with daymetr\nManipulate data with tidyverse:\n\nFormat date elements\nUse select() to choose variables from a data frame.\nUse filter() to choose data based on values.\nUse mutate() to create new variables.\nUse reframe() to summarize datasets.\nUse full_join() to merge datasets\n\nVisualize data with ggplot2\nLearn to write basic functions",
    "crumbs": [
      "Introduction to R",
      "Data Basics"
    ]
  },
  {
    "objectID": "data_basics.html#learn-the-basics-of-manipulating-data-in-r",
    "href": "data_basics.html#learn-the-basics-of-manipulating-data-in-r",
    "title": "Data Basics",
    "section": "",
    "text": "In this workshop you will:\n\nDownload climate data with daymetr\nManipulate data with tidyverse:\n\nFormat date elements\nUse select() to choose variables from a data frame.\nUse filter() to choose data based on values.\nUse mutate() to create new variables.\nUse reframe() to summarize datasets.\nUse full_join() to merge datasets\n\nVisualize data with ggplot2\nLearn to write basic functions",
    "crumbs": [
      "Introduction to R",
      "Data Basics"
    ]
  },
  {
    "objectID": "data_basics.html#calling-a-function-in-r",
    "href": "data_basics.html#calling-a-function-in-r",
    "title": "Data Basics",
    "section": "Calling a Function in R",
    "text": "Calling a Function in R\nIn all the above examples, we called the created functions by using the function name and adding the necessary arguments inside the parenthesis. In R, function arguments can be passed by position, by name (so-called named arguments), by mixing position-based and name-based matching, or by omitting the arguments altogether.\nIf we pass the arguments by position, we need to follow the same sequence of arguments as defined in the function:\n\nsubtract_two_nums &lt;- function(x, y){\n    x - y\n}\n\nprint(subtract_two_nums(3, 1))\n\n[1] 2\n\n\nIn the above example, x is equal to 3 and y to 1, and not vice versa.\nIf we pass the arguments by name, i.e., explicitly specify what value each parameter defined in the function takes, the order of the arguments doesn’t matter:\n\nsubtract_two_nums &lt;- function(x, y){\n  x - y\n}\nprint(subtract_two_nums(x=3, y=1))\n\n[1] 2\n\nprint(subtract_two_nums(y=1, x=3))\n\n[1] 2\n\n\nThings to remember when using functions inside other functions: - Functions can have default values for arguments. - Functions can return multiple values using a list or other data structures. - You can use the return() statement to specify the value to be returned by the function. - If you want to be able to use the function independent of another function, it should be created outside a function instead of nesting the functions.",
    "crumbs": [
      "Introduction to R",
      "Data Basics"
    ]
  },
  {
    "objectID": "spatial_projections.html",
    "href": "spatial_projections.html",
    "title": "Spatial Projections of a randomForest model",
    "section": "",
    "text": "In this workshop, we will create a spatial projection of our random forest model for monthly methane exchange from natural ecosystems.\nTo date, we have completed model calibration, validation, and sensitivity analysis. Next, we can apply the model to a landscape to estimate natural methane emissions. For this workshop, we will calculate Connecticut’s natural emissions.\n\nIn this workshop, we will:\n\nMake a list of the variables, their units, and the exact name and class of each variable in your model.\nDetermine where you can get a spatial version of each variable in your model.\nFormat each spatial layer to match the exact conditions of the data used to fit the model.\nMake spatial predictions.\nUse predictions to calculate an annual budget.\n\nLoad libraries:\n\nlibrary(randomForest)\nlibrary(tidyverse)\nlibrary(tidyterra)\n\n\n\n(1) Make a list of the variables, their units, the exact name and class of each variable in your model.\nLoad the datasets and the model.\n\nload(file=\"data/final_model.RDATA\" )\n\nThere are four items in this.RDATA file.\n(1) the randomForest model,\n(2) the flux net dataset,\n(3) the training data, and\n(4) the testing data.\n\nLook at the model to determine which variables are in it:\n\nFCH4_F_gC.rf\n\n\nCall:\n randomForest(formula = FCH4_F_gC ~ P_F + TA_F + Upland, data = train,      keep.forest = T, importance = TRUE, mtry = 1, ntree = 500,      keep.inbag = TRUE) \n               Type of random forest: regression\n                     Number of trees: 500\nNo. of variables tried at each split: 1\n\n          Mean of squared residuals: 4.40718\n                    % Var explained: 32.54\n\n\nThe model includes precipitation in mm (P_F), mean air temperature in degrees Celsius (TA_F), and an indicator for upland ecosystems (Upland).\nCheck the class of each variable.\n\nclass(train$P_F)\n\n[1] \"numeric\"\n\nclass(train$TA_F)\n\n[1] \"numeric\"\n\nclass(train$Upland)\n\n[1] \"factor\"\n\n\nTo project this model in space, we need the following variables:\n\nMonthly total precipitation in mm and the name of the layer needs to be “P_F”\nMonthly mean air temperature in degrees Celsius and the layer name needs to be “TA_F”\nWe need an indicator for upland ecosystems called Upland. All inundated ecosystems (+ snow) are given the value “0” and non-inundated ecosystems are given the value “1”. Croplands and urban areas should be filtered out of this layer.\n\n\n\n\n(2) Determine where you can get a spatial version of each variable in your model.\nWe will spatialize the model for Connecticut in the year 2021.\n\nMonthly total precipitation (mm): Terra climate (getTerraClim()).\nMonthly mean air temperature temperature in degrees Celsius: Terra climate (getTerraClim()).\nIndicator for Upland ecosystems (Upland): MODIS Land Cover Data (Majority_Land_Cover_Type_1). downloaded from: (2001 - 2022) https://lpdaac.usgs.gov/products/mcd12c1v061/ the user guide is available here: https://lpdaac.usgs.gov/documents/101/MCD12_User_Guide_V6.pdf.\n\n\nTo use raster layers with the predict function, they must have the same CRS, resolution, and extent!\n#(3) Format each spatial layer. Download the climate layers needed for P_F and T_F using getTerraClim().\n\nlibrary(AOI)\nlibrary(climateR)\nlibrary(terra)\nlibrary(tidyverse)\n\nCreate an AOI for Connecticut.\n\nct &lt;- AOI::aoi_get(state=\"CT\")\nplot(ct$geometry)\n\n\n\n\n\n\n\n\nDownload terra climate data (Precipitation and air temperature) for 2021.\n\nglobal.clim.N &lt;- ct %&gt;% getTerraClim(varname = c(\"ppt\", \"tmin\", \"tmax\"), \n                                     startDate = \"2021-01-01\",\n                                     endDate = \"2021-12-31\")\n\nSubset the data for each variable.\n\nglobal.clim.ppt &lt;- global.clim.N$ppt\nglobal.clim.tmin &lt;-global.clim.N$tmin\nglobal.clim.tmax &lt;- global.clim.N$tmax \n\nWe need mean air temperature. Calculate the mean using the maximum and minimum air temperature.\n\nglobal.clim.tmean &lt;-   mean(global.clim.tmin, global.clim.tmax)\nglobal.clim.tmean\n\nclass       : SpatRaster \ndimensions  : 28, 48, 13  (nrow, ncol, nlyr)\nresolution  : 0.04166674, 0.04166679  (x, y)\nextent      : -73.75, -71.75, 40.91667, 42.08334  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=longlat +ellps=WGS84 +no_defs \nsource(s)   : memory\nnames       : tmin_~total, tmin_~total, tmin_~total, tmin_~total, tmin_~total, tmin_~total, ... \nmin values  :       -5.00,        -5.9,         1.3,        7.05,       12.20,       18.65, ... \nmax values  :        1.55,         1.3,         6.0,       11.15,       15.75,       22.00, ... \ntime        : 2021-01-01 to 2022-01-01 UTC \n\n\nRename the layers to start with “tmean” to match the data.\n\nnames(global.clim.tmean) &lt;- gsub(pattern = \"tmin\", replacement = \"tmean\", x = names(global.clim.tmean))\nglobal.clim.tmean\n\nclass       : SpatRaster \ndimensions  : 28, 48, 13  (nrow, ncol, nlyr)\nresolution  : 0.04166674, 0.04166679  (x, y)\nextent      : -73.75, -71.75, 40.91667, 42.08334  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=longlat +ellps=WGS84 +no_defs \nsource(s)   : memory\nnames       : tmean~total, tmean~total, tmean~total, tmean~total, tmean~total, tmean~total, ... \nmin values  :       -5.00,        -5.9,         1.3,        7.05,       12.20,       18.65, ... \nmax values  :        1.55,         1.3,         6.0,       11.15,       15.75,       22.00, ... \ntime        : 2021-01-01 to 2022-01-01 UTC \n\n\nRemove the layers you no longer need.\n\nrm(global.clim.tmin, global.clim.tmax)\n\nSave the layers.\n\nwriteRaster(global.clim.tmean, \"data/TERRA_TMEAN_2021_CT.tif\", overwrite=TRUE )\nwriteRaster(global.clim.ppt, \"data/TERRA_PPT_2021_CT.tif\", overwrite=TRUE )\n\nNow, we need to get the MODIS IGBP layers. The dataset provided was developed from MODIS Land Cover Data (Majority_Land_Cover_Type_1) downloaded from: (2001 - 2022) https://lpdaac.usgs.gov/products/mcd12c1v061/. This dataset was downloaded for the entire globe and cropped to include only Connecticut.\nLoad the data.\n\nigbp.ct &lt;- terra::rast(\"data/MODIS_IGBP_2001-2022_CT.tif\")\nigbp.ct \n\nclass       : SpatRaster \ndimensions  : 22, 39, 22  (nrow, ncol, nlyr)\nresolution  : 0.05, 0.05  (x, y)\nextent      : -73.75, -71.8, 40.95, 42.05  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat Unknown datum based upon the Clarke 1866 ellipsoid \nsource      : MODIS_IGBP_2001-2022_CT.tif \nnames       : Major~ype_1, Major~ype_1, Major~ype_1, Major~ype_1, Major~ype_1, Major~ype_1, ... \nmin values  :           0,           0,           0,           0,           0,           0, ... \nmax values  :          14,          13,          14,          14,          14,          13, ... \ntime (raw)  : 992563200 to 1655251200 \n\nigbp.ct[[1]] %&gt;% plot\n\n\n\n\n\n\n\n\nThis layer needs to be reformatted. Using the User Guide we can determine what each numerical value represents: https://lpdaac.usgs.gov/documents/1409/MCD12_User_Guide_V61.pdf\n1: ENF. 2: EBF. 3: DNF. 4: DBF. 5: MF. 6: CS. 7: OS. 8: WS. 9 : SAV. 10 : GRA. 11: WET. 12 : CRO. 13 : URB. 14 : CRO. 15 : SNO. 16: Barren. 17 : WAT. 0: Unclassified.\nlook at the layer. Here I use”[[1]]” to see only the first layer, which is for the year 2001.\n\nterra::plot(igbp.ct[[1]])\n\n\n\n\n\n\n\n\nReclassify each value, one at a time, and think about how you should reclassify each. We want to give all uplands the value “1” and all inundated systems the value “0”.\nFirst, make a copy of the raters (igbp.ct) and call it igbp.ct.r:\n\nigbp.ct.r &lt;- igbp.ct\n\nReclassify 0 value to NA.\n\nigbp.ct.r[ igbp.ct.r == 0] &lt;- NA \nterra::plot(igbp.ct.r[[1]] )\n\n\n\n\n\n\n\n\nReclassify the other values:\n\nigbp.ct.r[ igbp.ct.r == 1] &lt;- 1 \nigbp.ct.r[ igbp.ct.r == 2] &lt;- 1\nigbp.ct.r[ igbp.ct.r == 3] &lt;- 1\nigbp.ct.r[ igbp.ct.r == 4] &lt;- 1\nigbp.ct.r[ igbp.ct.r == 5] &lt;- 1\nigbp.ct.r[ igbp.ct.r == 6] &lt;- 1\nigbp.ct.r[ igbp.ct.r == 7] &lt;- 1\nigbp.ct.r[ igbp.ct.r == 8] &lt;- 1\nigbp.ct.r[ igbp.ct.r == 9] &lt;- 1\nigbp.ct.r[ igbp.ct.r == 10] &lt;- 1\nigbp.ct.r[ igbp.ct.r == 11] &lt;- 0\nigbp.ct.r[ igbp.ct.r == 12] &lt;- NA\nigbp.ct.r[ igbp.ct.r == 13] &lt;- NA\nigbp.ct.r[ igbp.ct.r == 14] &lt;- NA\nigbp.ct.r[ igbp.ct.r == 15] &lt;- 0\nigbp.ct.r[ igbp.ct.r == 16] &lt;- 1\nigbp.ct.r[ igbp.ct.r == 17] &lt;- 0\n\nLook at the final raster to ensure everything is reclassified to upland since Connecticut doesn’t have anything else at the resolution of MODIS.\n\nterra::plot(igbp.ct.r[[1]], col='red' ) \n\n\n\n\n\n\n\n\nFormat the upland layer as a factor by first making a data frame that has the raster values 0 and 1 and the corresponding factor level.\n\nfactors.df &lt;- data.frame(id=c(1, 0), cover=c(\"upland\", \"inundated\"))\n\nCreate a for loop to assign the factor levels to each raster layer one at a time:\n\nfor ( i in 1:22){\n  print(i)\n  levels(igbp.ct.r[[i]]) &lt;- factors.df\n  is.factor(igbp.ct.r)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n[1] 11\n[1] 12\n[1] 13\n[1] 14\n[1] 15\n[1] 16\n[1] 17\n[1] 18\n[1] 19\n[1] 20\n[1] 21\n[1] 22\n\nterra::plot(igbp.ct.r[[1]], col=\"red\" )\n\n\n\n\n\n\n\n\nWe only need the layer for 2021. Subset the 2021 layer.\n\nigbp.ct.r.2021 &lt;- igbp.ct.r[[21]]\n\nWe will use the CRS of the terra climate layers and make everything match this.\n\nigbp.ct.r.2021 &lt;- terra::project( igbp.ct.r.2021, global.clim.ppt)\n\nAll the resolutions must be the same to combine the rasters into one item. We will set the terra climate layers to match the igbp.ct.r layer:\n\nglobal.clim.tmean.resample &lt;- resample( global.clim.tmean, igbp.ct.r.2021)\nglobal.clim.ppt.resample &lt;- resample( global.clim.ppt, igbp.ct.r.2021)\n\nNow, export the files to save a version processed as needed.\n\nwriteRaster(global.clim.tmean.resample, \"data/products/TERRA_TMEAN_2021_CT_rs.tif\", overwrite=TRUE )\nwriteRaster(global.clim.ppt.resample, \"data/products/TERRA_PPT_2021_CT_rs.tif\", overwrite=TRUE )\nwriteRaster(igbp.ct.r.2021, \"data/products/MODIS_Upland_2021_CT.tif\", overwrite=TRUE )\n\n\n\n\n(4) Make predictions\nCombine all the variables into a raster stack, only including one month since igbp.ct.r.2021 has one layer and the climate has 12, one for each month.\n\nmodel.rasters.m1 &lt;- c(igbp.ct.r.2021, global.clim.tmean.resample[[1]], global.clim.ppt.resample[[1]] )\n\nIf you have any issues combining the raster layers into one object, you may not have made everything the same resolution or extent.\nMake the names of the raster layers match the dataframe.\n\nnames(model.rasters.m1 ) &lt;- c(\"Upland\", \"TA_F\", \"P_F\" )\nmodel.rasters.m1\n\nclass       : SpatRaster \ndimensions  : 28, 48, 3  (nrow, ncol, nlyr)\nresolution  : 0.04166674, 0.04166679  (x, y)\nextent      : -73.75, -71.75, 40.91667, 42.08334  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=longlat +ellps=WGS84 +no_defs \nsource(s)   : memory\nnames       : Upland,  TA_F,  P_F \nmin values  : upland, -5.00, 41.2 \nmax values  : upland,  1.55, 65.6 \nunit        :       ,      ,   mm \n\n\nCheck the dataframe again to ensure you don’t need to make additional changes to the raster.\n\nclass(train$Upland )\n\n[1] \"factor\"\n\nsummary(train$Upland )\n\ninundated    upland \n      974       697 \n\nlevels(train$Upland )\n\n[1] \"inundated\" \"upland\"   \n\nmodel.rasters.m1$Upland\n\nclass       : SpatRaster \ndimensions  : 28, 48, 1  (nrow, ncol, nlyr)\nresolution  : 0.04166674, 0.04166679  (x, y)\nextent      : -73.75, -71.75, 40.91667, 42.08334  (xmin, xmax, ymin, ymax)\ncoord. ref. : +proj=longlat +ellps=WGS84 +no_defs \nsource(s)   : memory\ncategories  : cover \nname        : Upland \nmin value   : upland \nmax value   : upland \n\n\nYou are ready to use the predict function to predict you model in space.\n\nmodel.rasters.m1.pred &lt;- terra::predict(  object= model.rasters.m1, model=FCH4_F_gC.rf)\n\nplot(model.rasters.m1.pred)\n\n\n\n\n\n\n\n\nWe can do this in a for loop to get all 12 months.\nFirst, determine where you want to export the files to, and make a new folder there called predictions.\n\nsetwd('data/products/') # sets the working directory to your products folder\ndirectory &lt;- getwd() # saves the path to directory\nsubDir &lt;- 'predictions' # You will use this to make the folder called predictions\n\ndir.create(file.path(directory , subDir)) # this makes the new folder in data called predictions\n\nWarning in dir.create(file.path(directory, subDir)):\n'/Users/ac3656/GitHub/EDS_course/data/products/predictions' already exists\n\npredictions.dir &lt;- paste(directory,\"/\",subDir, sep=\"\")\n\nMake the forloop to make predictions for all 12 months.\n\nfor ( i in 1:12){\n \n  print(i)\n  \n  model.rasters &lt;- c(igbp.ct.r.2021, global.clim.tmean.resample[[i]], global.clim.ppt.resample[[i]] )\n  names(model.rasters) &lt;- c(\"Upland\", \"TA_F\", \"P_F\" )\n  pred &lt;- terra::predict(  object= model.rasters, model=FCH4_F_gC.rf)\n  \n  units(pred) &lt;- 'g C m-2 month-1' # Add the units\n  names(pred ) &lt;- \"F_CH4\" # Name the layer\n  \n  writeRaster(pred ,paste(predictions.dir,\"/\",\"MODEL_PRED_m\",i,\".tif\", sep =\"\"), overwrite=TRUE )\n}\n\nDelete the json files created:\n\nsetwd(predictions.dir)\ndelete &lt;- list.files( path = predictions.dir, pattern=\".json\")\nfile.remove(delete)\n\nlogical(0)\n\n\nMake of list of all the files in a directory that you want to import, and import all the files in the list with rast().\n\nsetwd(predictions.dir)\npred &lt;- list.files( path = predictions.dir, pattern=\"MODEL_PRED_m\")\npredictions &lt;- rast(pred)\n\n\n\n(5) Use predictions\nCreate the 2021 methane budget. To get an annual budget, sum the total monthly fluxes.\n\npredictions.2021.total &lt;- sum(predictions )\nunits(predictions.2021.total ) &lt;- 'g C m-2 year-1' # add the units\nnames(predictions.2021.total ) &lt;- \"F_CH4\"\n\nTo determine the total amount of methane in 2021 for natural areas we need to consider the area:\n\nct.area = cellSize(predictions.2021.total, unit=\"m\")\npredictions.2021.total$F_CH4_total &lt;- (predictions.2021.total$F_CH4 * ct.area)/1000000000000 \nunits(predictions.2021.total$F_CH4_total) &lt;- \"Gigatons of carbon per year\"\n\n# Total emissions in 2021:\nglobal( predictions.2021.total$F_CH4_total, \"sum\", na.rm=T)\n\n                  sum\nF_CH4_total 0.1154527\n\n\nNow you are ready to follow the same workflow for your model. (1) For your final project, determine where you will project your model.\n(2) Make a list of the variables, their units, the exact name and class of each variable in your model.\n(3) Determine where you can get a spatial version of each variable in your model.\n(4) Format each spatial layer. (5) Make predictions. (6) Use predictions.\nEnsure your raster layers all have the same CRS and resolution!",
    "crumbs": [
      "Dynamic Models in R",
      "Spatial Model Visualization"
    ]
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Course Setup",
    "section": "",
    "text": "The data files required for this course are available through the Environmental Data Initiative (EDI) repository. EDI is a data repository that hosts ecological and environmental data for the long-term.\nThere are various ways to acquire data from data repositories. The method that a lot of people are accustomed to is by clicking “Download” to manually grab the files and then dragging them to the appropriate folder on their local computer. This method works, but it is not the most reproducible way to run your workflow since it’s difficult to document the clicking of your cursor.\nInstead, you can acquire data programmatically with R and save all the steps in this part of the workflow in a script. If you, or anyone else, want to reproduce the same results in the future, you can just rerun the same script.\nTo get the data programmatically through R, first navigate to the data package landing page at: https://doi.org/10.6073/pasta/5100e60582808c66095e767be806109f\n\n\n\nThe landing page shows a summary of the metadata for this data package. Metadata, or data about data, gives context to the variables in the dataset by documenting what was measured, who made the measurements, when and where were they made, and why was the data collected/created in the first place.\nWe will need the direct URL to “EDS_Course_Materials.zip” in order to download it through R. To get this specific URL, click on “View Full Metadata”.\n\n\n\nThis will bring us to a page with detailed descriptions of everything associated with this data package.\n\n\n\nScroll down to “Detailed Metadata” and click on the plus/minus symbols next to “Data Entities” to show more information on “EDS_Course_Materials.zip”.\n\n\n\nThis section describes what the file is for, along with additional metadata such as the file size and format. Copy the URL for this file.\nOnce you’ve got the URL, navigate to your workspace/working directory in RStudio for this course. You can check your current working directory by running getwd(). If the output of getwd() does not return the file path to your folder for this course, you will need to run setwd(dir = \"...\") by replacing ... with the file path of your choice.\nThen follow the code below to download and unzip “EDS_Course_Materials.zip”.\n\n## --------------------------------------------- ##\n#               EDI Download -----\n## --------------------------------------------- ##\n\n# Let's download EDS_Course_Materials.zip from this EDI package:\n# https://doi.org/10.6073/pasta/5100e60582808c66095e767be806109f \n\n# Paste the direct URL to EDS_Course_Materials.zip\nedi_url &lt;- \"https://pasta.lternet.edu/package/data/eml/edi/2081/1/757761d2f8d14f9026c67a449beac216\"\n\n# Uncomment and run the line below if you get a timeout error while downloading the file\n# This will set the download timeout to 120 seconds instead of the default 60\n# options(timeout = 120) \n\n# Download the zip file to your local computer\ndownload.file(url = edi_url, destfile = \"EDS_Course_Materials.zip\")\n\n# Unzip it into a folder called \"data\"\nunzip(\"EDS_Course_Materials.zip\", exdir = \"data\")\n\nIf you encounter an error while downloading that says, “Timeout of 60 seconds was reached”, run options(timeout = 120) to increase the duration of the timeout to 120 seconds. Feel free to set an even higher number if your computer needs extra time.\nDue to the way the data has been packaged, you may see this folder organization once you’ve unzipped the file:\nyour-working-directory/\n├── data/               \n│   ├── __MACOSX/             \n│   └── EDS_Course_Materials/             \n│       ├── EDS_Course_Materials_README.pdf\n│       ├── Elevation.tif\n│       ├── Elevation.tif.aux.json\n│       ├── ENV720-MOD13A3-061-results.csv\n│       ├── final_model.RDATA\n│       ├── FLUXNET_FLUXES.RDATA\n│       ├── FluxNet_Sites_2024.csv\n│       ├── GlobalClimate.tif\n│       ├── GlobalClimate.tif.aux.json\n│       ├── GlobalSoil_grids.tif\n│       ├── GlobalSoil_grids.tif.aux.json\n│       ├── MODIS_IGBP_2001-2022_CT.tif\n│       ├── MODIS_IGBP_2001-2022_CT.tif.aux.json\n│       ├── MODIS_IGBP_2001-2022_CT.tif.aux.xml\n│       ├── Monthly_Fluxes.RDATA\n│       ├── RANDOMFOREST_DATASET.RDATA\n│       ├── SensitivityProducts.RDATA\n│       ├── TERRA_PPT_2021_CT.tif\n│       └── TERRA_TMEAN_2021_CT.tif   \n│\n└── more_stuff/            \nFor the purposes of this course, we need to move all the content under data/EDS_Course_Materials to just the data folder. Run the following code to do so.\n\n## --------------------------------------------- ##\n#               Moving Files -----\n## --------------------------------------------- ##\n\n# List all the files inside \"data/EDS_Course_Materials\"\nnames &lt;- dir(file.path(\"data\", \"EDS_Course_Materials\")) \n\n# Create a function to move all the files from \"data/EDS_Course_Materials\" to \"data\"\nrelocate_function &lt;- function(file_name){\n  file.rename(from = file.path(\"data/EDS_Course_Materials\", file_name),\n              to = file.path(\"data\", file_name) )\n}\n\n# Apply the function to all files\nlapply(names, relocate_function)\n\n# Delete the now empty EDS_Course_Materials folder\nunlink(file.path(\"data\", \"EDS_Course_Materials\"), recursive = TRUE)\n\nIf you’ve followed all the steps correctly, your working directory should now look something like this:\nyour-working-directory/\n├── data/               \n│   ├── __MACOSX/             \n│   ├── EDS_Course_Materials_README.pdf\n│   ├── Elevation.tif\n│   ├── Elevation.tif.aux.json\n│   ├── ENV720-MOD13A3-061-results.csv\n│   ├── final_model.RDATA\n│   ├── FLUXNET_FLUXES.RDATA\n│   ├── FluxNet_Sites_2024.csv\n│   ├── GlobalClimate.tif\n│   ├── GlobalClimate.tif.aux.json\n│   ├── GlobalSoil_grids.tif\n│   ├── GlobalSoil_grids.tif.aux.json\n│   ├── MODIS_IGBP_2001-2022_CT.tif\n│   ├── MODIS_IGBP_2001-2022_CT.tif.aux.json\n│   ├── MODIS_IGBP_2001-2022_CT.tif.aux.xml\n│   ├── Monthly_Fluxes.RDATA\n│   ├── RANDOMFOREST_DATASET.RDATA\n│   ├── SensitivityProducts.RDATA\n│   ├── TERRA_PPT_2021_CT.tif\n│   └── TERRA_TMEAN_2021_CT.tif   \n│\n└── more_stuff/",
    "crumbs": [
      "Setup"
    ]
  },
  {
    "objectID": "setup.html#data-acquisition",
    "href": "setup.html#data-acquisition",
    "title": "Course Setup",
    "section": "",
    "text": "The data files required for this course are available through the Environmental Data Initiative (EDI) repository. EDI is a data repository that hosts ecological and environmental data for the long-term.\nThere are various ways to acquire data from data repositories. The method that a lot of people are accustomed to is by clicking “Download” to manually grab the files and then dragging them to the appropriate folder on their local computer. This method works, but it is not the most reproducible way to run your workflow since it’s difficult to document the clicking of your cursor.\nInstead, you can acquire data programmatically with R and save all the steps in this part of the workflow in a script. If you, or anyone else, want to reproduce the same results in the future, you can just rerun the same script.\nTo get the data programmatically through R, first navigate to the data package landing page at: https://doi.org/10.6073/pasta/5100e60582808c66095e767be806109f\n\n\n\nThe landing page shows a summary of the metadata for this data package. Metadata, or data about data, gives context to the variables in the dataset by documenting what was measured, who made the measurements, when and where were they made, and why was the data collected/created in the first place.\nWe will need the direct URL to “EDS_Course_Materials.zip” in order to download it through R. To get this specific URL, click on “View Full Metadata”.\n\n\n\nThis will bring us to a page with detailed descriptions of everything associated with this data package.\n\n\n\nScroll down to “Detailed Metadata” and click on the plus/minus symbols next to “Data Entities” to show more information on “EDS_Course_Materials.zip”.\n\n\n\nThis section describes what the file is for, along with additional metadata such as the file size and format. Copy the URL for this file.\nOnce you’ve got the URL, navigate to your workspace/working directory in RStudio for this course. You can check your current working directory by running getwd(). If the output of getwd() does not return the file path to your folder for this course, you will need to run setwd(dir = \"...\") by replacing ... with the file path of your choice.\nThen follow the code below to download and unzip “EDS_Course_Materials.zip”.\n\n## --------------------------------------------- ##\n#               EDI Download -----\n## --------------------------------------------- ##\n\n# Let's download EDS_Course_Materials.zip from this EDI package:\n# https://doi.org/10.6073/pasta/5100e60582808c66095e767be806109f \n\n# Paste the direct URL to EDS_Course_Materials.zip\nedi_url &lt;- \"https://pasta.lternet.edu/package/data/eml/edi/2081/1/757761d2f8d14f9026c67a449beac216\"\n\n# Uncomment and run the line below if you get a timeout error while downloading the file\n# This will set the download timeout to 120 seconds instead of the default 60\n# options(timeout = 120) \n\n# Download the zip file to your local computer\ndownload.file(url = edi_url, destfile = \"EDS_Course_Materials.zip\")\n\n# Unzip it into a folder called \"data\"\nunzip(\"EDS_Course_Materials.zip\", exdir = \"data\")\n\nIf you encounter an error while downloading that says, “Timeout of 60 seconds was reached”, run options(timeout = 120) to increase the duration of the timeout to 120 seconds. Feel free to set an even higher number if your computer needs extra time.\nDue to the way the data has been packaged, you may see this folder organization once you’ve unzipped the file:\nyour-working-directory/\n├── data/               \n│   ├── __MACOSX/             \n│   └── EDS_Course_Materials/             \n│       ├── EDS_Course_Materials_README.pdf\n│       ├── Elevation.tif\n│       ├── Elevation.tif.aux.json\n│       ├── ENV720-MOD13A3-061-results.csv\n│       ├── final_model.RDATA\n│       ├── FLUXNET_FLUXES.RDATA\n│       ├── FluxNet_Sites_2024.csv\n│       ├── GlobalClimate.tif\n│       ├── GlobalClimate.tif.aux.json\n│       ├── GlobalSoil_grids.tif\n│       ├── GlobalSoil_grids.tif.aux.json\n│       ├── MODIS_IGBP_2001-2022_CT.tif\n│       ├── MODIS_IGBP_2001-2022_CT.tif.aux.json\n│       ├── MODIS_IGBP_2001-2022_CT.tif.aux.xml\n│       ├── Monthly_Fluxes.RDATA\n│       ├── RANDOMFOREST_DATASET.RDATA\n│       ├── SensitivityProducts.RDATA\n│       ├── TERRA_PPT_2021_CT.tif\n│       └── TERRA_TMEAN_2021_CT.tif   \n│\n└── more_stuff/            \nFor the purposes of this course, we need to move all the content under data/EDS_Course_Materials to just the data folder. Run the following code to do so.\n\n## --------------------------------------------- ##\n#               Moving Files -----\n## --------------------------------------------- ##\n\n# List all the files inside \"data/EDS_Course_Materials\"\nnames &lt;- dir(file.path(\"data\", \"EDS_Course_Materials\")) \n\n# Create a function to move all the files from \"data/EDS_Course_Materials\" to \"data\"\nrelocate_function &lt;- function(file_name){\n  file.rename(from = file.path(\"data/EDS_Course_Materials\", file_name),\n              to = file.path(\"data\", file_name) )\n}\n\n# Apply the function to all files\nlapply(names, relocate_function)\n\n# Delete the now empty EDS_Course_Materials folder\nunlink(file.path(\"data\", \"EDS_Course_Materials\"), recursive = TRUE)\n\nIf you’ve followed all the steps correctly, your working directory should now look something like this:\nyour-working-directory/\n├── data/               \n│   ├── __MACOSX/             \n│   ├── EDS_Course_Materials_README.pdf\n│   ├── Elevation.tif\n│   ├── Elevation.tif.aux.json\n│   ├── ENV720-MOD13A3-061-results.csv\n│   ├── final_model.RDATA\n│   ├── FLUXNET_FLUXES.RDATA\n│   ├── FluxNet_Sites_2024.csv\n│   ├── GlobalClimate.tif\n│   ├── GlobalClimate.tif.aux.json\n│   ├── GlobalSoil_grids.tif\n│   ├── GlobalSoil_grids.tif.aux.json\n│   ├── MODIS_IGBP_2001-2022_CT.tif\n│   ├── MODIS_IGBP_2001-2022_CT.tif.aux.json\n│   ├── MODIS_IGBP_2001-2022_CT.tif.aux.xml\n│   ├── Monthly_Fluxes.RDATA\n│   ├── RANDOMFOREST_DATASET.RDATA\n│   ├── SensitivityProducts.RDATA\n│   ├── TERRA_PPT_2021_CT.tif\n│   └── TERRA_TMEAN_2021_CT.tif   \n│\n└── more_stuff/",
    "crumbs": [
      "Setup"
    ]
  }
]